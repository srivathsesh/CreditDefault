---
title: "Credit Default Prediction"
author: "Sri Seshadri"
date: "9/28/2019"
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
output:
    pdf_document:
      toc: true
      toc_depth: 3
 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
```

# 1. Introduction (DRAFT)

This paper discusses the various modeling approaches to predict customers defaulting on their credit card payments by using six months' credit card bill amounts, payment history. A working model that takes in customer data and reports a list of customers at risk of default and the amount of money that is at risk. The following are the key insights that were discovered from the data:

  1. X % increase in monthly bill for a customer paying bills at a rate of y% of the monthly bill has Z% increased risk compared to average customer.
  2. 

# 2. Executive Summary (DRAFT)

Insights as of 10/13:

1. Total delinquency (total number of months late over 6 months) when greater than 2 has increases the chance of default (15% to 52%)
2. Historical delinquencies, Variance of invoice and average amount paid towards bill are key contributors towards prediction of default.


# 3. Data

Data are available for 30,000 customers spanning between April 2005 and September 2005. The description of the data are shown in Table 1 below. Table 2 and Table 3 show the general summary of the data. Upon inspection of the data summaries, there were data quality issues identified, that are discussed in the following section.

```{r, warning = F, message = F}
library(tidyverse)
library(caret)
library(magrittr)
library(pander)
library(knitr)
library(kableExtra)
library(matrixStats)
library(OneR)
source('getPerformance.R')
load('checkpoint2_3.RData')
#setting seed

set.seed(10)

#------------------------------------------------------
#               READ DATA IN
#---------------------------------------------------------

creditdata <- readRDS("credit_card_default.RData")

creditdata <- as.data.frame(creditdata) %>% as_tibble()
creditdata %<>% 
  mutate_at(c("DEFAULT","EDUCATION","MARRIAGE","SEX","data.group"),.funs = as.factor)

#-----------------------------------------------------
#              DATA DICTIONARY
#-----------------------------------------------------

Dict <- tibble(FEATURE = colnames(creditdata)[1:25],
               DESCRIPTION =
                 c("CUSTOMER ID",
                   "CREDIT LIMIT",
                   "GENDER",
                   "EDUCATION",
                   "MARITAL STATUS",
                   "AGE",
                   "REPAYMENT STATUS SEP 2005",
                   "REPAYMENT STATUS AUG 2005",
                   "REPAYMENT STATUS JUL 2005",
                   "REPAYMENT STATUS JUN 2005",
                   "REPAYMENT STATUS MAY 2005",
                   "REPAYMENT STATUS APR 2005",
                   "BILL SEP 2005",
                   "BILL AUG 2005",
                   "BILL JUL 2005",
                   "BILL JUN 2005",
                   "BILL MAY 2005",
                   "BILL APR 2005",
                   "PAYMENT SEP 2005",
                   "PAYMENT AUG 2005",
                   "PAYMENT JUL 2005",
                   "PAYMENT JUN 2005",
                   "PAYMENT MAY 2005",
                   "PAYMENT APR 2005",
                   "DEFAULTING CUSTOMER"
                  ),
               COMMENTS = 
                 c("",
                   "INCLUDES SUPPL CARDS",
                   "1 = MALE, 2 = FEMALE",
                   "1 = GRAD,2 = UNIV,3 = HIGH SCH,4 = OTHERS",
                   "1=MARRIED,2=SINGLE,3=OTHERS",
                   "YEARS",
                   "assumed SEP 2005 ; -1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "1 = DEFAULT, 0 = NON-DEFAULT"
                 )
           
       ) 

#*******************************************
#         REPORT DICTIONARY
#*******************************************

kable(Dict,format = "latex",align = "c",caption = "Data Dictionary") %>% 
  kable_styling(full_width = F,latex_options = "hold_position",font_size = 7) %>% 
  column_spec(1,border_left = T) %>% 
  column_spec(3,width = "20em",border_right = T) %>% 
  row_spec(0,bold = T) %>% 
  collapse_rows(valign = "middle")


# Get stats on the data
skimr::skim_with(numeric = list(hist = NULL),integer = list(hist = NULL))
skimr::skim_to_wide(creditdata) %>% 
  filter(type == "factor") %>% 
  filter(!variable %in% "data.group") %>% 
  select(variable,missing,complete,n,n_unique,top_counts) %>% 
  #mutate(top_counts = cell_spec(top_counts,"latex",align = "r")) %>% 
  kable(format = "latex",digits = 1,booktabs = T,caption = "categorical variables summary") %>% 
  kable_styling(latex_options = "hold_position")

skimr::skim_to_wide(creditdata) %>% 
  filter(type == "integer") %>% 
  select(variable,missing,complete,n,mean,sd,p0,p25,median,p75,p100) %>% 
  #mutate(top_counts = cell_spec(top_counts,"latex",align = "r")) %>% 
  kable(format = "latex",digits = 1,booktabs = T,caption = "numeric variables summary") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)

```

## 3.1 Data Issues

1. Table 2 shows the variable $EDUCATION$ to have 7 possible values, while the dictionary in table 1 shows us expect 4 possible values. The values 0,5 and 6 would be assigned to a category "unknown", coded as 9.

2. Similarly for 54 customers, the value of $MARRIAGE$ is entered as 0. The 0 values are assumed to be "unknown" marital status.

3. The variable PAY_0 is assumed to be the repayment status as of September 2005.Hence renamed as PAY_1

4. Repayment status has values that include -2 and 0. Approximately 26,000 customers have repayment status that is not described in the dictionary. It is very likely that the data dictionary had an omission in describing -2 and 0. Therefore they are assumed to be non-delayed payments.

5. There are 543 customers in the data who are marked delinquent when the average bill over 6 months is 0. It is not clear if the customers defaulted in the months prior to the 6 months of data that is provided.


```{r,eval=T}

# remap unknown educaton to code 9
creditdata %<>% 
  mutate(EDUCATION = as.numeric(EDUCATION)-1) %>% 
  mutate(EDUCATION = ifelse(EDUCATION %in% c(0,5,6),9,EDUCATION)) %>% 
  mutate(EDUCATION = as.factor(EDUCATION)) 

# rename PAY_0 to PAY_1
creditdata %<>% 
  rename(PAY_1 = PAY_0)



```

## 3.2 Data Splitting

The data was separated into three groups for model training, validation and testing purposes. The number of customers in each of these groups are shown below in table 4.

```{r,eval=T}
creditdata %>% 
  mutate(data.group = ifelse(data.group == 1, "Train", ifelse(data.group == 2, "Test", "Validate"))) %>% 
  group_by(data.group) %>% 
  summarise(datapoints = n()) %>% 
  knitr::kable(caption = "Train,Validation and Test splits",format = "latex",booktabs = T) %>% 
  kable_styling(latex_options = c("striped","hold_position"))


```


# 4. Feature Engineering

Table 5 shows the features that are derived from the data along with their name and calculation method. The AGE and ppk (the distance between credit limit and average bill in units of 3 time standard deviations of bill amounts) variables are continuous variables that are binned using Weights Of Evidence (WOE) method.

It is interesting to note that the WOE indicates that the customers in higher age group (over 39 years) have relatively higher risk of default compared to customers in the late twenties. Possibly due to potential loss in employment and difficulty to secure an income source. This hypothesis will be explored in the exploratory data analysis section below.

ppk values were binned and replaced with their respective WOE. Figure 1 shows the binning of these variables.


\newpage
\blandscape

```{r,eval=T}
# EngFeats <- data.frame(
#   Feature = c(
#     "Payment to Bill ratio",
#     "Average Payment to Bill ratio",
#     "Proportion of credit used in the month",
#     " Month's in/decrease in credit use",
#     "Average proportion of credit used",
#     "Average Montly bill",
#     "Month's in/decrease in bill",
#     "Average amount paid",
#     "Month's in/decrease in payment",
#     "Balance growth over 6 months",
#     "Credit utilization growth 6 months",
#     "Month's in/decrease in Payment ratio",
#     "Customer with increased utilization over time",
#     "Customer with increased bill over time",
#     "Customer with reduced payments time",
#     "Customer with reduced payment/bill over time",
#     "Max bill",
#     "Max payment",
#     "Max delinquency",
#     "Age of customers between mentioned range",
#      "Ppk -Dist to credit limit in units of 3 std.dev of Bill",
#     "Ppk discretized with WOE values"
#     
#     
#   ),
#   `Column_Names` = c(
#     "PMT_RATIO2 - PMT_RATIO6",
#     # "$PMT\\_RATIO3$",
#     # "$PMT\\_RATIO4$",
#     # "$PMT\\_RATIO5$",
#     # "$PMT\\_RATIO6$",
#     
#     "$Avg\\_PMT\\_RATIO$",
#     
#     "$UTIL\\_1$ to $UTIL\\_6$ ",
#     # "$UTIL\\_2$",
#     # "$UTIL\\_3$",
#     # "$UTIL\\_4$",
#     # "$UTIL\\_5$",
#     # "$UTIL\\_6$",
#     
#     "$UTILMR1$ to $UTILMR5$",
#     # "$UTILMR4$",
#     # "$UTILMR3$",
#     # "$UTILMR2$",
#     # "$UTILMR1$",
#   
#    
#     "$Avg\\_UTIL$",
#     
#     "$Avg\\_BILL\\_Amt$",
#     
#      "$BILLAMTMR1$ to $BILLAMTMR5$ ", 
#      # "$BILLAMTMR4$",
#      # "$BILLAMTMR3$",
#      # "$BILLAMTMR2$",
#      # "$BILLAMTMR1$",
#     
#      "$Avg\\_PMT\\_Amt$",
#     
#       "$PAYMR1$ to $PAYMR5$",
#       # "$PAYMR4$",
#       # "$PAYMR3$",
#       # "$PAYMR2$",
#       # "$PAYMR1$",
#     
#     "$Bal\\_Growth\\_6mo$", 
#     
#     "$UTIL\\_Growth\\_6mo$", 
#     
#          "$PMT\\_RATIOMR2$ to $PMT\\_RATIOMR5$", 
#          # "$PMT\\_RATIOMR4$",
#          # "$PMT\\_RATIOMR3$", 
#          # "$PMT\\_RATIOMR2$", 
#     
#     "$UtilFlag$",
#     
#     '$balFlag$',
#     
#     "$payFlag$",
#     
#     
#     "$payRatioFlag$",
#     
#     "$Max\\_Bill\\_Amt$",
#     
#     "$Max\\_Pmt\\_Amt$",
#     
#     "$Max\\_Dlq$",
#     
#     "$Age\\_21\\_25$",
#     
#     "$ppk$",
#     
#     "$ppkwoe$"
#     
#     
#     
#   ),
#   Calculation = c(' $\\frac{PAY\\_AMT_{mnth-1}}{BILL\\_AMT{mnth}}$ $mnth \\in {2,3,4,5,6}$',
#                   # ' $\\frac{PAY\\_AMT2}{BILL\\_AMT3}$',
#                   # ' $\\frac{PAY\\_AMT3}{BILL\\_AMT4}$',
#                   # ' $\\frac{PAY\\_AMT4}{BILL\\_AMT5}$',
#                   # ' $\\frac{PAY\\_AMT5}{BILL\\_AMT6}$',
#   
#                   ' $\\frac{\\sum_{Month=1}^{5}PAY\\_AMT_{mnth}}{\\sum_{mnth=2}^{6}BILL\\_AMT_{mnth}}$',
#                   
#                   ' $\\frac{BILL\\_AMT_{mnth}}{LIMIT\\_BAL}$ $mnth \\in {1,2,3,4,5,6}$',
#                   # ' $\\frac{BILL\\_AMT2}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT3}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT4}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT5}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT6}{LIMIT\\_BAL}$',
#                   
#                   '$UTIL_{mnth-1}- UTIL_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$UTIL\\_4 - UTIL\\_5$',
#                   # '$UTIL\\_3 - UTIL\\_4$',
#                   # '$UTIL\\_2 - UTIL\\_3$',
#                   # '$UTIL\\_1 - UTIL\\_2$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}UTIL_{mnth}}{5}$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}BILL\\_AMT_{mnth}}{5}$',
#                   
#                   '$BILL\\_AMT_{mnth-1} - BILL\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$BILL\\_AMT4 - BILL\\_AMT5$',
#                   # '$BILL\\_AMT3 - BILL\\_AMT4$',
#                   # '$BILL\\_AMT2 - BILL\\_AMT3$',
#                   # '$BILL\\_AMT1 - BILL\\_AMT2$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}PAY\\_AMT_{mnth}}{5}$',
#                   
#                   
#                   
#                   '$PAY\\_AMT_{mnth-1} - PAY\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$PAY\\_AMT4 - PAY\\_AMT5$',
#                   # '$PAY\\_AMT3 - PAY\\_AMT4$',
#                   # '$PAY\\_AMT2 - PAY\\_AMT3$',
#                   # '$PAY\\_AMT1 - PAY\\_AMT2$',
#                   
#                   "$BILL\\_AMT_{mnth-1} - BILL\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$",
#                   
#                   "$UTIL\\_1 - UTIL\\_6$",
#                   
#                   "$PMT\\_RATIO5 - PMT\\_RATIO6$",
#                   # "$PMT\\_RATIO4 - PMT\\_RATIO5$",
#                   # "$PMT\\_RATIO3 - PMT\\_RATIO4$",
#                   # "$PMT\\_RATIO2 - PMT\\_RATIO3$",
#                   
#                   "$\\forall {UTILMR5,UTILMR4,UTILMR3,UTILMR2,UTILMR1} > 0$",
#                   
#                   "$\\forall {BILLAMTMR5,BILLAMTMR4,BILLAMTMR3,BILLAMTMR2,BILLAMTMR1} > 0$",
#                   
#                   "$\\forall{PAYMR5,PAYMR4,PAYMR3,PAYMR2,PAYMR1} > 0$",
#                   
#                   "$\\forall{PMT\\_RATIOMR5,PMT\\_RATIOMR4,PMT\\_RATIOMR3,PMT\\_RATIOMR2} > 0$",
#                   
#                   '$max(BILL\\_AMT1,BILL\\_AMT2,BILL\\_AMT3,BILL\\_AMT4,BILL\\_AMT5,BILL\\_AMT6)$',
#                   
#                   '$max(PAY\\_AMT1,PAY\\_AMT2,PAY\\_AMT3,PAY\\_AMT4,PAY\\_AMT5,PAY\\_AMT6)$',
#                   
#                   '$max(PAY\\_1,PAY\\_2,PAY\\_3,PAY\\_4,PAY\\_5,PAY\\_6)$',
#                   
#                   '$if AGE>=21 AND AGE<=25 then 1 else 0$',
#                   
#               '$\\frac{(LIMIT\\_BAL - \\overline{BILL\\_AMT_{mth}})}{\\sqrt{\\sum_{mth=1}^6\\frac{(BILL\\_AMT_{mth} - \\overline{BILL\\_AMT_{mth}})^2}{5}}}$',
#                   
#                   'ppk binned and replaced by WOE'
#                   
#                   
#                   
#                   
#                   
#                   
#   )
#   
# 
# 
# )

knitr::kable(data.frame(l = ""),"latex", escape = F,booktabs = T,caption = "Engineered features") 

```

\elandscape
\newpage


```{r,eval=T}

# ------------------------------------------------------------
# PAY RATIO defined as payment over bill 
# Make the PAY_RATIO as 1 when the buill and pay amounts are 0
#-------------------------------------------------------------
creditdata %<>% 
  mutate(PMT_RATIO2 = ifelse(PAY_AMT1==0 & BILL_AMT2==0|BILL_AMT2 <= 0,1,PAY_AMT1/BILL_AMT2),
         PMT_RATIO3 = ifelse(PAY_AMT2==0 & BILL_AMT3== 0|BILL_AMT3 <= 0,1,PAY_AMT2/BILL_AMT3),
         PMT_RATIO4 = ifelse(PAY_AMT3==0 & BILL_AMT4==0|BILL_AMT4 <= 0,1,PAY_AMT3/BILL_AMT4),
         PMT_RATIO5 = ifelse(PAY_AMT4==0 & BILL_AMT5==0|BILL_AMT5 <= 0,1,PAY_AMT4/BILL_AMT5),
         PMT_RATIO6 = ifelse(PAY_AMT5==0 & BILL_AMT6==0|BILL_AMT6 <= 0,1,PAY_AMT5/BILL_AMT6)) %>% 
  mutate(
         Avg_PMT_RATIO = ifelse(rowSums(select(.,matches("BILL_AMT[2-6]")))==0,1,rowSums(select(.,matches("PAY_AMT[1-5]")))/rowSums(select(.,matches("BILL_AMT[2-6]")))))


```



```{r,eval=T}
library(matrixStats)

# Utilization defined as a % usage of credit limit. When there is a negative bill the util is made 0.

creditdata %<>% 
  mutate(UTIL_1 = ifelse(BILL_AMT1/LIMIT_BAL < 0 , 0 , BILL_AMT1/LIMIT_BAL),
         UTIL_2 = ifelse(BILL_AMT2/LIMIT_BAL < 0 , 0 , BILL_AMT2/LIMIT_BAL),
         UTIL_3 = ifelse(BILL_AMT3/LIMIT_BAL < 0 , 0 , BILL_AMT3/LIMIT_BAL),
         UTIL_4 = ifelse(BILL_AMT4/LIMIT_BAL < 0 , 0 , BILL_AMT4/LIMIT_BAL),
         UTIL_5 = ifelse(BILL_AMT5/LIMIT_BAL < 0 , 0 , BILL_AMT5/LIMIT_BAL),
         UTIL_6 = ifelse(BILL_AMT6/LIMIT_BAL < 0 , 0 , BILL_AMT6/LIMIT_BAL))
```



```{r,eval=T}
#--------------------------------------------------------------------------------------------------------------
# While we that the median balance increases over time, how much of the population is causing this to increase
# Calculate the difference between subsequent months in UTIL and BIll_AMTs
#--------------------------------------------------------------------------------------------------------------

creditdata %<>% 
  mutate(Avg_UTIL = rowMeans(select(.,starts_with("UTIL"))),
         UTILMR5 = UTIL_5 - UTIL_6,
         UTILMR4 = UTIL_4 - UTIL_5,
         UTILMR3 = UTIL_3 - UTIL_4,
         UTILMR2 = UTIL_2 - UTIL_3,
         UTILMR1 = UTIL_1 - UTIL_2,
         Avg_BILL_Amt = rowMeans(select(.,starts_with("BILL_AMT")),na.rm = T),
         stdBILL = ifelse(rowSds(select(.,starts_with("BILL_AMT")) %>% as.matrix()) == 0, 0.01,rowSds(select(.,starts_with("BILL_AMT")) %>% as.matrix()))) %>% 
  mutate(ppk = (LIMIT_BAL - Avg_BILL_Amt)/(3*stdBILL),
         BILLAMTMR5 = BILL_AMT5 - BILL_AMT6,
         BILLAMTMR4 = BILL_AMT4 - BILL_AMT5,
         BILLAMTMR3 = BILL_AMT3 - BILL_AMT4,
         BILLAMTMR2 = BILL_AMT2 - BILL_AMT3,
         BILLAMTMR1 = BILL_AMT1 - BILL_AMT2,
         Avg_PMT_Amt = rowMeans(select(.,starts_with("PAY_AMT")),na.rm = T))


creditdata %<>%
  mutate(PAYMR5 = PAY_AMT5 - PAY_AMT6,
         PAYMR4 = PAY_AMT4 - PAY_AMT5,
         PAYMR3 = PAY_AMT3 - PAY_AMT4,
         PAYMR2 = PAY_AMT2 - PAY_AMT3,
         PAYMR1 = PAY_AMT1 - PAY_AMT2)

creditdata %<>% 
  mutate(Bal_Growth_6mo = BILL_AMT1 - BILL_AMT6,
         UTIL_Growth_6mo = UTIL_1 - UTIL_6,
         PMT_RATIOMR5 = PMT_RATIO5 - PMT_RATIO6,
         PMT_RATIOMR4 = PMT_RATIO4 - PMT_RATIO5,
         PMT_RATIOMR3 = PMT_RATIO3 - PMT_RATIO4,
         PMT_RATIOMR2 = PMT_RATIO2 - PMT_RATIO3
         )


  

# Customers with increasing utilization over time
creditdata %>% filter_at(vars(starts_with("UTILM")),all_vars(. > 0)) %>% select(ID) ->IDs

# customers with increasing Balance over time
creditdata %>% filter_at(vars(starts_with("BILLAMTMR")),all_vars(. > 0)) %>% select(ID) ->BalIDs

# customers with decreasing payments over time

creditdata %>% filter_at(vars(starts_with("PAYMR")),all_vars(. < 0)) %>% select(ID) ->payIDs

# customers with decreasing payment ratio over time

creditdata %>% filter_at(vars(starts_with("PMT_RATIOMR")),all_vars(. < 0)) %>% select(ID) ->payRatioIDs

creditdata %<>% 
  mutate(utilFlag = ifelse(ID %in% IDs$ID,1,0))

creditdata %<>% 
  mutate(balFlag = ifelse(ID %in% BalIDs$ID,1,0))

creditdata %<>% 
  mutate(payFlag = ifelse(ID %in% payIDs$ID,1,0))

creditdata %<>% 
  mutate(payRatioFlag = ifelse(ID %in% payRatioIDs$ID,1,0))





## Get percentage increase between month 6 and month 1

creditdata %<>%
  mutate(PerBALinc = ifelse(!is.infinite((BILL_AMT1 - BILL_AMT6)/BILL_AMT6)&!is.na((BILL_AMT1 - BILL_AMT6)/BILL_AMT6),(BILL_AMT1 - BILL_AMT6)/BILL_AMT6,(BILL_AMT1 - BILL_AMT6)/0.01),
         PerUTILinc = ifelse(!is.infinite((UTIL_1 - UTIL_6)/UTIL_6)&!is.na((UTIL_6 - UTIL_1)/UTIL_6),(UTIL_1- UTIL_6)/UTIL_6,(UTIL_1 - UTIL_6)/0.0001)
         )

```



```{r,eval=T}
creditdata %<>% 
  mutate(Max_Bill_Amt = rowMaxs(select(.,matches("BILL_AMT[0-9]")) %>% as.matrix()),
         Max_Pmt_Amt = rowMaxs(select(.,matches("PAY_AMT[0-9]")) %>% as.matrix()))
```


```{r,eval=T}

# Customers delinquent often
# creditdata %<>%
#   mutate(Max_Dlq = ifelse(rowMaxs(select(.,matches("PAY_[0-9]")) %>% as.matrix()) < 0,0,rowMaxs(select(.,matches("PAY_[0-9]")) %>% as.matrix()) ))

creditdata %>% 
  select(ID,matches("PAY_[0-9]")) %>% 
  gather(key = "Period", value = "DLQ",-ID) %>% 
  mutate(DLQ = ifelse(DLQ < 0, 0,DLQ)) %>% 
  group_by(ID) %>% 
  summarise(DLQ_Count = count(DLQ > 0),
            DLQ_Total = sum(DLQ),
            DLQ_Max = max(DLQ)) -> Delequencies

creditdata %<>% 
  left_join(Delequencies)

#colnames(creditdata)
  
```


```{r AGEppkBinning,eval=T,fig.cap="WOE binning",fig.height=4,fig.pos='asis'}
# creditdata %<>% 
#   mutate(AgeBins = cut(AGE,breaks = seq(from = 25, to = 80,by = 5)))
#   
# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 7) -> tst7
# 
# Information::plot_infotables(tst7,"AGE")

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 6) -> tst6

Information::plot_infotables(tst6,"AGE") + coord_flip()->p1


# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) -> tst5
# 
# Information::plot_infotables(tst5,"AGE")
# 
# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) -> tst4
# 
# Information::plot_infotables(tst4,"AGE")

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(ppk, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) -> tst5ppk

Information::plot_infotables(tst5ppk,"ppk") + coord_flip()->p2

gridExtra::grid.arrange(p1,p2,ncol=2,top = grid::textGrob("WOE binning"))

```


```{r,eval=F}
library(OneR)

OneR::optbin(as.factor(DEFAULT) ~ AGE, data = creditdata, method = "infogain")-> optbin

plot(optbin)

```


```{r,eval=T}


creditdata %<>% 
  mutate(AGE_21_25 = ifelse(AGE>=21 & AGE<=25,1,0),
         AGE_26_29 = ifelse(AGE>=26 & AGE<=29,1,0),
         AGE_30_33 = ifelse(AGE>=30 & AGE<=33,1,0),
         AGE_34_38 = ifelse(AGE>=34 & AGE<=38,1,0),
         AGE_39_44 = ifelse(AGE>=39 & AGE<=44,1,0))

creditdata %<>% 
  mutate(ppkwoe = ifelse(ppk<0.49,0.30368328,
                         ifelse(ppk < 1.82,-0.06824883,
                                ifelse(ppk < 6.98,-0.08848357,
                                       ifelse(ppk < 27.3,-0.29885247,0.09571116)))))
#colnames(creditdata)
```


```{r,eval=F}
table(creditdata$DEFAULT,creditdata$utilFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$utilFlag))


table(creditdata$DEFAULT,creditdata$balFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$balFlag))

table(creditdata$DEFAULT,creditdata$payFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$payFlag))
```

## 4.1 Data Quality of Engineered Features

The use of very small values like a penny when denominator dollar amounts are 0 prevented the feature from going to an infinite value.Table 6 Shows the summary of the engineered features.


```{r}

EngFeatNames <- creditdata %>% select(matches("RATIO|Avg|UTIL|MR|6mo|Flag|Max|Age_|ppk|inc|DLQ|stdB|EDUCATION|SEX|MARRIAGE")) %>% colnames()
SelectedFeat <- EngFeatNames[!str_detect(EngFeatNames,"[A-Z][0-9]|UTIL_[0-9]")]

creditdata %>% 
  select(!!EngFeatNames) %>% 
  skimr::skim_to_wide() %>% 
  select(-type,-sd,-mean) %>% 
  kable("latex",digits = 2,booktabs = T,caption = "Engineering features summary",longtable = T) %>% 
  kable_styling(latex_options = "repeat_header",font_size = 7)
```

## 5. Exploratory Data Analysis (EDA).

In this section the training data are analyzed to explore potential relationships between the predictor and propensity of default. Table 7 shows the distribution of the defaulting customers among the 15180 customers.

```{r train}
train <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat,DEFAULT)
data.frame(prop.table(table(train$DEFAULT))) %>% 
  rename("DEFAULT" = Var1) %>% 
  kable("latex", booktabs = T, digits = 2,caption = "Distribution of Defaulting customers in training data") %>% 
  kable_styling(latex_options = "hold_position")
```

### 5.1 Magnitude of revolving credit

It is hypothesized that the magnitude of revolving credit is a reflection of income of customers and income has an effect on credit card default. There is no evidence of magnitude of revolving credit or income having an influence on default. In instances where either the payment or the bill amount were $<=0$ the value was assumed to be one-tenth of a New Taiwan(NT) penny. Figure 2 shows the log of the amounts for easier visualization. 

```{r,fig.cap="Effect of revolving credit on default",fig.height=4,fig.pos='asis'}
train %>% 
  ggplot(aes(x=log(ifelse(Avg_BILL_Amt<=0,0.001,Avg_BILL_Amt)),y=log(ifelse(Avg_PMT_Amt<=0,0.001,Avg_PMT_Amt)),col = DEFAULT)) + 
  xlab("log(Avg Bill Amount)") +
  ylab("log(Avg Payment Amount)")+
  geom_point(alpha = 0.4) + 
  theme_bw() -> scp

ggExtra::ggMarginal(scp,type = "histogram",groupColour = T) -> mp

mp
```



### 5.2 Past Delinquencies in payment

Here the frequency of delinquency (how often customers are delinquent) in payments' effect on default is explored. It is surprising that there are customers who were not late on payment were marked default. Its likely those customers though were not late, might have paid less than the minimum due. The second panel in the plot below explores the continued delinquency's effect on default. The higher total months late associates to increased odds of defaulting (thicker tails in the density plot). The weights of evidence plot on total & maximum delinquencies shows increased chances of defaulting when total delinquency is past 1 month. This potentially indicates that the bank pardon's a single late payment.

```{r,fig.cap="Effect of Delinqucies",fig.height=4,fig.pos = 'asis'}
train %>% 
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,1,0))) %>% 
  ggplot(aes(x=as.numeric(DLQ_Count),group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Frequency of Delinquency (DLQ_Count)") ->p1


train %>%
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,"Bill = 0","Bill > 0"))) %>%
  ggplot(aes(x=DLQ_Total,group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Total months late (DLQ_Total)") ->p2

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Total, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Total") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Total WOE") -> p3

train %>%
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,"Bill = 0","Bill > 0"))) %>%
  ggplot(aes(x=DLQ_Max,group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Max months late (DLQ_Max)") ->p4

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Max, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Max") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Max WOE") -> p5

gridExtra::grid.arrange(p1,p2,p3,p5, ncol = 2)

```

## 5.3 Utilization of credit limit

A process capability metric used in the quality engineering domain is used here to determine how close does the bill come to the credit limit in the units of 3 times standard deviation of the bills. Figure 1 in section 4, shows that ppk less that 0.5 have a higher odds (odds of 1.34) for defaulting. It is surprising that there are is a increase in the odds for customers who have higher ppk. It was hypothesized that these very low utilization of credit may be from higher age group of customers, but there is no solid evidence of that as seen in the second panel of the plot below. It is to be noted that for easier visualization the natural log of ppk was used with negative ppk (bills higher than credit limit) set to very low (1e-07) value. The bottom panel of the plot shows the effect due to the average utilization, 

```{r,fig.cap="Utilization of credit limit's effect on Default",fig.height=4,fig.pos = 'asis'}
train %>% ggplot(aes(x=log(ifelse(ppk <=0, 0.0000001,ppk)),group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("log(ppk)")-> u1
creditdata %>% filter(train==1) %>%  ggplot(aes(x=AGE, y = log(ifelse(ppk <=0, 0.0000001,ppk)), col = DEFAULT)) + geom_point(alpha = 0.4) + theme_bw() + ylab("log(ppk)")-> u2
train %>% ggplot(aes(y=Avg_UTIL,x = DEFAULT,col = DEFAULT)) + geom_boxplot() + theme_bw()-> u3
#gridExtra::tableGrob(broom::glance(chisq.test(utilflagtbl))) -> u4
gridExtra::grid.arrange(u1,u2,u3,ncol = 2)
```

The customers who have  continuous growth in utilization over 6 months is flagged and analyzed. The chi-squared test of independence shows there is a statistical difference. 


```{r}
table(train$DEFAULT,train$utilFlag) -> utilflagtbl
prop.table(utilflagtbl) %>% 
  data.frame() %>% rename("utilFlag" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Continous growth in utilization's effect on DEFAULT - ChiSq test") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(utilflagtbl) %>% pander()

```

## 5.4 Effect of balances

The effect of increased balances is quantified in the table below. The customers whose balances increases over time (month after month) are flagged and analyzed for the odds in default. The chi squared test of independence show there is a statistical significance due to increasing credit card balance over time. 

```{r}
table(train$DEFAULT,train$balFlag) -> balflagtbl
prop.table(balflagtbl) %>% 
  data.frame() %>% rename("balFlag" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Continous growth in balance's effect on DEFAULT - ChiSq test") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(balflagtbl) %>% pander()
```

## 5.5 Effect of decreased payments

Payments as a ratio of bill amounts, average payment amounts, continuous decrease in payments/pay ratios over time did not yield statistically significant results when explored graphically using density plots. However, in the section 5.7.1, decision trees help bring out signal from payment variables. The influence of payments are discussed in section 5.9.1 below.

## 5.6 Effect of Age

Figure 1 in section 4 shows relatively higher chances of defaulting in the early ages (21-24) and in the age group past mid-life 45 years and above. The increased chances in default past mid-life may be potentially attributed to loss in employment and or health related events in life.

## 5.7 Effect of Gender, Education and Marital Status

Effect of SEX on default is explored. In the data, 60% of the samples are of females (coded as 2). It can be seen from Figure 5 that proportions of defaulters are in each group are comparable (25% for males and 21% for females). However the chi-squared test shows the difference in the proportions as statistically significant. This may be likely due to the sensitivity to high number of samples in the training set. The importance of SEX in predicting the defaulters is explored in the model based EDA section below. 

The customers with graduate degree (coded 1), university education (coded 2) and high school diploma (coded 3) have 20%,24% and 26% defaulters respectively. The other and unknown categories have 7% and 8%  defaulters. The effect of education is statistically significant.

Customers who are married, single and marital status as others have 24%,21% and 25% defaulters respectively. The interaction of Gender and Education within every marital status was explored graphically in Figure 5, interactions were not found for the major marital status levels. 

Chi-squared tests show gender, education and marital status are significant. As mentioned above, this may be due to sample size being high and small differences are being deemed statistically significant.  



```{r,fig.cap = "GENDER,EDUCATION & MARITAL STATUS' effect on DEFAULT",fig.pos='asis'}
lattice::barchart(prop.table(table(train$SEX,train$DEFAULT)),ylab = "SEX") -> bp1
lattice::barchart(prop.table(table(train$EDUCATION,train$DEFAULT)),grid=T,ylab = "EDUCATION") -> bp2
lattice::barchart(prop.table(table(train$MARRIAGE,train$DEFAULT)),grid=T,ylab = "MARRIAGE") -> bp3


library(ggmosaic)

ggplot(data = train) + 
  geom_mosaic( aes(x=product(EDUCATION,SEX),fill = DEFAULT)) + 
  facet_wrap(~MARRIAGE) + xlab("DEFAULT:SEX") + ylab("EDUCATION") + 
  ggtitle("DEFAULT by SEX & EDUCATION by martial status") +
  theme(plot.title = element_text(size = 10))->msp1

gridExtra::grid.arrange(bp1,bp2,bp3,msp1,ncol = 2,nrow= 2)

table(train$DEFAULT,train$SEX) -> SEXtbl
prop.table(SEXtbl) %>% 
  data.frame() %>% rename("SEX" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "GENDER effect on DEFAULT - ChiSq test",digits = 2) %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(SEXtbl) %>% pander()


table(train$DEFAULT,train$EDUCATION) -> EDUtbl
prop.table(EDUtbl) %>% 
  data.frame() %>% rename("EDUCATION" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "EDUCATION effect on DEFAULT - ChiSq test",digits = 2) %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(EDUtbl) %>% pander()


table(train$DEFAULT,train$MARRIAGE) -> MARtbl
prop.table(MARtbl) %>% 
  data.frame() %>% rename("EDUCATION" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Marital Status' effect on DEFAULT - ChiSq test",digits = 2) %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(MARtbl) %>% pander()

```


## 5.8 Model based EDA

In this section, two methods are employed to gain more insights on the effects of the predictor variables on chances of default. 

- Tree based methods.
- Simple logistic regression.


### 5.8.1 Tree based methods.

Multivariate decision tree method, where a tree is grown using all predictor variables in the model. Here a generalizable tree is obtained by using two different methods of pruning a) Using a cost complexity parameter on the leaves b) Depth of the tree. Decision trees are fit on 10 fold cross validated samples to arrive at a generalizable decision tree. 

#### 5.8.1.1 Decision tree with complexity parameter tuning

The best tuning for cost parameter based on 10 fold cross validation was 0.015. Figure 6 shows the tuning and the importance of the various predictors. It is to be noted that count of delinquencies (DLQ_Count) and maximum delinquencies (DLQ_Max) were removed from the predictors list for the model shown below. When included in the model, all the trees zero in on DLQ_Total, DLQ_Count and DLQ_Max leaving very little signal from other predictors to come out. 

It can be seen that apart from total delinquencies (DLQ_Total) the standard deviation of bill amount (stdBILL), Average payment amount (Avg_PMT_Amt), Average payment ratio (Avg_PMT_RATIO) and maximum payment amount (Max_Pmt_Amt)  are key contributors to the model. Upon inspecting the decision tree in Figure 7, discretizing Avg_PMT_RATIO at cut points 0.12 could be useful for modeling purposes. Below further discretization of Avg_PMT_RATIO using WOE is explored. Avg_PMT_Amt cut points are not very clear from the decision tree. The standard deviation of bill cut point for discretization appears to be 1729 NT dollars

The WOE methods would be used to check or corroborate the decision trees and explore opportunities for further engineering these variables. 

```{r, eval = T,fig.cap = "left:Tuning results of Cost Complexity; right: variable importance plot",fig.height=4,fig.pos='asis'}

set.seed(10)

valid <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat,DEFAULT)

library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

x = train %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
xvalid <- valid %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
y = as.factor(train$DEFAULT)
yvalid <- as.factor(valid$DEFAULT)

levels(y) <- c("no","yes")
levels(yvalid) <- c("no","yes")
rpartFit <- caret::train(x = x, 
                  y = y,
                  method = "rpart",
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
                  )
stopCluster(cl)


gridExtra::grid.arrange(plot(rpartFit),plot(varImp(rpartFit),tl.cex = 5),ncol = 2)

```


\newpage
\blandscape

```{r, fig.cap="Decision tree based on cost parameter tuning",fig.height=9, fig.width=16, fig.pos='asis'}
partykit::as.party(rpartFit$finalModel) -> prty
library(partykit)
plot(prty,gp = gpar(fontsize = 10),
     inner_panel = node_inner,
     ip_args = list(
       abbreviate = F,
       id = F
     ),
     tp_args = list(
       id = F
     ))


```

\elandscape
\newpage

##### 5.8.1.1.1 Results of Decision tree with complexity parameter tuning

```{r}
getPerformance(x,xvalid,y,yvalid,rpartFit,title = "rpart decision tree - complexity cost tuning") -> rpartstats
```

Figure 7 confirms the interpretation of the splitting point of 0.03 and 0.12 for average payment ratio (or a singleton cut point of 0.12). The split points align with WOE for the average payment ratio (left panel of Fig 7). The average payment amount can be binned into 5 groups as shown in the right panel of figure 7. The splitting of total delinquencies as seen in the analysis in section 5.2 is reproduced in Figure 7. 

```{r,fig.cap="WOE binning of (top left) Avg_PMT_RATIO, (top right) Avg_PMT_Amt,(bottom left) Bill Standard Deviation, (bottom right) DLQ_Total",fig.pos='asis'}
Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(Avg_PMT_RATIO, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) ->payratioamt

Information::plot_infotables(payratioamt,"Avg_PMT_RATIO") + coord_flip() + theme_bw() + ggtitle("") + ylab("Avg_PMT_RATIO WOE") + geom_vline(xintercept = c(1.5,2.5),lty = 2, col = 'red') -> pymtratio

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(Avg_PMT_Amt, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->avgpayamt

Information::plot_infotables(avgpayamt,"Avg_PMT_Amt") + coord_flip() + theme_bw() + ggtitle("") + ylab("Avg_PMT_Amt WOE") -> avgpymt

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(stdBILL, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) -> stdbillwoe
Information::plot_infotables(stdbillwoe,"stdBILL") + coord_flip() + theme_bw() + ggtitle("") + ylab("stdBill WOE") + geom_vline(xintercept = c(2.5),lty = 2, col = 'red') -> stdbillplt

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Total, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Total") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Total WOE") -> p3

gridExtra::grid.arrange(pymtratio,avgpymt,stdbillplt,p3,ncol = 2)
```

#### 5.8.1.2 Decision tree with depth of tree tuning

When constraining the tree building process on the depth of the tree, the contribution of total delinquencies is very high relative to the other predictor variables. Figures 8 and 9 show the performance & variable importance and decision tree. The interpretation is consistent with the tree built in above section. 

```{r,eval = T,fig.cap="Decision tree based on depth of tree",fig.pos = "asis",message=F, warning=F}

# cl <- makePSOCKcluster(5)
# registerDoParallel(cl)
# 
# ctrl <- trainControl(method = "cv",
#                      summaryFunction = twoClassSummary,
#                      classProbs = TRUE)
# 
# x = train %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
# xvalid <- valid %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
# y = as.factor(train$DEFAULT)
# yvalid <- as.factor(valid$DEFAULT)
# levels(y) <- c("no","yes")
# levels(yvalid) <- c("no","yes")
# rpartFit2 <- caret::train(x = x, 
#                   y = y,
#                   method = "rpart2",
#                   tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#                   )
# stopCluster(cl)


gridExtra::grid.arrange(plot(rpartFit2),plot(varImp(rpartFit2),tl.cex = 5),ncol = 2)

```



```{r,eval = T, fig.cap = "Decision tree pruned by depth of tree", fig.pos="asis", message=F, warning = F}
partykit::as.party(rpartFit2$finalModel) -> prty
library(partykit)
plot(prty,gp = gpar(fontsize = 7),
     inner_panel = node_inner,
     ip_args = list(
       abbreviate = F,
       id = F
     ),
     tp_args = list(
       id = F
     ))
```


```{r}
getPerformance(x,xvalid,y,yvalid,rpartFit2,title = "rpart decision tree - depth of tree tuning") -> rpartstats2
```


#### 5.8.1.3 Decision tree with one parameter at a time.

In this section, trees are built using only one parameter per model. The goal is to get a baseline or null model whose performance would be lower threshold for any complex model built in the following sections. 

The best single predictor model is using DLQ_Count with an accuracy of 80.23%. But it can be seen the "No information Rate" is 77.4% (i.e a null model with no predictors). The DLQ_Count variable provides a gain of ~ 2.8 percentage points in accuracy. 

```{r, message = T,warning = F}
library(OneR)
xOneR= train %>% select(-Max_Bill_Amt,-DLQ_Max) %>% as.data.frame()
validOneR <- valid %>% select(-Max_Bill_Amt,-DLQ_Max) %>% as.data.frame()

levels(xOneR$DEFAULT) <- c("no","yes")
levels(validOneR$DEFAULT) <- c("no","yes")
#OneR(DEFAULT ~ .,data = train %>% select(-DLQ_Count)%>% as.data.frame(), verbose = T) -> baselinemod
OneR(DEFAULT ~ .,data = xOneR, verbose = F) -> baselinemod
summary(baselinemod)
#confusionMatrix(predict(baselinemod,newdata = train %>% as.data.frame()), train$DEFAULT,positive = "1")

getPerformance(xOneR,
               validOneR,xOneR$DEFAULT,validOneR$DEFAULT,baselinemod,title = "OneR Decision tree single predictor") -> baselinemodstats
```

#### 5.8.1.4 Re-Engineer features

Based on the discoveries above, the following vraiables are discretized

- Average Payment Ratio.
  + Cut points at 0.05 and 0.11. Therefore two additional variables are created, namely $Avg\_PMT\_RATIO\_X\_005$ - when equals 1 denote average payment ratio below 0.05 and $Avg\_PMT\_RATIO\_005\_011$ when 1 denote average payment ratio between 0.05 and 0.11. When both these variables are 0, it denotes avergae payment ratio above 0.11.
  
- Average Payment Amount.
  + Cut points at 910,1749,3333,6835. Four variables binary features were created to discretize the average payment amount.
  + The variables created are $AvgPMT\_0\_910$, $AvgPMT\_911\_1749$, $AvgPMT\_1750\_3333$,$AvgPMT\_3333\_6835$
  
- Standard deviation of bill.
  + cut point at 4521.27. Variable $stdBILL\_x\_4521$ was created to denote standard deviations less than or equal to 4521.27. 
  
- Total delinquencies.
  + Cut point at 2. Variable $DLQ\_Total\_x\_1$ was created to denote Total delinquencies less than 2.


```{r,message=F,warning = F}

#--------------------------------------------------------------------------
#                      RE-ENGINEER DATA 
#--------------------------------------------------------------------------

creditdata %<>% 
  mutate(AvgPMT_0_910 = ifelse(Avg_PMT_Amt >= 0 & Avg_PMT_Amt <= 910,1, 0),
         AvgPMT_911_1749 = ifelse(Avg_PMT_Amt > 910 & Avg_PMT_Amt <= 1749,1, 0),
         AvgPMT_1750_3333 = ifelse(Avg_PMT_Amt > 1749 & Avg_PMT_Amt <= 3333.17,1, 0),
         AvgPMT_3333_6835 = ifelse(Avg_PMT_Amt > 3333.17 & Avg_PMT_Amt <= 6835,1, 0)
        # AvgPMT_6336_LOT = ifelse(Avg_PMT_Amt >6835,1, 0)
         ) %>% 
  mutate(Avg_PMT_RATIO_X_005 = ifelse(Avg_PMT_RATIO < 0.05, 1, 0),
         Avg_PMT_RATIO_005_011 = ifelse(Avg_PMT_RATIO >= 0.05 & Avg_PMT_RATIO < 0.11,1,0)
         #Avg_PMT_RATIO_011_X = ifelse(Avg_PMT_RATIO > 0.011,1,0)
         ) %>% 
  mutate(stdBILL_x_4521 = ifelse(stdBILL <= 4521.27, 1,0),
         DLQ_Total_x_1 = ifelse(DLQ_Total < 2,1,0))

#--------------------------------------------------------------------------------
#               log pmt_amount & bill
# -------------------------------------------------------------------------------

creditdata %<>% 
  mutate(logBillAvg = log(ifelse(Avg_BILL_Amt<=0,0.001,Avg_BILL_Amt)),
         logPayAvg = log(ifelse(Avg_PMT_Amt<=0,0.001,Avg_PMT_Amt)),
         logppk = log(ifelse(ppk <=0, 0.0000001,ppk))
  )

#-------------------------------------------------------
#       re assign train data with new predictors 
#-------------------------------------------------------
PmtRatioBucketsCol <- creditdata %>% select(matches("Avg_PMT_RATIO_|AvgPMT_|stdBILL_|DLQ_Total_")) %>% colnames()
SelectedFeat2 <- c(SelectedFeat[!str_detect(SelectedFeat,"Avg_PMT|std|DLQ_Total")],PmtRatioBucketsCol)


train2 <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat2,DEFAULT)
levels(train2$DEFAULT) <- c("no","yes")

valid2 <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat2,DEFAULT)
levels(valid2$DEFAULT) <- c("no","yes")
```

#### 5.8.1.5 Re-run of Decision tree with one parameter at a time

The one predictor decision trees are built again with re-engineered features. The results are identical from the previous run. The DLQ_Count variables is further discretized with a cut point of 3. The new feature is named $DLQ\_Count\_3\_X$.


```{r}
#------------------------------------------------------------
#        re-run baseline model that includes new features
#------------------------------------------------------------

OneR(DEFAULT ~ .,data = train2 %>% as.data.frame(), verbose = F) -> baselinemod
summary(baselinemod)

c#onfusionMatrix(predict(baselinemod,newdata = train2 %>% as.data.frame()), train2$DEFAULT,positive = "yes")

getPerformance(train2 %>% as.data.frame(),valid2 %>% as.data.frame(),trainY = train2$DEFAULT, validY = valid2$DEFAULT,model = baselinemod, title = "OneR - single predictor decision tree", modelname = "OneR") -> basemodstats



```

#### 5.8.1.6 Dropping highly correlated variables

The features $Max\_Bill\_Amt$, $DLQ\_Max$ ,$utilFlag$ and $MARRIAGE.2$ (marital status - single) were not adding new information to the predictor space due to their correlation with other variables. Hence they were removes from the predictor space.

```{r,fig.cap="Correlations amongst predictors",fig.pos="asis"}
#--------------------------------------------------------------
#    Feature engineering to split DLQ_Count into 2 buckets
#--------------------------------------------------------------
creditdata %<>% 
  mutate(DLQ_Count_3_X = as.factor(ifelse(DLQ_Count > 2, 1,0)))

SelectedFeat3 <- c(SelectedFeat2[which(SelectedFeat2 == "DLQ_Count")*-1],"DLQ_Count_3_X")

#---------------------------
#   re-assign training & validation data
#-----------------------

train2 <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(train2$DEFAULT) <- c("no","yes")

valid2 <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(valid2$DEFAULT) <- c("no","yes")

train2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)

valid2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)

# -------------------------------------------------------
#     Find correlations
#---------------------------------------------------------

# marriage and education one hot encoded

predict(caret::dummyVars(~EDUCATION,data = train2,fullRank = T),newdata = train2) -> EDUDUM
predict(caret::dummyVars(~MARRIAGE,data = train2,fullRank = T),newdata = train2) -> MARDUM

corrplot::corrplot(cbind.data.frame(train2,EDUDUM,MARDUM) %>% select(-EDUCATION,-MARRIAGE,-DEFAULT) %>% mutate_if(is.factor,as.numeric) %>% cor(.),tl.cex = 0.5)
findCorrelation(cbind.data.frame(train2,EDUDUM,MARDUM) %>% select(-EDUCATION,-MARRIAGE,-DEFAULT) %>% mutate_if(is.factor,as.numeric) %>% cor(.)) -> corvars2remove
cbind.data.frame(train2,EDUDUM,MARDUM) %>% select(-EDUCATION,-MARRIAGE,-DEFAULT) %>% mutate_if(is.factor,as.numeric) %>% cor(.) %>% colnames() -> corcols

corcols[corvars2remove] -> removeCols

train2 %<>% select(-one_of(c(removeCols,'ppk')))

valid2 %<>% select(-one_of(c(removeCols,'ppk')))

```


### 5.8.2 Simple Logistic Regression model

The numerical variables (not the factor variables) in the predictor space was centered and scaled for ease of modeling and interpretation. A simple logistic regression model was fit. The model coefficients for the most features is intuitive, for example the males have 12% more odds than feamles to default. For one standard deviation increase in average utilization odds of default increases by 6%. Likewise, one standard deviation increase in average bill amount increases the odds for defaulting by 14%. Customers delinquent by 3 or more months have the odds increase by 2.75 times. Also with decreasing levels in the buckets of average payments the odds of default increases. However, the odds of defaults 15 - 17% lower for lower payment to bill ratios, which is very counterintuitive. 


```{r}

registerDoSEQ()


preProc <- preProcess(train2 %>% select_if(is.numeric),method = c("center","scale"))

predict(preProc,newdata = train2) -> trainProc
y_trainProc <- trainProc$DEFAULT
levels(y_trainProc) <- c("no","yes")

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = "final")

caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')], 
                  y = y_trainProc,
                  method = "glm",
                 family = "binomial",
                  #tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
      
                  )  ->  logitmdl2

broom::tidy(logitmdl2$finalModel) %>% 
  mutate(oddsRatio = exp(estimate)) %>% 
  kable("latex",digits = 4, caption = "Summary of Simple Logistic regression model")
```

#### 5.8.2.1  Simple Logistic Regression model Performance

The performance table below shows an improved AUC than the single predictor decision trees, howrver the F1 statistic is poorer. 

```{r, fig.cap }
#confusionMatrix(predict(logitmdl2),y_trainProc,positive = "yes")
logit2.roc <- pROC::roc(response = y_trainProc, predictor = predict(logitmdl2$finalModel))

# -------------------------------------------
#        Get the validation data ready
#--------------------------------------------


validProc <- predict(preProc,newdata = valid2)
validY <- validProc$DEFAULT
levels(validY) <- c("no","yes")

perf.logit <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],validProc[,-which(names(trainProc) %in% 'DEFAULT')],y_trainProc,validY,logitmdl2$finalModel, title = "Simple Logistic regression")

#perf.logit$Perf %>% kable("latex",digits = 2)
```




## 6. Model based Feature Selection

In this section model based feature selection method is employed for better choosing the features for further modeling. Here the following strategies are used for feature selection

  1. Regression based methods
    + logistic regression with stepwise feature selection.
    + Lasso regression (binomial distribution)
    
  2. Tree based methods
  
    + Random forest
    + Extreme Gradient Boosting (xgBoost)
    
The importance of features in all the modles are examined and features are selected based on importance amongst all the models for further modeling.

** All models are tuned based on 10 fold cross validated samples on the training data set and tested on the validation data **
    


### 6.1 Logistic model with stepwise feature selection

```{r, warning=F, message = F, fig.cap= "feature imporance based on stepwise selection "}

# registerDoSEQ()
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')], 
#                   y = y_trainProc,
#                   #tuneLength = 10,
#                   method = "glmStepAIC",
#                   family = "binomial",
#                   metric = "ROC",
#                   trControl = ctrl,
#              verbose = F
#       
#                   )  ->  glmLogit



broom::tidy(glmLogit$finalModel) %>% 
  kable("latex",digits = 4, caption = "")

#glmLogit.perf$Perf

data.frame(Preds = rownames(varImp(glmLogit$finalModel)), Importance = varImp(glmLogit$finalModel)) %>% 
  arrange(desc(Overall)) %>% top_n(12) %>% 
  ggplot(aes(x=fct_reorder(Preds,Overall), y = Overall)) + geom_point() + geom_col(width = .1) + coord_flip() + theme_bw() +
  xlab("Predictors") + ggtitle("Top 12 predictors - Stepwise selection - Logit")

```

#### 6.1.1 Performance of stepwise logit model

The model though has a AUC of 77%, the sensitivity is 16% with F1 as 0.27 for test and train. Different threshold probability cut off to classify as defaulting customer was explored, however 0.5 threshold had the maximum F1.

```{r, fig.cap = "Stepwise logit regression"}
glmLogit.perf <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],validProc[,-which(names(validProc) %in% 'DEFAULT')],y_trainProc,validY,glmLogit$finalModel, title = "Stepwise AIC logistic regression")
```

```{r, eval = F}
thresholder(glmLogit,threshold = seq(0.1,0.9,by = 0.1)) ->thdcal_glmLogit
thdcal_glmLogit %>% 
  ggplot(aes(x=prob_threshold,y = F1
             )) + geom_point()

```


### 6.2 Lasso regression

The important predictors are more or less identical to the stepwise selection model above. 

```{r,fig.cap = "variable importance"}
# registerDoSEQ()
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')] %>% data.matrix(), 
#                   y = y_trainProc,
#                   method = "glmnet",
#                   family = "binomial",
#                   metric = "ROC",
#                   trControl = ctrl,
#              tuneGrid = expand.grid(alpha = 1,
#                                    lambda = seq(0.001,0.1,by = 0.001))
#             
#       
#                   )  ->  glmnetLogit

# lassoPred <- coef(glmnetLogit$finalModel,glmnetLogit$bestTune$lambda) %>% as.matrix()
# colnames(lassoPred) <- "Coef"
# 
# Importance <- data.frame(Pred = rownames(lassoPred),Coef = lassoPred[,1])

Importance %>% top_n(10) %>% ggplot(aes(x=fct_reorder(Pred,abs(Coef)), y = abs(Coef))) + 
  geom_col(width = 0.1) + geom_point() + coord_flip() + 
  theme_bw() + xlab("Predictors") + ggtitle("Lasso - variable importance (top 10)")
```

#### 6.2.1 Lasso regression performance


```{r}
glmnetLogit.perf <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')] %>% data.matrix(),validProc[,-which(names(validProc) %in% 'DEFAULT')] %>% data.matrix(),y_trainProc,validY,glmnetLogit, title = "Lasso regression")

# thresholder(glmnetLogit,threshold = seq(0.1,0.7,by = 0.1)) ->thdcal_glmnetLogit
# thdcal_glmnetLogit %>% 
#   ggplot(aes(x=prob_threshold,y = F1
#              )) + geom_point()

```


### 6.3 Random forest 

Random forest methods were tuned with different predictors for each iteration. The below figure shows the tuning results and the top 20 important features. The threshold (probability cut off) to predict the positive class (defaulters) were tuned for the training data by optimizing it for the F1 score. While a threshold of 0.4 yielded the best result in the training data, it did not yield a significant improvement in the validation data. Hence the threshold was retained at its defualt of 0.5.

```{r,fig.cap = "Random forest, feature selection",fig.pos = "asis"}
# cl <- makePSOCKcluster(5)
# registerDoParallel(cl)
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')], 
#                   y = y_trainProc,
#                   tuneLength = 10,
#                   method = "rf",
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   )  ->  rf
# stopCluster(cl)
# 
# plot(rf,main = "Random forest tuning") -> rfTuning
# plot(varImp(rf),top = 20, main = "Random forest top 20 predictors",tl.cex = 0.5, grid = F) -> rfVarImp

gridExtra::grid.arrange(rfTuning,rfVarImp, ncol = 2)


```

#### 6.3.1 Random forest performance

```{r}
#perf.rf <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],validProc[,-which(names(validProc) %in% 'DEFAULT')],y_trainProc,validY,rf$finalModel, cutoff = 0.4,title = "Random forest")
#thresholder(rf,threshold = seq(0.1,0.7,by = 0.1)) ->thdcal_rf
# thdcal_rf %>%
#   ggplot(aes(x=prob_threshold,y = F1
#              )) + geom_point()
#perf.rf$Perf
perf.rf$Plots
```




```{r,eval = F}
recipes::recipe(DEFAULT ~ ., data = trainProc) -> recep
recep <- recipes::step_mutate(recep,Avg_BILL_Amt = ifelse(Avg_BILL_Amt <= 0, 0.0001, Avg_BILL_Amt))
recep %<>% recipes::step_YeoJohnson(Avg_BILL_Amt)
recep %<>% recipes::step_scale(Avg_BILL_Amt)
recipes::prep(recep) -> finalrecep
recipes::bake(finalrecep,trainProc) -> trainProc2

recipes::bake(finalrecep,validProc) -> validProc2
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
caret::train(x = trainProc2[,-which(names(trainProc) %in% 'DEFAULT')], 
                  y = y_trainProc,
                  tuneLength = 10,
                  method = "rf",
                  metric = "ROC",
                  trControl = ctrl
      
                  )  ->  rf2
stopCluster(cl)

getPerformance(trainProc2[,-which(names(trainProc2) %in% 'DEFAULT')],validProc2[,-which(names(validProc2) %in% 'DEFAULT')],y_trainProc,validY,rf2$finalModel)

```

```{r,eval = F}
#plot(varImp(rf2))
```

### 6.5 xgboost

An extreme gradient boosting model was tuned with a constant learning rate of 10% and L2 regularization of 0. The model was tuned over the depth of trees and 10% to 90% of the predictor space. The tuning was performed over 100% and 75% of the training samples. Figures below shows the model tuning results and the variable importance. 

```{r}

library(xgboost)
#---------------------------------
#       Convert data to matrix
#----------------------------------

trainProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> trainProcMat

validProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> validProcMat



#---------------------------------
#     tuning grid  
#---------------------------------
 
xgb_trcontrol <- trainControl(
  method = "cv",
  number = 5,
  allowParallel = TRUE,
  verboseIter = FALSE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
 
xgbGrid <- expand.grid(
 
  nrounds = c(100,200),
  max_depth = c(2,5,10,15),
  eta = 0.1,
  gamma = 0,
  #lambda = c(0,0.05,0.1,0.5),
  min_child_weight=1,
  subsample= c(0.75,1),
  colsample_bytree = seq(0.1,0.9,length.out = 5)
  
 
)
 
 
 
library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
 
xgb_model <- caret::train(
  trainProcMat[,-which(colnames(trainProcMat) %in% 'DEFAULT')],y_trainProc,
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  #objective = "binary:logistic",
  metric =  "ROC"
)
 
 
 
# xgb <- xgboost(data = xymat,
#                label = y,
#                eta = 0.1,
#                max_depth = 10,
#                subsample = 0.5,
#                colsample_bytree = 0.5,
#                seed = 123,
#                eval_metric = )
 
stopCluster(cl)
```

```{r, fig.cap = "xgBoost tuning"}
plot(xgb_model, main = "xgboost tuning")
```

```{r, fig.cap = "xgBoost variable importance"}
plot(varImp(xgb_model),top = 20, main = "Top 20 predictors - xgboost") 

#gridExtra::grid.arrange(xgbtuning,varImpxgb,ncol = 2)


```

```{r}
#xgb.Perf <- getPerformance(trainProcMat[,-which(colnames(trainProcMat) %in% 'DEFAULT')],validProcMat[,-which(colnames(validProcMat) %in% 'DEFAULT')],y_trainProc,validY,xgb_model$finalModel, title = "xgBoost performance")
xgb.Perf$Plots
```




```{r,eval = F}

thresholder(glmnetLogit,threshold = seq(0.1,0.9,by = 0.1)) ->thdcal

thdcal %>% 
  ggplot(aes(x=prob_threshold,y = F1
             )) + geom_point()
```

## 7. Selected Features

The union of the top 10 predictors from the models fit in section 6 are as below

```{r}
# LassoPreds <- Importance %>% top_n(8,abs(Coef))
# xgbPreds <- data.frame(Preds = rownames(varImp(xgb_model)$importance),Imp = varImp(xgb_model)$importance) %>% top_n(10)
# rf2Preds <- data.frame(Preds = rownames(varImp(rf2)$importance),Imp = varImp(rf2)$importance) %>% top_n(10)
# varImp(glmLogit$finalModel)->ImpglmLogit
# glmPreds <- data.frame(Preds = as.character(rownames(ImpglmLogit)), Importance = ImpglmLogit$Overall)
# glmPreds$Preds <- as.character(glmPreds$Preds)
# glmPreds %<>% mutate(Preds = ifelse(str_detect(Preds,'EDUCATION'),'EDUCATION',
#                                     ifelse(str_detect(Preds,'SEX'),'SEX',
#                                            ifelse(str_detect(Preds,'MARRIAGE'),'MARRIAGE',Preds))))
# #glmPreds$Preds <- as.character(glmPreds$Preds)
# # clean glmPreds
# which(str_sub(glmPreds$Preds,nchar(as.character(glmPreds$Preds)))=="1") -> cleanReq
# glmPreds$Preds[cleanReq] <- substr(as.character(glmPreds$Preds)[cleanReq],start = 1,stop = nchar(as.character(glmPreds$Preds)[cleanReq])-1)
# glmPreds %<>% arrange(desc(Importance)) 
# glmPredictors <- glmPreds %>% top_n(10) %>% .$Preds
# rfPreds <- data.frame(Preds = rownames(varImp(rf)$importance),Imp = varImp(rf)$importance) %>% top_n(10)
# 
# # manual extraction of the preds rather than intersection for now
# Preds <-union(union(union( union(LassoPreds$Pred[-1],xgbPreds$Preds),rf2Preds$Preds),rfPreds$Preds),glmPredictors)

#trainProc %>% select(!!Preds) %>% mutate_if(is.factor,as.numeric) %>% cor() -> corsPred

#corrplot::corrplot(corsPred,tl.cex = .7,method = "number")

matrix(c(Preds,""),ncol = 3) %>% 
  kable("latex",caption = "Predictors chosen for modeling") %>% 
  kable_styling(latex_options = "hold_position")
```


```{r,eval=F}
caret::train(x = trainProc[,which(names(trainProc) %in% c("DLQ_Total_x_1", "DLQ_Count_3_X"))], 
                  y = y_trainProc,
                  method = "glm",
                 family = "binomial",
                  #tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
      
                  ) -> simpleLogit

getPerformance(trainProc[,which(names(trainProc) %in% c("DLQ_Total_x_1", "DLQ_Count_3_X"))],validProc[,which(names(validProc) %in% c("DLQ_Total_x_1", "DLQ_Count_3_X"))],y_trainProc,validY,simpleLogit$finalModel)
```

## 8. Modeling

In this section the predictors identified from the previous section are used to build classification model to predict defaulting customers. The models are fit and their appropriate statistics are shown in this section.

### 8.1 Logistic regression 

The summary of the logistic regression makes intuitive sense for the most part. For example one standard deviation increase in avergae bill amount leads to 11% increased odds of default. Increased balance over 6 months on credit card increases odds of  default by 7%. Measure of balance as a distance to credit limit decreases in 3 standard deviations decreases (the distances were replaced by weights of evidence - ppkwoe), the odds of default increases by 17%. As the average monthly payments decreases the odds of default increases from 24% to 114% (see odds ratio of AvgPMT_0_910 to AvgPMT_1750_3333). As the number of delinquencies increase beyond 3, the odds of defaulting increases almost 3 times. The lower levels of standard deviations of bill amounts have surprisingly higher odds of default (27%), this could be potentially interpreted as customers only paying the interest and leaving the principal amount on their credit card. 

```{r}
# registerDoSEQ()
# 
# caret::train(x = trainProc[,which(names(trainProc) %in% Preds)], 
#                   y = y_trainProc,
#                   method = "glm",
#                  family = "binomial",
#                   #tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   ) -> logitSelectedPred

#summary(logitSelectedPred$finalModel) %>% pander()


broom::tidy(logitSelectedPred$finalModel) %>% 
  mutate(oddsRatio = exp(estimate)) %>% 
  kable("latex",digits = 4,caption = "logistic regression coefficients and odds- ratio")
```

#### 8.1.1 Logistic regression model performance

The sensitivity of the model is poor at 15%. In the previous sections, it was seen xgBoost and lasso regression had close to 30% sensitivity. The F1 score of the model is 0.27. 

```{r}
#getPerformance(trainProc[,which(names(trainProc) %in% Preds)],validProc[,which(names(validProc) %in% Preds)],y_trainProc,validY,logitSelectedPred$finalModel,title = "Logit model with selected features") -> logitSelectedPredPerf

logitSelectedPredPerf$Plots
```

#### 8.1.2 Improved logistic regression model

The statistically insignificant features are now removed and the logistic regression model is re-fit. The sensitivity improves by a 1% point. 

```{r}
# ctrl <- trainControl(method = "cv",
#                      summaryFunction = twoClassSummary,
#                      classProbs = TRUE,
#                      savePredictions = "final")
# PredsLogit <- setdiff(Preds,c("PerUTILinc","perBALinc","AVG_UTIL","Max_Pmt_Amt","payFlag","UTIL_Growth_6mo","Avg_BILL_Amt","Avg_UTIL"))
# df <- trainProc
# df_valid <- validProc
# df$ppkwoe <- as.factor(df$ppkwoe)
# df_valid$ppkwoe <- as.factor(df_valid$ppkwoe)
# caret::train(x = df[,PredsLogit], 
#                   y = y_trainProc,
#                   method = "glm",
#                  family = "binomial",
#                   #tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   ) -> logitSelectedPred2

#getPerformance(df[,PredsLogit],df_valid[,PredsLogit],y_trainProc,validY,logitSelectedPred2$finalModel, title = "Logit refit") -> logitSelectedPredPerf2

logitSelectedPredPerf2$Plots

```
### 8.2 Naive Bayes 

Though our predictor space contains features that are not independant of each other, a Naive Bayes classifier is fit to the data to evaluate its performance. The AUC is comparable to the rest of the models, but Naive Bayes' Sensitivity is as high as the xgBoost's model with a high specificity. The F1 score is 0.5, much higher than all other models. 

```{r}
# search_grid <- expand.grid(
#   usekernel = c(TRUE, FALSE),
#   fL = 0:5,
#   adjust = seq(0, 5, by = 1)
# )
# 
# caret::train(x = df[,Preds], 
#                   y = y_trainProc,
#                   method = "nb",
#                   metric = "ROC",
#                   trControl = ctrl,
#                   tuneGrid = search_grid
#       
#                   ) -> nbSelectedPred
# 
# #plot(nbSelectedPred)

#getPerformance(df[,-which(names(df) %in% 'DEFAULT')],df_valid[,-which(names(df_valid) %in% 'DEFAULT')],y_trainProc,validY,nbSelectedPred, title = "Naive Bayes model Performance") -> nbPerf
nbPerf$Plots
```


## 8.3 Neural network

A feed forward neural network with 1 input layer with as 3 hidden layers with drop out layers (rate of 10% drop out) after each hidden layer was chosen. With input layer having as many neurons as the number of chosen predictors and the output layer with a single neuron. All the hidden layers have ReLU activation and output layer has a sigmoid activation. The loss function that is to be minimized is the binary crossentropy. The model architecture is shown below.

```{r}
library(keras)

y_train_nn <-  ifelse(y_trainProc == "yes",1,0)
y_valid_nn <-  ifelse(validY=="yes",1,0)

# model <- keras_model_sequential() %>% 
#     # first hidden layer
#   layer_dense(units = 8,activation = "relu",input_shape = length(Preds)) %>% 
#     # add drop out layer
#   layer_dropout(rate = 0.1) %>% 
#   # second hidden layer
#   layer_dense(units = 4, activation = "relu") %>% 
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # third layer
#    layer_dense(units = 2, activation = "relu") %>% 
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # output layer
#   layer_dense(units = 1,activation = "sigmoid")
# 
# # creating custome metric
# metric_auc <- custom_metric("auc",function(y_true,y_pred){
#   K <- backend()
#   y_true <- K$eval(y_true)
#   y_pred <- K$eval(y_pred)
#   roccurve <- roc(response = y_true, predictor = y_pred)
#   aucval <- pROC::auc(roccurve)
#   K$constant(aucval,'float64')
# })
# 
# # compile the network
# model %>% 
# compile(
#   optimizer = 'adam',
#   loss = "binary_crossentropy",
#   metrics = c('accuracy')
# )
# 
# model


```


```{r}
# set.seed(10)
# history <- model %>% 
#   fit(trainProcMat[,Preds],
#       y_train_nn,
#       epochs = 200,
#       batch_size = 30,
#       validation_data = list(validProcMat[,Preds],y_valid_nn))
```

```{r}
# y_train_pred_prob <- predict_proba(model,x = trainProcMat[,Preds]) %>% as.vector()
# y_train_pred_class <- predict_classes(model,x = trainProcMat[,Preds]) %>% as.vector()
# estimates_keras_tbl <- tibble(
#   truth      = y_trainProc,
#   estimate   = as.factor(y_train_pred_class) %>% fct_recode(yes = "1",no = "0"),
#   class_prob = y_train_pred_prob
# )
# 
# y_valid_pred_prob <- predict_proba(model,x = validProcMat[,Preds]) %>% as.vector()
# y_valid_pred_class <- predict_classes(model,x = validProcMat[,Preds]) %>% as.vector()
# estimates_valid <- tibble(
#   truth      = validY,
#   estimate   = as.factor(y_valid_pred_class) %>% fct_recode(yes = "1",no = "0"),
#   class_prob = y_valid_pred_prob
# )
# 
# estimates_keras_tbl %>% yardstick::roc_auc(truth, class_prob)
# estimates_valid %>% yardstick::roc_auc(truth, class_prob)
# 
#  estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
#  estimates_valid %>% f_meas(truth, estimate, beta = 1)
#  
#  estimates_keras_tbl %>% metrics(truth, estimate)
#  estimates_valid %>% metrics(truth, estimate)
#  
#  tibble(
#   precision = estimates_keras_tbl %>% precision(truth, estimate),
#   recall    = estimates_keras_tbl %>% recall(truth, estimate)
# )
#  tibble(
#   precision = estimates_valid %>% precision(truth, estimate),
#   recall    = estimates_valid %>% recall(truth, estimate)
# )
 
```

```{r,eval =F}
auc_roc <- R6::R6Class("ROC", 
                       inherit = KerasCallback, 
                       public = list(
                             
                             losses = NULL,
                             x = NA,
                             y = NA,
                             x_val = NA,
                             y_val = NA,
                             
                             initialize = function(training = list(), validation= list()){
                                   self$x <- training[[1]]                                   
                                   self$y <- training[[2]]
                                   self$x_val <- validation[[1]]
                                   self$y_val <- validation[[2]]
                             },
                             
                             
                             on_epoch_end = function(epoch, logs = list()){
                                   
                                   self$losses <- c(self$losses, logs[["loss"]])
                                   y_pred <- self$model$predict(self$x)
                                   y_pred_val <- self$model$predict(self$x_val)
                                   score = Metrics::auc(actual = self$y, predicted =  y_pred)
                                   score_val = Metrics::auc(actual = self$y_val, predicted =  y_pred_val)
                                   print(paste("epoch: ", epoch+1, " roc:", score, ' roc_val:', score_val))
                             }    
                       ))

# auc_roc_metric <- auc_roc$new(training = list(trainProcMat[,Preds], y_train_nn), validation = list(validProcMat[,Preds],y_valid_nn))

# tensorboard("logs/run_f")

# history2 <- model %>% 
#   fit(trainProcMat[,Preds],
#       y_train_nn,
#       epochs = 100,
#       batch_size = 30,
#       validation_data = list(validProcMat[,Preds],y_valid_nn),
#       callbacks = list(auc_roc_metric,callback_tensorboard("logs/run_b",histogram_freq = 1,write_graph = T,write_grads = T)))
 
```

```{r}
# estimates_keras_tbl %>% yardstick::roc_auc(truth, class_prob)
# estimates_valid %>% yardstick::roc_auc(truth, class_prob)
# 
#  estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
#  estimates_valid %>% f_meas(truth, estimate, beta = 1)
#  
#  estimates_keras_tbl %>% metrics(truth, estimate)
#  estimates_valid %>% metrics(truth, estimate)
#  
#  tibble(
#   precision = estimates_keras_tbl %>% precision(truth, estimate),
#   recall    = estimates_keras_tbl %>% recall(truth, estimate)
# )
#  tibble(
#   precision = estimates_valid %>% precision(truth, estimate),
#   recall    = estimates_valid %>% recall(truth, estimate)
# )
```

```{r,eval =F}
# take the train data and set it for neural network
recipes::recipe(DEFAULT ~ ., data = trainProc) -> recep
#recep <- recipes::step_mutate(recep,Avg_BILL_Amt = ifelse(Avg_BILL_Amt <= 0, 0.0001, Avg_BILL_Amt))
#recep %<>% recipes::step_YeoJohnson(Avg_BILL_Amt)
#recep %<>% recipes::step_scale(Avg_BILL_Amt)
recep %<>% 
  recipes::step_dummy(EDUCATION)

recep %<>% 
  recipes::step_dummy(MARRIAGE)

recipes::prep(recep) -> finalrecep
recipes::bake(finalrecep,trainProc) -> trainProc2

recipes::bake(finalrecep,validProc)-> validProc2

trainProc2 %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> trainProcMat2

validProc2 %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> validProcMat2


```


```{r}
# model2 <- keras_model_sequential() %>% 
#     # first hidden layer
#   layer_dense(units = 8,activation = "relu",input_shape = ncol(trainProcMat2)-1) %>% 
#     # add drop out layer
#   layer_dropout(rate = 0.1) %>% 
#   # second hidden layer
#   layer_dense(units = 4, activation = "relu") %>% 
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # third layer
#    layer_dense(units = 2, activation = "relu") %>% 
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # output layer
#   layer_dense(units = 1,activation = "sigmoid")
# 
# # creating custome metric
# metric_auc <- custom_metric("auc",function(y_true,y_pred){
#   K <- backend()
#   y_true <- K$eval(y_true)
#   y_pred <- K$eval(y_pred)
#   roccurve <- roc(response = y_true, predictor = y_pred)
#   aucval <- pROC::auc(roccurve)
#   K$constant(aucval,'float64')
# })
# 
# # compile the network
# model2 %>% 
# compile(
#   optimizer = 'adam',
#   loss = "binary_crossentropy",
#   metrics = c('accuracy')
# )

model2
```

```{r,eval = F}
auc_roc_metric <- auc_roc$new(training = list(trainProcMat2[,-27], y_train_nn), validation = list(validProcMat2[,-27], y_valid_nn))

tensorboard("logs/run_z")

history3 <- model2 %>%
  fit(trainProcMat2[,-27],
      y_train_nn,
      epochs = 100,
      batch_size = 30,
      validation_data = list(validProcMat2[,-27],y_valid_nn),
      callbacks = list(auc_roc_metric,callback_tensorboard("logs/run_z",histogram_freq = 1,write_graph = T,write_grads = T)))

# history3 <- model2 %>% 
#   fit(trainProcMat2[,-27],
#       y_train_nn,
#       epochs = 50,
#       batch_size = 30,
#       validation_data = list(validProcMat2[,-27],y_valid_nn))


y_train_pred_prob2 <- predict_proba(model2,x = trainProcMat2[,-27]) %>% as.vector()
y_train_pred_class2 <- predict_classes(model2,x = trainProcMat2[,-27]) %>% as.vector()


y_valid_pred_prob2 <- predict_proba(model2,x = validProcMat2[,-27]) %>% as.vector()
y_valid_pred_class2 <- predict_classes(model2,x = validProcMat2[,-27]) %>% as.vector()



estimates_keras_tbl2 <- tibble(
  truth      = y_trainProc,
  estimate   = as.factor(y_train_pred_class2) %>% fct_recode(yes = "1",no = "0"),
  class_prob = y_train_pred_prob2
)

confusionMatrix(estimates_keras_tbl2$estimate,estimates_keras_tbl2$truth, positive = "yes") -> trainNNConf


estimates_valid2 <- tibble(
  truth      = validY,
  estimate   = as.factor(y_valid_pred_class2) %>% fct_recode(yes = "1",no = "0"),
  class_prob = y_valid_pred_prob2
)

confusionMatrix(estimates_valid2$estimate,estimates_valid2$truth, positive = "yes") -> validNNConf

# estimates_keras_tbl2 %>% yardstick::roc_auc(truth, class_prob)
# estimates_valid2 %>% yardstick::roc_auc(truth, class_prob)
```

```{r, fig.cap = "Neural Network training"}
plot(history3)+ggtitle("Neural Network training and validation statistics") + theme_bw()

```

