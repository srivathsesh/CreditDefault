---
title: "Credit Default Prediction"
author: "Sri Seshadri"
date: "9/28/2019"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \usepackage{setspace}\doublespacing
- \usepackage{float}
includes:
  in_header:header.tex
  

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
library(kfigr)
```

\newpage

# 1. Introduction (DRAFT)

This paper discusses the various modeling approaches to predict customers defaulting on their credit card payments by using six months' credit card bill amounts, payment history. A working model that takes in customer data and reports a list of customers at risk of default and the amount of money that is at risk. The following are the key insights that were discovered from the data:

  1. X % increase in monthly bill for a customer paying bills at a rate of y% of the monthly bill has Z% increased risk compared to average customer.
  2. 



# 2. Data

Data are available for 30,000 customers spanning between April 2005 and September 2005. The description of the data are shown in Table 1 below. Table 2 and Table 3 show the general summary of the data. Upon inspection of the data summaries, there were data quality issues identified, that are discussed in the following section.

```{r, warning = F, message = F}
library(tidyverse)
library(caret)
library(magrittr)
library(pander)
library(knitr)
library(kableExtra)
library(matrixStats)
library(OneR)
source('getPerformance.R')
load('checkpoint3.RData')
#setting seed

set.seed(10)

#------------------------------------------------------
#               READ DATA IN
#---------------------------------------------------------

creditdata <- readRDS("credit_card_default.RData")

creditdata <- as.data.frame(creditdata) %>% as_tibble()
creditdata %<>% 
  mutate_at(c("DEFAULT","EDUCATION","MARRIAGE","SEX","data.group"),.funs = as.factor)

#-----------------------------------------------------
#              DATA DICTIONARY
#-----------------------------------------------------

Dict <- tibble(FEATURE = colnames(creditdata)[1:25],
               DESCRIPTION =
                 c("CUSTOMER ID",
                   "CREDIT LIMIT",
                   "GENDER",
                   "EDUCATION",
                   "MARITAL STATUS",
                   "AGE",
                   "REPAYMENT STATUS SEP 2005",
                   "REPAYMENT STATUS AUG 2005",
                   "REPAYMENT STATUS JUL 2005",
                   "REPAYMENT STATUS JUN 2005",
                   "REPAYMENT STATUS MAY 2005",
                   "REPAYMENT STATUS APR 2005",
                   "BILL SEP 2005",
                   "BILL AUG 2005",
                   "BILL JUL 2005",
                   "BILL JUN 2005",
                   "BILL MAY 2005",
                   "BILL APR 2005",
                   "PAYMENT SEP 2005",
                   "PAYMENT AUG 2005",
                   "PAYMENT JUL 2005",
                   "PAYMENT JUN 2005",
                   "PAYMENT MAY 2005",
                   "PAYMENT APR 2005",
                   "DEFAULTING CUSTOMER"
                  ),
               COMMENTS = 
                 c("",
                   "INCLUDES SUPPL CARDS",
                   "1 = MALE, 2 = FEMALE",
                   "1 = GRAD,2 = UNIV,3 = HIGH SCH,4 = OTHERS",
                   "1=MARRIED,2=SINGLE,3=OTHERS",
                   "YEARS",
                   "assumed SEP 2005 ; -1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "1 = DEFAULT, 0 = NON-DEFAULT"
                 )
           
       ) 

#*******************************************
#         REPORT DICTIONARY
#*******************************************

kable(Dict,format = "latex",align = "c",caption = "Data Dictionary") %>% 
  kable_styling(full_width = F,latex_options = "hold_position",font_size = 7) %>% 
  column_spec(1,border_left = T) %>% 
  column_spec(3,width = "20em",border_right = T) %>% 
  row_spec(0,bold = T) %>% 
  collapse_rows(valign = "middle")


# Get stats on the data
skimr::skim_with(numeric = list(hist = NULL),integer = list(hist = NULL))
skimr::skim_to_wide(creditdata) %>% 
  filter(type == "factor") %>% 
  filter(!variable %in% "data.group") %>% 
  select(variable,missing,complete,n,n_unique,top_counts) %>% 
  #mutate(top_counts = cell_spec(top_counts,"latex",align = "r")) %>% 
  kable(format = "latex",digits = 1,booktabs = T,caption = "categorical variables summary") %>% 
  kable_styling(latex_options = "hold_position")

skimr::skim_to_wide(creditdata) %>% 
  filter(type == "integer") %>% 
  select(variable,missing,complete,n,mean,sd,p0,p25,median,p75,p100) %>% 
  #mutate(top_counts = cell_spec(top_counts,"latex",align = "r")) %>% 
  kable(format = "latex",digits = 1,booktabs = T,caption = "numeric variables summary") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)

```

## 2.1 Data Issues

1. Table 2 shows the variable $EDUCATION$ to have 7 possible values, while the dictionary in table 1 shows us expect 4 possible values. The values 0,5 and 6 would be assigned to a category "unknown", coded as 9.

2. Similarly for 54 customers, the value of $MARRIAGE$ is entered as 0. The 0 values are assumed to be "unknown" marital status.

3. The variable PAY_0 is assumed to be the repayment status as of September 2005. Hence renamed as PAY_1

4. Repayment status has values that include -2 and 0. Approximately 26,000 customers have repayment status that is not described in the dictionary. It is very likely that the data dictionary had an omission in describing -2 and 0. Therefore they are assumed to be non-delayed payments.

5. There are 543 customers in the data who are marked delinquent when the average bill over 6 months is 0. It is not clear if the customers defaulted in the months prior to the 6 months of data that is provided.


```{r,eval=T}

# remap unknown educaton to code 9
creditdata %<>% 
  mutate(EDUCATION = as.numeric(EDUCATION)-1) %>% 
  mutate(EDUCATION = ifelse(EDUCATION %in% c(0,5,6),9,EDUCATION)) %>% 
  mutate(EDUCATION = as.factor(EDUCATION)) 

# rename PAY_0 to PAY_1
creditdata %<>% 
  rename(PAY_1 = PAY_0)

creditdata %>% 
  group_by(data.group,DEFAULT) %>% 
  summarise(VAR = sum(BILL_AMT1,na.rm = T)) %>% 
  ungroup() %>% 
  filter(DEFAULT ==1)-> VARs

trainVAR <- VARs$VAR[VARs$data.group == 1]
validVAR <- VARs$VAR[VARs$data.group == 2]
testVAR <- VARs$VAR[VARs$data.group == 3]
```

## 2.2 Data Splitting

The data was separated into three groups for model training, validation and testing purposes. The number of customers in each of these groups are shown below in table 4.

```{r,eval=T}
creditdata %>% 
  mutate(data.group = ifelse(data.group == 1, "Train", ifelse(data.group == 2, "Test", "Validate"))) %>% 
  group_by(data.group) %>% 
  summarise(datapoints = n()) %>% 
  knitr::kable(caption = "Train,Validation and Test splits",format = "latex",booktabs = T) %>% 
  kable_styling(latex_options = c("striped","hold_position"))


```


# 3. Feature Engineering

Table 5 shows the features that are derived from the data along with their name and calculation method. The AGE and ppk (the distance between credit limit and average bill in units of 3 time standard deviations of bill amounts) variables are continuous variables that are binned using Weights Of Evidence (WOE) method.

It is interesting to note that the WOE indicates that the customers in higher age group (over 39 years) have relatively higher risk of default compared to customers in the late twenties. Possibly due to potential loss in employment and difficulty to secure an income source. This hypothesis will be explored in the exploratory data analysis section below.

ppk values were binned and replaced with their respective WOE. `r figr('AGEppkBinning', TRUE, type="Figure")` shows the binning of these variables.


\newpage
\blandscape

```{r,eval=T}
# EngFeats <- data.frame(
#   Feature = c(
#     "Payment to Bill ratio",
#     "Average Payment to Bill ratio",
#     "Proportion of credit used in the month",
#     " Month's in/decrease in credit use",
#     "Average proportion of credit used",
#     "Average Montly bill",
#     "Month's in/decrease in bill",
#     "Average amount paid",
#     "Month's in/decrease in payment",
#     "Balance growth over 6 months",
#     "Credit utilization growth 6 months",
#     "Month's in/decrease in Payment ratio",
#     "Customer with increased utilization over time",
#     "Customer with increased bill over time",
#     "Customer with reduced payments time",
#     "Customer with reduced payment/bill over time",
#     "Max bill",
#     "Max payment",
#     "Max delinquency",
#     "Age of customers between mentioned range",
#      "Ppk -Dist to credit limit in units of 3 std.dev of Bill",
#     "Ppk discretized with WOE values"
#     
#     
#   ),
#   `Column_Names` = c(
#     "PMT_RATIO2 - PMT_RATIO6",
#     # "$PMT\\_RATIO3$",
#     # "$PMT\\_RATIO4$",
#     # "$PMT\\_RATIO5$",
#     # "$PMT\\_RATIO6$",
#     
#     "$Avg\\_PMT\\_RATIO$",
#     
#     "$UTIL\\_1$ to $UTIL\\_6$ ",
#     # "$UTIL\\_2$",
#     # "$UTIL\\_3$",
#     # "$UTIL\\_4$",
#     # "$UTIL\\_5$",
#     # "$UTIL\\_6$",
#     
#     "$UTILMR1$ to $UTILMR5$",
#     # "$UTILMR4$",
#     # "$UTILMR3$",
#     # "$UTILMR2$",
#     # "$UTILMR1$",
#   
#    
#     "$Avg\\_UTIL$",
#     
#     "$Avg\\_BILL\\_Amt$",
#     
#      "$BILLAMTMR1$ to $BILLAMTMR5$ ", 
#      # "$BILLAMTMR4$",
#      # "$BILLAMTMR3$",
#      # "$BILLAMTMR2$",
#      # "$BILLAMTMR1$",
#     
#      "$Avg\\_PMT\\_Amt$",
#     
#       "$PAYMR1$ to $PAYMR5$",
#       # "$PAYMR4$",
#       # "$PAYMR3$",
#       # "$PAYMR2$",
#       # "$PAYMR1$",
#     
#     "$Bal\\_Growth\\_6mo$", 
#     
#     "$UTIL\\_Growth\\_6mo$", 
#     
#          "$PMT\\_RATIOMR2$ to $PMT\\_RATIOMR5$", 
#          # "$PMT\\_RATIOMR4$",
#          # "$PMT\\_RATIOMR3$", 
#          # "$PMT\\_RATIOMR2$", 
#     
#     "$UtilFlag$",
#     
#     '$balFlag$',
#     
#     "$payFlag$",
#     
#     
#     "$payRatioFlag$",
#     
#     "$Max\\_Bill\\_Amt$",
#     
#     "$Max\\_Pmt\\_Amt$",
#     
#     "$Max\\_Dlq$",
#     
#     "$Age\\_21\\_25$",
#     
#     "$ppk$",
#     
#     "$ppkwoe$"
#     
#     
#     
#   ),
#   Calculation = c(' $\\frac{PAY\\_AMT_{mnth-1}}{BILL\\_AMT{mnth}}$ $mnth \\in {2,3,4,5,6}$',
#                   # ' $\\frac{PAY\\_AMT2}{BILL\\_AMT3}$',
#                   # ' $\\frac{PAY\\_AMT3}{BILL\\_AMT4}$',
#                   # ' $\\frac{PAY\\_AMT4}{BILL\\_AMT5}$',
#                   # ' $\\frac{PAY\\_AMT5}{BILL\\_AMT6}$',
#   
#                   ' $\\frac{\\sum_{Month=1}^{5}PAY\\_AMT_{mnth}}{\\sum_{mnth=2}^{6}BILL\\_AMT_{mnth}}$',
#                   
#                   ' $\\frac{BILL\\_AMT_{mnth}}{LIMIT\\_BAL}$ $mnth \\in {1,2,3,4,5,6}$',
#                   # ' $\\frac{BILL\\_AMT2}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT3}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT4}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT5}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT6}{LIMIT\\_BAL}$',
#                   
#                   '$UTIL_{mnth-1}- UTIL_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$UTIL\\_4 - UTIL\\_5$',
#                   # '$UTIL\\_3 - UTIL\\_4$',
#                   # '$UTIL\\_2 - UTIL\\_3$',
#                   # '$UTIL\\_1 - UTIL\\_2$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}UTIL_{mnth}}{5}$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}BILL\\_AMT_{mnth}}{5}$',
#                   
#                   '$BILL\\_AMT_{mnth-1} - BILL\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$BILL\\_AMT4 - BILL\\_AMT5$',
#                   # '$BILL\\_AMT3 - BILL\\_AMT4$',
#                   # '$BILL\\_AMT2 - BILL\\_AMT3$',
#                   # '$BILL\\_AMT1 - BILL\\_AMT2$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}PAY\\_AMT_{mnth}}{5}$',
#                   
#                   
#                   
#                   '$PAY\\_AMT_{mnth-1} - PAY\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$PAY\\_AMT4 - PAY\\_AMT5$',
#                   # '$PAY\\_AMT3 - PAY\\_AMT4$',
#                   # '$PAY\\_AMT2 - PAY\\_AMT3$',
#                   # '$PAY\\_AMT1 - PAY\\_AMT2$',
#                   
#                   "$BILL\\_AMT_{mnth-1} - BILL\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$",
#                   
#                   "$UTIL\\_1 - UTIL\\_6$",
#                   
#                   "$PMT\\_RATIO5 - PMT\\_RATIO6$",
#                   # "$PMT\\_RATIO4 - PMT\\_RATIO5$",
#                   # "$PMT\\_RATIO3 - PMT\\_RATIO4$",
#                   # "$PMT\\_RATIO2 - PMT\\_RATIO3$",
#                   
#                   "$\\forall {UTILMR5,UTILMR4,UTILMR3,UTILMR2,UTILMR1} > 0$",
#                   
#                   "$\\forall {BILLAMTMR5,BILLAMTMR4,BILLAMTMR3,BILLAMTMR2,BILLAMTMR1} > 0$",
#                   
#                   "$\\forall{PAYMR5,PAYMR4,PAYMR3,PAYMR2,PAYMR1} > 0$",
#                   
#                   "$\\forall{PMT\\_RATIOMR5,PMT\\_RATIOMR4,PMT\\_RATIOMR3,PMT\\_RATIOMR2} > 0$",
#                   
#                   '$max(BILL\\_AMT1,BILL\\_AMT2,BILL\\_AMT3,BILL\\_AMT4,BILL\\_AMT5,BILL\\_AMT6)$',
#                   
#                   '$max(PAY\\_AMT1,PAY\\_AMT2,PAY\\_AMT3,PAY\\_AMT4,PAY\\_AMT5,PAY\\_AMT6)$',
#                   
#                   '$max(PAY\\_1,PAY\\_2,PAY\\_3,PAY\\_4,PAY\\_5,PAY\\_6)$',
#                   
#                   '$if AGE>=21 AND AGE<=25 then 1 else 0$',
#                   
#               '$\\frac{(LIMIT\\_BAL - \\overline{BILL\\_AMT_{mth}})}{\\sqrt{\\sum_{mth=1}^6\\frac{(BILL\\_AMT_{mth} - \\overline{BILL\\_AMT_{mth}})^2}{5}}}$',
#                   
#                   'ppk binned and replaced by WOE'
#                   
#                   
#                   
#                   
#                   
#                   
#   )
#   
# 
# 
# )

knitr::kable(data.frame(l = ""),"latex", escape = F,booktabs = T,caption = "Engineered features") 

```

\elandscape
\newpage


```{r,eval=T}

# ------------------------------------------------------------
# PAY RATIO defined as payment over bill 
# Make the PAY_RATIO as 1 when the buill and pay amounts are 0
#-------------------------------------------------------------
creditdata %<>% 
  mutate(PMT_RATIO2 = ifelse(PAY_AMT1==0 & BILL_AMT2==0|BILL_AMT2 <= 0,1,PAY_AMT1/BILL_AMT2),
         PMT_RATIO3 = ifelse(PAY_AMT2==0 & BILL_AMT3== 0|BILL_AMT3 <= 0,1,PAY_AMT2/BILL_AMT3),
         PMT_RATIO4 = ifelse(PAY_AMT3==0 & BILL_AMT4==0|BILL_AMT4 <= 0,1,PAY_AMT3/BILL_AMT4),
         PMT_RATIO5 = ifelse(PAY_AMT4==0 & BILL_AMT5==0|BILL_AMT5 <= 0,1,PAY_AMT4/BILL_AMT5),
         PMT_RATIO6 = ifelse(PAY_AMT5==0 & BILL_AMT6==0|BILL_AMT6 <= 0,1,PAY_AMT5/BILL_AMT6)) %>% 
  mutate(
         Avg_PMT_RATIO = ifelse(rowSums(select(.,matches("BILL_AMT[2-6]")))==0,1,rowSums(select(.,matches("PAY_AMT[1-5]")))/rowSums(select(.,matches("BILL_AMT[2-6]")))))


```



```{r,eval=T}
library(matrixStats)

# Utilization defined as a % usage of credit limit. When there is a negative bill the util is made 0.

creditdata %<>% 
  mutate(UTIL_1 = ifelse(BILL_AMT1/LIMIT_BAL < 0 , 0 , BILL_AMT1/LIMIT_BAL),
         UTIL_2 = ifelse(BILL_AMT2/LIMIT_BAL < 0 , 0 , BILL_AMT2/LIMIT_BAL),
         UTIL_3 = ifelse(BILL_AMT3/LIMIT_BAL < 0 , 0 , BILL_AMT3/LIMIT_BAL),
         UTIL_4 = ifelse(BILL_AMT4/LIMIT_BAL < 0 , 0 , BILL_AMT4/LIMIT_BAL),
         UTIL_5 = ifelse(BILL_AMT5/LIMIT_BAL < 0 , 0 , BILL_AMT5/LIMIT_BAL),
         UTIL_6 = ifelse(BILL_AMT6/LIMIT_BAL < 0 , 0 , BILL_AMT6/LIMIT_BAL))
```



```{r,eval=T}
#--------------------------------------------------------------------------------------------------------------
# While we that the median balance increases over time, how much of the population is causing this to increase
# Calculate the difference between subsequent months in UTIL and BIll_AMTs
#--------------------------------------------------------------------------------------------------------------

creditdata %<>% 
  mutate(Avg_UTIL = rowMeans(select(.,starts_with("UTIL"))),
         UTILMR5 = UTIL_5 - UTIL_6,
         UTILMR4 = UTIL_4 - UTIL_5,
         UTILMR3 = UTIL_3 - UTIL_4,
         UTILMR2 = UTIL_2 - UTIL_3,
         UTILMR1 = UTIL_1 - UTIL_2,
         Avg_BILL_Amt = rowMeans(select(.,starts_with("BILL_AMT")),na.rm = T),
         stdBILL = ifelse(rowSds(select(.,starts_with("BILL_AMT")) %>% as.matrix()) == 0, 0.01,rowSds(select(.,starts_with("BILL_AMT")) %>% as.matrix()))) %>% 
  mutate(ppk = (LIMIT_BAL - Avg_BILL_Amt)/(3*stdBILL),
         BILLAMTMR5 = BILL_AMT5 - BILL_AMT6,
         BILLAMTMR4 = BILL_AMT4 - BILL_AMT5,
         BILLAMTMR3 = BILL_AMT3 - BILL_AMT4,
         BILLAMTMR2 = BILL_AMT2 - BILL_AMT3,
         BILLAMTMR1 = BILL_AMT1 - BILL_AMT2,
         Avg_PMT_Amt = rowMeans(select(.,starts_with("PAY_AMT")),na.rm = T))


creditdata %<>%
  mutate(PAYMR5 = PAY_AMT5 - PAY_AMT6,
         PAYMR4 = PAY_AMT4 - PAY_AMT5,
         PAYMR3 = PAY_AMT3 - PAY_AMT4,
         PAYMR2 = PAY_AMT2 - PAY_AMT3,
         PAYMR1 = PAY_AMT1 - PAY_AMT2)

creditdata %<>% 
  mutate(Bal_Growth_6mo = BILL_AMT1 - BILL_AMT6,
         UTIL_Growth_6mo = UTIL_1 - UTIL_6,
         PMT_RATIOMR5 = PMT_RATIO5 - PMT_RATIO6,
         PMT_RATIOMR4 = PMT_RATIO4 - PMT_RATIO5,
         PMT_RATIOMR3 = PMT_RATIO3 - PMT_RATIO4,
         PMT_RATIOMR2 = PMT_RATIO2 - PMT_RATIO3
         )


  

# Customers with increasing utilization over time
creditdata %>% filter_at(vars(starts_with("UTILM")),all_vars(. > 0)) %>% select(ID) ->IDs

# customers with increasing Balance over time
creditdata %>% filter_at(vars(starts_with("BILLAMTMR")),all_vars(. > 0)) %>% select(ID) ->BalIDs

# customers with decreasing payments over time

creditdata %>% filter_at(vars(starts_with("PAYMR")),all_vars(. < 0)) %>% select(ID) ->payIDs

# customers with decreasing payment ratio over time

creditdata %>% filter_at(vars(starts_with("PMT_RATIOMR")),all_vars(. < 0)) %>% select(ID) ->payRatioIDs

creditdata %<>% 
  mutate(utilFlag = ifelse(ID %in% IDs$ID,1,0))

creditdata %<>% 
  mutate(balFlag = ifelse(ID %in% BalIDs$ID,1,0))

creditdata %<>% 
  mutate(payFlag = ifelse(ID %in% payIDs$ID,1,0))

creditdata %<>% 
  mutate(payRatioFlag = ifelse(ID %in% payRatioIDs$ID,1,0))





## Get percentage increase between month 6 and month 1

creditdata %<>%
  mutate(PerBALinc = ifelse(!is.infinite((BILL_AMT1 - BILL_AMT6)/BILL_AMT6)&!is.na((BILL_AMT1 - BILL_AMT6)/BILL_AMT6),(BILL_AMT1 - BILL_AMT6)/BILL_AMT6,(BILL_AMT1 - BILL_AMT6)/0.01),
         PerUTILinc = ifelse(!is.infinite((UTIL_1 - UTIL_6)/UTIL_6)&!is.na((UTIL_6 - UTIL_1)/UTIL_6),(UTIL_1- UTIL_6)/UTIL_6,(UTIL_1 - UTIL_6)/0.0001)
         )

```



```{r,eval=T}
creditdata %<>% 
  mutate(Max_Bill_Amt = rowMaxs(select(.,matches("BILL_AMT[0-9]")) %>% as.matrix()),
         Max_Pmt_Amt = rowMaxs(select(.,matches("PAY_AMT[0-9]")) %>% as.matrix()))
```


```{r,eval=T}

# Customers delinquent often
# creditdata %<>%
#   mutate(Max_Dlq = ifelse(rowMaxs(select(.,matches("PAY_[0-9]")) %>% as.matrix()) < 0,0,rowMaxs(select(.,matches("PAY_[0-9]")) %>% as.matrix()) ))

creditdata %>% 
  select(ID,matches("PAY_[0-9]")) %>% 
  gather(key = "Period", value = "DLQ",-ID) %>% 
  mutate(DLQ = ifelse(DLQ < 0, 0,DLQ)) %>% 
  group_by(ID) %>% 
  summarise(DLQ_Count = count(DLQ > 0),
            DLQ_Total = sum(DLQ),
            DLQ_Max = max(DLQ)) -> Delequencies

creditdata %<>% 
  left_join(Delequencies)

#colnames(creditdata)
  
```


```{r AGEppkBinning,eval=T,fig.cap="WOE binning",fig.height=3,fig.pos='asis'}
# creditdata %<>% 
#   mutate(AgeBins = cut(AGE,breaks = seq(from = 25, to = 80,by = 5)))
#   
# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 7) -> tst7
# 
# Information::plot_infotables(tst7,"AGE")

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 6) -> tst6

Information::plot_infotables(tst6,"AGE") + coord_flip()->p1


# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) -> tst5
# 
# Information::plot_infotables(tst5,"AGE")
# 
# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) -> tst4
# 
# Information::plot_infotables(tst4,"AGE")

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(ppk, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) -> tst5ppk

Information::plot_infotables(tst5ppk,"ppk") + coord_flip()->p2

gridExtra::grid.arrange(p1,p2,ncol=2,top = grid::textGrob("WOE binning"))

```


```{r,eval=F}
library(OneR)

OneR::optbin(as.factor(DEFAULT) ~ AGE, data = creditdata, method = "infogain")-> optbin

plot(optbin)

```


```{r,eval=T}


creditdata %<>% 
  mutate(AGE_21_25 = ifelse(AGE>=21 & AGE<=25,1,0),
         AGE_26_29 = ifelse(AGE>=26 & AGE<=29,1,0),
         AGE_30_33 = ifelse(AGE>=30 & AGE<=33,1,0),
         AGE_34_38 = ifelse(AGE>=34 & AGE<=38,1,0),
         AGE_39_44 = ifelse(AGE>=39 & AGE<=44,1,0))

creditdata %<>% 
  mutate(ppkwoe = ifelse(ppk<0.49,0.30368328,
                         ifelse(ppk < 1.82,-0.06824883,
                                ifelse(ppk < 6.98,-0.08848357,
                                       ifelse(ppk < 27.3,-0.29885247,0.09571116)))))
#colnames(creditdata)
```


```{r,eval=F}
table(creditdata$DEFAULT,creditdata$utilFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$utilFlag))


table(creditdata$DEFAULT,creditdata$balFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$balFlag))

table(creditdata$DEFAULT,creditdata$payFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$payFlag))
```

## 3.1 Data Quality of Engineered Features

The use of very small values like a penny when denominator dollar amounts are 0 prevented the feature from going to an infinite value.Table 6 and 7 Shows the summary of the engineered features.


```{r}

EngFeatNames <- creditdata %>% select(matches("RATIO|Avg|UTIL|MR|6mo|Flag|Max|Age_|ppk|inc|DLQ|stdB|EDUCATION|SEX|MARRIAGE")) %>% colnames()
SelectedFeat <- EngFeatNames[!str_detect(EngFeatNames,"[A-Z][0-9]|UTIL_[0-9]")]

creditdata %>% 
  select(!!EngFeatNames) %>%
  select_if(is.numeric) %>% 
  skimr::skim_to_wide() %>% 
  select(-type,-sd,-mean) %>% 
  kable("latex",digits = 2,booktabs = T,caption = "Engineered features summary - Numerical",longtable = T) %>% 
  kable_styling(latex_options = "repeat_header",font_size = 7)

creditdata %>% 
  select(!!EngFeatNames) %>%
  select_if(is.factor) %>% 
  skimr::skim_to_wide() %>% 
  #select(-type,-sd,-mean) %>% 
  kable("latex",digits = 2,booktabs = T,caption = "Engineered features summary - categorical",longtable = T) %>% 
  kable_styling(latex_options = "repeat_header",font_size = 7)

```

# 4. Exploratory Data Analysis (EDA).

In this section the training data are analyzed to explore potential relationships between the predictor and propensity of default. Table 8 shows the distribution of the defaulting customers among the 15180 customers.

```{r train}
train <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat,DEFAULT)
data.frame(prop.table(table(train$DEFAULT))) %>% 
  rename("DEFAULT" = Var1) %>% 
  kable("latex", booktabs = T, digits = 2,caption = "Distribution of Defaulting customers in training data") %>% 
  kable_styling(latex_options = "hold_position")
```

## 4.1 Magnitude of revolving credit

It is hypothesized that the magnitude of revolving credit is a reflection of income of customers and income has an effect on credit card default. There is no evidence of magnitude of revolving credit or income having an influence on default. In instances where either the payment or the bill amount were $<=0$ the value was assumed to be one-tenth of a New Taiwan(NT) penny. `r figr('marginalPlot', TRUE, type="Figure")` shows the log of the amounts for easier visualization. 

```{r marginalPlot,fig.cap="Effect of revolving credit on default",fig.height=4,fig.pos='asis',fig.height=3}
train %>% 
  ggplot(aes(x=log(ifelse(Avg_BILL_Amt<=0,0.001,Avg_BILL_Amt)),y=log(ifelse(Avg_PMT_Amt<=0,0.001,Avg_PMT_Amt)),col = DEFAULT)) + 
  xlab("log(Avg Bill Amount)") +
  ylab("log(Avg Payment Amount)")+
  geom_point(alpha = 0.4) + 
  theme_bw() -> scp

ggExtra::ggMarginal(scp,type = "histogram",groupColour = T) -> mp

mp
```



## 4.2 Past Delinquencies in payment

Here the frequency of delinquency (how often customers are delinquent) in payments' effect on default is explored. It is surprising that there are customers who were not late on payment but were marked default. Its likely those customers though were not late, might have paid less than the minimum due. The second panel in the plot below explores the continued delinquency's effect on default. The higher total months late associates to increased odds of defaulting (thicker tails in the density plot). The weights of evidence plot on total & maximum delinquencies in `r figr('Delinquencies', TRUE, type="Figure")` shows increased chances of defaulting when total delinquency is past 1 month. This potentially indicates that the bank pardon's a single late payment.

```{r Delinquencies,fig.cap="Effect of Delinqucies",fig.height=4,fig.pos = 'asis',fig.show='asis'}
train %>% 
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,1,0))) %>% 
  ggplot(aes(x=as.numeric(DLQ_Count),group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Frequency of Delinquency (DLQ_Count)") ->p1


train %>%
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,"Bill = 0","Bill > 0"))) %>%
  ggplot(aes(x=DLQ_Total,group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Total months late (DLQ_Total)") ->p2

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Total, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Total") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Total WOE") -> p3

train %>%
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,"Bill = 0","Bill > 0"))) %>%
  ggplot(aes(x=DLQ_Max,group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Max months late (DLQ_Max)") ->p4

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Max, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Max") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Max WOE") -> p5

gridExtra::grid.arrange(p1,p2,p3,p5, ncol = 2)

```

## 4.3 Utilization of credit limit

A process capability metric used in the quality engineering domain is used here to determine how close does the bill come to the credit limit in the units of 3 times standard deviation of the bills. Figure 1 in section 4, shows that ppk less that 0.5 have a higher odds (odds of 1.34) for defaulting. It is to be noted that there is an increase in the odds of default for customers who have higher ppk. It was hypothesized that these very low utilization of credit may be from higher age group of customers, but there is no solid evidence of that as seen in the second panel of `r figr('ppkUtilAge', TRUE, type="Figure")` . It is to be noted that for easier visualization the natural log of ppk was used with negative ppk (bills higher than credit limit) set to very low (1e-07) value. The bottom panel of the plot shows the effect due to the average utilization, 

```{r ppkUtilAge,fig.cap="Utilization of credit limit's effect on Default",fig.height=4,fig.pos = 'asis',fig.height=4}
train %>% ggplot(aes(x=log(ifelse(ppk <=0, 0.0000001,ppk)),group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("log(ppk)")-> u1
creditdata %>% filter(train==1) %>%  ggplot(aes(x=AGE, y = log(ifelse(ppk <=0, 0.0000001,ppk)), col = DEFAULT)) + geom_point(alpha = 0.4) + theme_bw() + ylab("log(ppk)")-> u2
train %>% ggplot(aes(y=Avg_UTIL,x = DEFAULT,col = DEFAULT)) + geom_boxplot() + theme_bw()-> u3
#gridExtra::tableGrob(broom::glance(chisq.test(utilflagtbl))) -> u4
gridExtra::grid.arrange(u1,u2,u3,ncol = 2)
```


The customers who have  continuous growth in utilization over 6 months is flagged and analyzed. The chi-squared test of independence shows there is a statistical difference. 


```{r}
table(train$DEFAULT,train$utilFlag) -> utilflagtbl
prop.table(utilflagtbl) %>% 
  data.frame() %>% rename("utilFlag" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Continous growth in utilization's effect on DEFAULT - ChiSq test") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(utilflagtbl) %>% pander()

```


## 4.4 Effect of balances

The effect of increased balances is quantified in the tables 11 & 12 below. The customers whose balances increases over time (month after month) are flagged and analyzed for the odds in default. The chi squared test of independence show there is a statistical significance due to increasing credit card balance over time. 

```{r}
table(train$DEFAULT,train$balFlag) -> balflagtbl
prop.table(balflagtbl) %>% 
  data.frame() %>% rename("balFlag" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Continous growth in balance's effect on DEFAULT - ChiSq test") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(balflagtbl) %>% pander()
```


## 4.5 Effect of decreased payments

Payments as a ratio of bill amounts, average payment amounts, continuous decrease in payments/pay ratios over time did not yield statistically significant results when explored graphically using density plots. However, in the section 5.8.1.1, decision trees help bring out signal from payment variables. The influence of payments are discussed in section 5.8.1.2 below.

## 4.6 Effect of Age

Figure 1 in section 4 shows relatively higher chances of defaulting in the early ages (21-24) and in the age group past mid-life 45 years and above. The increased chances in default past mid-life may be potentially attributed to loss in employment and or health related events in life.



## 4.7 Effect of Gender, Education and Marital Status

Effect of SEX on default is explored. In the data, 60% of the samples are of females (coded as 2). It can be seen from `r figr('GenderEduMS', TRUE, type="Figure")` that proportions of defaulters are in each group are comparable (25% for males and 21% for females). However the chi-squared test shows the difference in the proportions as statistically significant. This may be likely due to the sensitivity to high number of samples in the training set. The importance of SEX in predicting the defaulters is explored in the model based EDA section below. 

The customers with graduate degree (coded 1), university education (coded 2) and high school diploma (coded 3) have 20%,24% and 26% defaulters respectively. The other and unknown categories have 7% and 8%  defaulters. The effect of education is statistically significant.

Customers who are married, single and marital status as others have 24%,21% and 25% defaulters respectively. The interaction of Gender and Education within every marital status was explored graphically in Figure 5, interactions were not found for the major marital status levels. 

Chi-squared tests show gender, education and marital status are significant. As mentioned above, this may be due to sample size being high and small differences are being deemed statistically significant.  



```{r GenderEduMS,fig.cap = "GENDER,EDUCATION & MARITAL STATUS' effect on DEFAULT",fig.pos='asis',fig.height=3.5,fig.show='asis'}
lattice::barchart(prop.table(table(train$SEX,train$DEFAULT)),ylab = list(label ="SEX",cex = 0.75),scales = list(x = list(cex =0.5)),xlab = list(cex = 0.75)) -> bp1
lattice::barchart(prop.table(table(train$EDUCATION,train$DEFAULT)),grid=T,ylab = list(label ="EDUCATION",cex = 0.75),scales = list(cex = 0.5),xlab = list(cex = 0.75)) -> bp2
lattice::barchart(prop.table(table(train$MARRIAGE,train$DEFAULT)),grid=T,ylab = list(label="MARRIAGE",cex=0.75),scales = list(cex = 0.5),xlab = list(cex = 0.75)) -> bp3


library(ggmosaic)

ggplot(data = train) + 
  geom_mosaic( aes(x=product(EDUCATION,SEX),fill = DEFAULT)) + 
  facet_wrap(~MARRIAGE) + xlab("DEFAULT:SEX") + ylab("EDUCATION") + 
  ggtitle("DEFAULT by SEX & EDUCATION by martial status") +
  theme(plot.title = element_text(size = 8), 
        axis.title.x= element_text(size = 6),
        axis.title.y= element_text(size = 6),
        legend.title = element_text(size =4),
        legend.text =  element_text(size = 3),
        legend.key.size = unit(1,"mm"),
        strip.text.x = element_text(margin = margin(0.175,0,0.175,0,"mm")),
        axis.text.x = element_text(size = 4),
        axis.text.y = element_text(size = 4)
        )->msp1

gridExtra::grid.arrange(bp1,bp2,bp3,msp1,ncol = 2,nrow= 2)
```


```{r}
table(train$DEFAULT,train$SEX) -> SEXtbl
prop.table(SEXtbl) %>% 
  data.frame() %>% rename("SEX" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "GENDER effect on DEFAULT - ChiSq test",digits = 2) %>% 
  kable_styling(latex_options = "hold_position",font_size = 6)
chisq.test(SEXtbl) %>% pander()
```

```{r}
table(train$DEFAULT,train$EDUCATION) -> EDUtbl
prop.table(EDUtbl) %>% 
  data.frame() %>% rename("EDUCATION" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "EDUCATION effect on DEFAULT - ChiSq test",digits = 2) %>% 
  kable_styling(latex_options = "hold_position",font_size = 6)
chisq.test(EDUtbl) %>% pander()
```

\newpage

```{r}

table(train$DEFAULT,train$MARRIAGE) -> MARtbl
prop.table(MARtbl) %>% 
  data.frame() %>% rename("EDUCATION" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Marital Status' effect on DEFAULT - ChiSq test",digits = 2) %>% 
  kable_styling(latex_options = "hold_position",font_size = 6)
panderOptions('graph.fontsize',7)
chisq.test(MARtbl) %>% pander()

```



## 4.8 Model based EDA

In this section, two methods are employed to gain more insights on the effects of the predictor variables on chances of default. 

- Tree based methods.
- Simple logistic regression.


### 4.8.1 Tree based methods.

Multivariate decision tree method, where a tree is grown using all predictor variables in the model. Here a generalizable tree is obtained by using two different methods of pruning a) Using a cost complexity parameter on the leaves b) Depth of the tree. Decision trees are fit on 10 fold cross validated samples to arrive at a generalizable decision tree. 

#### 4.8.1.1 Decision tree with complexity parameter tuning

The best tuning for cost parameter based on 10 fold cross validation was 0.015. `r figr('rpart1', TRUE, type="Figure")` shows the tuning and the importance of the various predictors. It is to be noted that count of delinquencies (DLQ_Count) and maximum delinquencies (DLQ_Max) were removed from the predictors list for the model shown below. When included in the model, all the trees zero in on DLQ_Total, DLQ_Count and DLQ_Max leaving very little signal from other predictors to come out. 

It can be seen that apart from total delinquencies (DLQ_Total) the standard deviation of bill amount (stdBILL), Average payment amount (Avg_PMT_Amt), Average payment ratio (Avg_PMT_RATIO) and maximum payment amount (Max_Pmt_Amt)  are key contributors to the model. Upon inspecting the decision tree in `r figr('rparttree', TRUE, type="Figure")`, discretizing Avg_PMT_RATIO at cut points 0.12 could be useful for modeling purposes. Below further discretization of Avg_PMT_RATIO using WOE is explored. Avg_PMT_Amt cut points are not very clear from the decision tree. The standard deviation of bill cut point for discretization appears to be 1729 NT dollars

The WOE methods would be used to check or corroborate the decision trees and explore opportunities for further engineering these variables. 

```{r rpart1, eval = T,fig.cap = "left:Tuning results of Cost Complexity; right: variable importance plot",fig.pos='asis',fig.height = 3.5}

set.seed(10)

valid <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat,DEFAULT)
test <- creditdata %>% filter(test==1) %>% select(!!SelectedFeat,DEFAULT)

library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

x = train %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
xvalid <- valid %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
xtest <- test %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
y = as.factor(train$DEFAULT)
yvalid <- as.factor(valid$DEFAULT)
ytest <- as.factor(test$DEFAULT)

levels(y) <- c("no","yes")
levels(yvalid) <- c("no","yes")
levels(ytest) <- c("no","yes")
rpartFit <- caret::train(x = x, 
                  y = y,
                  method = "rpart",
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
                  )
stopCluster(cl)


gridExtra::grid.arrange(plot(rpartFit),plot(varImp(rpartFit),top = 15),ncol = 2)

```


\newpage
\blandscape

```{r rparttree, fig.cap="Decision tree based on cost parameter tuning",fig.height=9, fig.width=16, fig.pos='asis'}
partykit::as.party(rpartFit$finalModel) -> prty
library(partykit)
plot(prty,gp = gpar(fontsize = 4),
     inner_panel = node_inner,
     ip_args = list(
       abbreviate = F,
       id = F
     ),
     tp_args = list(
       id = F
     ))


```

\elandscape
\newpage



`r figr('woes', TRUE, type="Figure")` confirms the interpretation of the splitting point of 0.03 and 0.12 for average payment ratio (or a singleton cut point of 0.12). The split points align with WOE for the average payment ratio (left panel of `r figr('woes', TRUE, type="Figure")`). The average payment amount can be binned into 5 groups as shown in the right panel of the figure. The splitting of total delinquencies as seen in the analysis in section 5.2 is reproduced in Figure `r figr('woes', TRUE, type="Figure")`. 

```{r woes,fig.cap="WOE binning of (top left) Avg_PMT_RATIO, (top right) Avg_PMT_Amt,(bottom left) Bill Standard Deviation, (bottom right) DLQ_Total",fig.pos='asis',fig.height=4}
Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(Avg_PMT_RATIO, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) ->payratioamt

Information::plot_infotables(payratioamt,"Avg_PMT_RATIO") + coord_flip() + theme_bw() + ggtitle("") + ylab("Avg_PMT_RATIO WOE") + geom_vline(xintercept = c(1.5,2.5),lty = 2, col = 'red') -> pymtratio

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(Avg_PMT_Amt, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->avgpayamt

Information::plot_infotables(avgpayamt,"Avg_PMT_Amt") + coord_flip() + theme_bw() + ggtitle("") + ylab("Avg_PMT_Amt WOE") -> avgpymt

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(stdBILL, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) -> stdbillwoe
Information::plot_infotables(stdbillwoe,"stdBILL") + coord_flip() + theme_bw() + ggtitle("") + ylab("stdBill WOE") + geom_vline(xintercept = c(2.5),lty = 2, col = 'red') -> stdbillplt

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Total, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Total") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Total WOE") -> p3

gridExtra::grid.arrange(pymtratio,avgpymt,stdbillplt,p3,ncol = 2)
```


##### 4.8.1.1.1 Performance of Decision tree with complexity parameter tuning

`r figr('model1', TRUE, type="Figure")` shows the performance metrics of the decision tree model in classifying the defaulters. The model has F1 score of 0.45,0.39 and 0.4 for training, validation and test data sets respectively. The sensitivity of the model is 0.33 and 0.29 for training and validation set respectively. There is opportunity for improvement in capturing more of the defaulters. Here another metric to measure the model performance is introduced; % Value At Risk recovered (%VAR recovered). It is the ratio of the total currency value the model identifies as at risk due to defaulting customers and the actual total currency value that has been defaulted on by defaulting customers. The % VAR when greater than 1, implies that model is identifying risk thats greater than actual, similary less than 1 indicates model identifying risk less than the actual. The model identifies risk 4%, 7% and 18% higher than actual for the training, validation and test data sets. 

```{r model1ROC, eval = F, fig.show='hold',fig.cap = "decision tree complexity tuning performance",fig.height=4,fig.pos="asis",fig.height=4}
#getPerformance(x,xvalid,y,yvalid,rpartFit,title = "rpart decision tree - complexity cost tuning",trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1],validBILL = creditdata$BILL_AMT1[creditdata$data.group==3]) -> rpartstats
#getPerformance(x,xtest,y,ytest,rpartFit,title = "rpart decision tree - complexity cost tuning",trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1],validBILL = creditdata$BILL_AMT1[creditdata$data.group==2]) -> rparttest
```

```{r model1,fig.cap = "model1 - decision tree cost complexity tuning"}
knitr::include_graphics('modelResults/model1.png')
```


### 4.8.1.2 Decision tree with depth of tree tuning

When constraining the tree building process on the depth of the tree, the contribution of total delinquencies is very high relative to the other predictor variables. `r figr('varImpDT', TRUE, type="Figure")` show the performance & variable importance of the decision tree. The interpretation is consistent with the tree built in above section. 

```{r varImpDT,eval = T,fig.cap="Decision tree based on depth of tree",fig.pos = "asis",message=F, warning=F,fig.height=3}

# cl <- makePSOCKcluster(5)
# registerDoParallel(cl)
# 
# ctrl <- trainControl(method = "cv",
#                      summaryFunction = twoClassSummary,
#                      classProbs = TRUE)
# 
# x = train %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
# xvalid <- valid %>% select(-DEFAULT,-Max_Bill_Amt,-DLQ_Max)
# y = as.factor(train$DEFAULT)
# yvalid <- as.factor(valid$DEFAULT)
# levels(y) <- c("no","yes")
# levels(yvalid) <- c("no","yes")
# rpartFit2 <- caret::train(x = x, 
#                   y = y,
#                   method = "rpart2",
#                   tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#                   )
# stopCluster(cl)


gridExtra::grid.arrange(plot(rpartFit2),plot(varImp(rpartFit2),tl.cex = 5,top = 10),ncol = 2)

```



```{r depthTree,eval = T, fig.cap = "Decision tree pruned by depth of tree", fig.pos="asis", message=F, warning = F,fig.height=3}
partykit::as.party(rpartFit2$finalModel) -> prty
library(partykit)
plot(prty,gp = gpar(fontsize = 7),
     inner_panel = node_inner,
     ip_args = list(
       abbreviate = F,
       id = F
     ),
     tp_args = list(
       id = F
     ))
```


```{r,eval = F}
# getPerformance(x,xvalid,y,yvalid,rpartFit2,title = "rpart decision tree - depth of tree tuning",trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1],validBILL = creditdata$BILL_AMT1[creditdata$data.group==3]) -> rpartstats2
#getPerformance(x,xtest,y,ytest,rpartFit2,title = "rpart decision tree - depth of tree tuning",trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1],validBILL = creditdata$BILL_AMT1[creditdata$data.group==2]) -> rparttest2

```

`r figr('depthTree', TRUE, type="Figure")` shows the depth of tree tuned decision tree and  `r figr('model2', TRUE, type="Figure")` shows its performance. It's performance is identical to complexity tuned model. However the F1 score on the validation and test sets are slightly better in this model. The % VAR recovered metrics is within two percentage points compared to the tree tuned based on complexity parameter.

```{r model2,fig.cap = "model 2 - decision tree depth of tree tuning"}
knitr::include_graphics('modelResults/model2.png')
```

#### 4.8.1.3 Decision tree with one parameter at a time.

In this section, trees are built using only one parameter per model. The goal is to get a baseline or null model whose performance would be lower threshold for any complex model built in the following sections. The best single predictor model is using DLQ_Count with an accuracy of 80.23%. "No information Rate" is 77.4% (i.e a null model with no predictors). The DLQ_Count variable provides a gain of ~ 2.8 percentage points in accuracy. `r figr('oneR1', TRUE, type="Figure")` shows the summary of the result. `r figr('oneRPerf', TRUE, type="Figure")` shows the performance of one predictor decision tree, the performance is slightly better in terms of F1 score (0.44 on train and validation data and 0.42 on test data) and AUC (67%,68% and 67% in train, validation and test data sets respectively). The model's overestimation of value at risk is in par with the multivariate decision trees discussed in the previous section.

```{r oneR1,fig.cap = "oneR decison tree summary", message = T,warning = F,out.width="300px",out.height="300px"}
library(OneR)
# xOneR= train %>% select(-Max_Bill_Amt,-DLQ_Max) %>% as.data.frame()
# validOneR <- valid %>% select(-Max_Bill_Amt,-DLQ_Max) %>% as.data.frame()
# testOneR <-  test %>% select(-Max_Bill_Amt,-DLQ_Max) %>% as.data.frame()
# 
# levels(xOneR$DEFAULT) <- c("no","yes")
# levels(validOneR$DEFAULT) <- c("no","yes")
# levels(testOneR$DEFAULT) <- c("no","yes")
# #OneR(DEFAULT ~ .,data = train %>% select(-DLQ_Count)%>% as.data.frame(), verbose = T) -> baselinemod
# OneR(DEFAULT ~ .,data = xOneR, verbose = F) -> baselinemod
# summary(baselinemod)
#confusionMatrix(predict(baselinemod,newdata = train %>% as.data.frame()), train$DEFAULT,positive = "1")
knitr::include_graphics('modelresults/oner1.png')
# getPerformance(xOneR,validOneR,xOneR$DEFAULT,validOneR$DEFAULT,baselinemod,title = "OneR Decision tree single predictor",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> baselinemodstats
#getPerformance(xOneR,testOneR,xOneR$DEFAULT,testOneR$DEFAULT,baselinemod_later,title = "OneR Decision tree single predictor",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) 
```

```{r oneRPerf,fig.cap = "model 3 - OneR model performance"}
knitr::include_graphics('modelResults/model3.png')
```

#### 4.8.1.4 Re-Engineer features

Based on the discoveries above, the following variables are discretized

- Average Payment Ratio.
    +   Cut points at 0.05 and 0.11. Therefore two additional variables are created, namely $Avg\_PMT\_RATIO\_X\_005$ - when equals 1 denote average payment ratio below 0.05 and $Avg\_PMT\_RATIO\_005\_011$ when 1 denote average payment ratio between 0.05 and 0.11. When both these variables are 0, it denotes average payment ratio above 0.11.
  
- Average Payment Amount.
    +   Cut points at 910,1749,3333,6835. Four variables binary features were created to discretize the average payment amount.
    +   The variables created are $AvgPMT\_0\_910$, $AvgPMT\_911\_1749$, $AvgPMT\_1750\_3333$,$AvgPMT\_3333\_6835$
  
- Standard deviation of bill.
    +   cut point at 4521.27. Variable $stdBILL\_x\_4521$ was created to denote standard deviations less than or equal to 4521.27. 
  
- Total delinquencies.
    +   Cut point at 2. Variable $DLQ\_Total\_x\_1$ was created to denote Total delinquencies less than 2.


```{r,message=F,warning = F}

#--------------------------------------------------------------------------
#                      RE-ENGINEER DATA 
#--------------------------------------------------------------------------

creditdata %<>% 
  mutate(AvgPMT_0_910 = ifelse(Avg_PMT_Amt >= 0 & Avg_PMT_Amt <= 910,1, 0),
         AvgPMT_911_1749 = ifelse(Avg_PMT_Amt > 910 & Avg_PMT_Amt <= 1749,1, 0),
         AvgPMT_1750_3333 = ifelse(Avg_PMT_Amt > 1749 & Avg_PMT_Amt <= 3333.17,1, 0),
         AvgPMT_3333_6835 = ifelse(Avg_PMT_Amt > 3333.17 & Avg_PMT_Amt <= 6835,1, 0)
        # AvgPMT_6336_LOT = ifelse(Avg_PMT_Amt >6835,1, 0)
         ) %>% 
  mutate(Avg_PMT_RATIO_X_005 = ifelse(Avg_PMT_RATIO < 0.05, 1, 0),
         Avg_PMT_RATIO_005_011 = ifelse(Avg_PMT_RATIO >= 0.05 & Avg_PMT_RATIO < 0.11,1,0)
         #Avg_PMT_RATIO_011_X = ifelse(Avg_PMT_RATIO > 0.011,1,0)
         ) %>% 
  mutate(stdBILL_x_4521 = ifelse(stdBILL <= 4521.27, 1,0),
         DLQ_Total_x_1 = ifelse(DLQ_Total < 2,1,0))

#--------------------------------------------------------------------------------
#               log pmt_amount & bill
# -------------------------------------------------------------------------------

creditdata %<>% 
  mutate(logBillAvg = log(ifelse(Avg_BILL_Amt<=0,0.001,Avg_BILL_Amt)),
         logPayAvg = log(ifelse(Avg_PMT_Amt<=0,0.001,Avg_PMT_Amt)),
         logppk = log(ifelse(ppk <=0, 0.0000001,ppk))
  )

#-------------------------------------------------------
#       re assign train data with new predictors 
#-------------------------------------------------------
PmtRatioBucketsCol <- creditdata %>% select(matches("Avg_PMT_RATIO_|AvgPMT_|stdBILL_|DLQ_Total_")) %>% colnames()
SelectedFeat2 <- c(SelectedFeat[!str_detect(SelectedFeat,"Avg_PMT|std|DLQ_Total")],PmtRatioBucketsCol)


train2 <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat2,DEFAULT)
levels(train2$DEFAULT) <- c("no","yes")

valid2 <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat2,DEFAULT)
levels(valid2$DEFAULT) <- c("no","yes")
```

#### 4.8.1.5 Re-run of Decision tree with one parameter at a time

The one predictor decision trees are built again with re-engineered features. The results are identical from the previous run. The DLQ_Count variables is further discretized with a cut point of 3. The new feature is named $DLQ\_Count\_3\_X$. `r figr('oner2', TRUE, type="Figure")` shows the summary of the result.


```{r oner2,fig.cap = "Single predictor decision tree re-run summary",out.height="300px",out.height="300px"}
#------------------------------------------------------------
#        re-run baseline model that includes new features
#------------------------------------------------------------

# OneR(DEFAULT ~ .,data = train2 %>% as.data.frame(), verbose = F) -> baselinemod
# summary(baselinemod)

#confusionMatrix(predict(baselinemod,newdata = train2 %>% as.data.frame()), train2$DEFAULT,positive = "yes")

# getPerformance(train2 %>% as.data.frame(),valid2 %>% as.data.frame(),trainY = train2$DEFAULT, validY = valid2$DEFAULT,model = baselinemod, title = "OneR - single predictor decision tree", modelname = "OneR") -> basemodstats

knitr::include_graphics('modelresults/oner2.png')

```

#### 4.8.1.6 Dropping highly correlated variables

A filtering based method is used to determine redundant features to be removed for modeling purposes. The features $Max\_Bill\_Amt$, $DLQ\_Max$ ,$utilFlag$ and $MARRIAGE.2$ (marital status - single) were not adding new information to the predictor space due to their correlation with other variables. Hence they were removes from the predictor space. `r figr('correlations', TRUE, type="Figure")` shows correlations amongst the features. It must be noted that some categorical variables were converted to numeric to observe correlations. The results of such an analysis must be carefully interpreted. In this case categorical variables with 2 levels are compared with categorical variables with 2 levels (1 and 0) for determination of redundant information in the predictor space.

```{r correlations,fig.cap="Correlations amongst predictors",fig.pos="asis",fig.height=3.5}
#--------------------------------------------------------------
#    Feature engineering to split DLQ_Count into 2 buckets
#--------------------------------------------------------------
creditdata %<>% 
  mutate(DLQ_Count_3_X = as.factor(ifelse(DLQ_Count > 2, 1,0)))

SelectedFeat3 <- c(SelectedFeat2[which(SelectedFeat2 == "DLQ_Count")*-1],"DLQ_Count_3_X")

#---------------------------
#   re-assign training & validation data
#-----------------------

train2 <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(train2$DEFAULT) <- c("no","yes")

valid2 <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(valid2$DEFAULT) <- c("no","yes")

test2 <- creditdata %>% filter(test == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(test2$DEFAULT) <- c("no","yes")

train2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)

valid2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)

test2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)

# -------------------------------------------------------
#     Find correlations
#---------------------------------------------------------

# marriage and education one hot encoded

predict(caret::dummyVars(~EDUCATION,data = train2,fullRank = T),newdata = train2) -> EDUDUM
predict(caret::dummyVars(~MARRIAGE,data = train2,fullRank = T),newdata = train2) -> MARDUM

corrplot::corrplot(cbind.data.frame(train2,EDUDUM,MARDUM) %>% select(-EDUCATION,-MARRIAGE,-DEFAULT) %>% mutate_if(is.factor,as.numeric) %>% cor(.),tl.cex = 0.5)
findCorrelation(cbind.data.frame(train2,EDUDUM,MARDUM) %>% select(-EDUCATION,-MARRIAGE,-DEFAULT) %>% mutate_if(is.factor,as.numeric) %>% cor(.)) -> corvars2remove
cbind.data.frame(train2,EDUDUM,MARDUM) %>% select(-EDUCATION,-MARRIAGE,-DEFAULT) %>% mutate_if(is.factor,as.numeric) %>% cor(.) %>% colnames() -> corcols

corcols[corvars2remove] -> removeCols

train2 %<>% select(-one_of(c(removeCols,'ppk')))

valid2 %<>% select(-one_of(c(removeCols,'ppk')))

test2 %<>% select(-one_of(c(removeCols,'ppk')))

```


### 4.8.2 Simple Logistic Regression model

The numerical variables (not the factor variables) in the predictor space was centered and scaled for ease of modeling and interpretation. A simple logistic regression model was fit. See Table 19 for table of coefficients and odds ratio. The model coefficients for most of the features are intuitive, for example the males have 12% more odds than females to default. For one standard deviation increase in average utilization odds of default increases by 6%. Likewise, one standard deviation increase in average bill amount increases the odds for defaulting by 14%. Customers delinquent by 3 or more months have the odds increase by 2.75 times. Also with decreasing levels in the buckets of average payments, the odds of default increases. However, the odds of defaults 15 - 17% lower for lower payment to bill ratios, which is very counter intuitive. 


```{r}

registerDoSEQ()


# preProc <- preProcess(train2 %>% select_if(is.numeric),method = c("center","scale"))
# 
# predict(preProc,newdata = train2) -> trainProc
# y_trainProc <- trainProc$DEFAULT
# levels(y_trainProc) <- c("no","yes")
# 
# ctrl <- trainControl(method = "cv",
#                      summaryFunction = twoClassSummary,
#                      classProbs = TRUE,
#                      savePredictions = "final")
# 
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')], 
#                   y = y_trainProc,
#                   method = "glm",
#                  family = "binomial",
#                   #tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   )  ->  logitmdl2

broom::tidy(logitmdl2$finalModel) %>% 
  mutate(oddsRatio = exp(estimate)) %>% 
  kable("latex",digits = 4, caption = "Summary of Simple Logistic regression model",booktabs = T) %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
```

#### 4.8.2.1  Simple Logistic Regression model Performance

The performance table below shows an improved AUC than the single predictor decision trees, however the F1 statistic is poorer. `r figr('mdl4', TRUE, type="Figure")` shows the performance of the simple logistic regression model while the AUC shows an improvement of 77%, the model lacks sensitivity and has a poorer F1 score of 0.25,0.26 and 0.25 for training, validation and test data sets. The % VAR recovered of 100%, 102% and 115% for train,validation and test data sets are better than the previous models.

```{r,eval = F }
#confusionMatrix(predict(logitmdl2),y_trainProc,positive = "yes")
logit2.roc <- pROC::roc(response = y_trainProc, predictor = predict(logitmdl2$finalModel))

# -------------------------------------------
#        Get the validation data ready
#--------------------------------------------


validProc <- predict(preProc,newdata = valid2)
validY <- validProc$DEFAULT
levels(validY) <- c("no","yes")

testProc <- predict(preProc,newdata = test2)
testY <- testProc$DEFAULT
levels(testY) <- c("no","yes")

#perf.logit <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],validProc[,-which(names(trainProc) %in% 'DEFAULT')],y_trainProc,validY,logitmdl2$finalModel, title = "Simple Logistic regression",,validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

#perf.logittest <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],testProc[,-which(names(trainProc) %in% 'DEFAULT')],y_trainProc,testY,logitmdl2$finalModel, title = "Simple Logistic regression",,validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

#perf.logit$Perf %>% kable("latex",digits = 2)
```


```{r mdl4,fig.cap = "model4 - Simple logit regression",fig.align='center',fig.height=4,fig.show='asis'}
knitr::include_graphics('modelResults/model4.png',auto_pdf = T)
```

\newpage

# 5. Model based Feature Selection

In this section model based feature selection method is employed for better choosing the features for further modeling. Here the following strategies are used for feature selection

  1. Regression based methods
    + logistic regression with stepwise feature selection.
    + Lasso regression (binomial distribution)
    
  2. Tree based methods
  
    + Random forest
    + Extreme Gradient Boosting (xgBoost)
    
The importance of features in all the models are examined and features are selected based on importance amongst all the models for further modeling.

**All models are tuned based on 10 fold cross validated samples on the training data set and tested on the validation data**
    


## 5.1 Logistic model with stepwise feature selection

Stepwise selection of predictors based on AIC values of a logistic regression was fit. The coefficients are listed in table 20 and the importance of features based on their coefficients is shown in `r figr('stpwise', TRUE, type="Figure")`

```{r}

# registerDoSEQ()
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')], 
#                   y = y_trainProc,
#                   #tuneLength = 10,
#                   method = "glmStepAIC",
#                   family = "binomial",
#                   metric = "ROC",
#                   trControl = ctrl,
#              verbose = F
#       
#                   )  ->  glmLogit



broom::tidy(glmLogit$finalModel) %>% 
  kable("latex",digits = 4, caption = "Stepwise logit coefficients",booktabs = T) %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
```


```{r stpwise,warning=F, message = F, fig.cap= "feature imporance based on stepwise selection ", fig.height=3,fig.width=3}
#glmLogit.perf$Perf

data.frame(Preds = rownames(varImp(glmLogit$finalModel)), Importance = varImp(glmLogit$finalModel)) %>% 
  arrange(desc(Overall)) %>% top_n(12) %>% 
  ggplot(aes(x=fct_reorder(Preds,Overall), y = Overall)) + geom_point() + geom_col(width = .1) + coord_flip() + theme_bw() +
  xlab("Predictors") + ggtitle("Top 12 predictors - Stepwise selection - Logit")

```

### 5.1.1 Performance of stepwise logit model

The model though has a AUC of 77%, the sensitivity is 16% with F1 as 0.25 and 0.27 for train and validation data sets respectively. Different threshold probability cut off values to classify defaulting customers were explored, however 0.5 threshold had the maximum F1. The sensitivity of the model is poor at 0.16. `r figr('stpwiseperf', TRUE, type="Figure")` shows the performance statistics.

```{r stpwiseperf, fig.cap = "model 5 - Stepwise logit regression performance"}
# glmLogit.perf <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],validProc[,-which(names(validProc) %in% 'DEFAULT')],y_trainProc,validY,glmLogit$finalModel, title = "Stepwise AIC logistic regression",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

# glmLogit.perf.test <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],testProc[,-which(names(testProc) %in% 'DEFAULT')],y_trainProc,testY,glmLogit$finalModel, title = "Stepwise AIC logistic regression",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])
knitr::include_graphics('modelResults/model5.png',auto_pdf = T)
```

```{r, eval = F}
thresholder(glmLogit,threshold = seq(0.1,0.9,by = 0.1)) ->thdcal_glmLogit
thdcal_glmLogit %>% 
  ggplot(aes(x=prob_threshold,y = F1
             )) + geom_point()

```


## 5.2 Lasso regression

The important predictors are more or less identical to the stepwise selection model above. The importance of features is shown in `r figr('lassoImp', TRUE, type="Figure")`

```{r lassoImp,fig.cap = "variable importance",fig.height = 3}
# registerDoSEQ()
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')] %>% data.matrix(), 
#                   y = y_trainProc,
#                   method = "glmnet",
#                   family = "binomial",
#                   metric = "ROC",
#                   trControl = ctrl,
#              tuneGrid = expand.grid(alpha = 1,
#                                    lambda = seq(0.001,0.1,by = 0.001))
#             
#       
#                   )  ->  glmnetLogit

# lassoPred <- coef(glmnetLogit$finalModel,glmnetLogit$bestTune$lambda) %>% as.matrix()
# colnames(lassoPred) <- "Coef"
# 
# Importance <- data.frame(Pred = rownames(lassoPred),Coef = lassoPred[,1])

Importance %>% top_n(10) %>% ggplot(aes(x=fct_reorder(Pred,abs(Coef)), y = abs(Coef))) + 
  geom_col(width = 0.1) + geom_point() + coord_flip() + 
  theme_bw() + xlab("Predictors") + ggtitle("Lasso - variable importance (top 10)")
```


```{r,eval =F}
broom::tidy(glmnetLogit$finalModel)
```

### 5.2.1 Lasso regression performance

`r figr('lassoPerf', TRUE, type="Figure")` shows Lasso regression has made the best classifications so far. With Sensitivity at 33 % and 34% for train and validation data sets respectively. With a F1 score of 0.43 for both data sets. The % VAR recovered is similar to that of stepwise logit model.

```{r lassoPerf,fig.cap = "Model 6 - Lasso regression performance"}
# glmnetLogit.perf <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')] %>% data.matrix(),validProc[,-which(names(validProc) %in% 'DEFAULT')] %>% data.matrix(),y_trainProc,validY,glmnetLogit, title = "Lasso regression",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

#glmnetLogit.perf.test <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')] %>% data.matrix(),testProc[,-which(names(testProc) %in% 'DEFAULT')] %>% data.matrix(),y_trainProc,testY,glmnetLogit, title = "Lasso regression",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

# thresholder(glmnetLogit,threshold = seq(0.1,0.7,by = 0.1)) ->thdcal_glmnetLogit
# thdcal_glmnetLogit %>% 
#   ggplot(aes(x=prob_threshold,y = F1
#              )) + geom_point()
knitr::include_graphics('modelResults/model6.png',auto_pdf = T)
```


## 5.3 Random forest 

Random forest methods were tuned with different predictors for each iteration. `r figr('rfimp', TRUE, type="Figure")` shows the tuning results and the top 20 important features. The threshold (probability cut off) to predict the positive class (defaulters) were tuned for the training data by optimizing it for the F1 score. While a threshold of 0.4 yielded the best result in the training data, it did not yield a significant improvement in the validation data. Hence the threshold was retained at its default of 0.5. 

```{r rfimp,fig.cap = "Random forest, feature selection",fig.pos = "asis",fig.height=4, fig.pos="asis"}
# cl <- makePSOCKcluster(5)
# registerDoParallel(cl)
# caret::train(x = trainProc[,-which(names(trainProc) %in% 'DEFAULT')], 
#                   y = y_trainProc,
#                   tuneLength = 10,
#                   method = "rf",
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   )  ->  rf
# stopCluster(cl)
# 
# plot(rf,main = "Random forest tuning") -> rfTuning
# plot(varImp(rf),top = 20, main = "Random forest top 20 predictors",tl.cex = 0.5, grid = F) -> rfVarImp

gridExtra::grid.arrange(rfTuning,rfVarImp, ncol = 2)


```

### 5.3.1 Random forest performance

`r figr('rfperf', TRUE, type="Figure")` shows the model over fit the training data with AUC as 1 and sensitivity of 0.96 and performed poorly on the validation and test data sets relative to Lasso regression model.


```{r rfperf,fig.cap = "model 7 - Random forest performance"}
#perf.rf <- getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],validProc[,-which(names(validProc) %in% 'DEFAULT')],y_trainProc,validY,rf$finalModel, cutoff = 0.4,title = "Random forest",,validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

#perf.rf.test <-getPerformance(trainProc[,-which(names(trainProc) %in% 'DEFAULT')],testProc[,-which(names(testProc) %in% 'DEFAULT')],y_trainProc,testY,rf, cutoff = 0.4,title = "Random forest",,validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

#thresholder(rf,threshold = seq(0.1,0.7,by = 0.1)) ->thdcal_rf
# thdcal_rf %>%
#   ggplot(aes(x=prob_threshold,y = F1
#              )) + geom_point()
#perf.rf$Perf
# perf.rf$Plots
knitr::include_graphics('modelResults/model7.png',auto_pdf = T)
```




```{r,eval = F}
recipes::recipe(DEFAULT ~ ., data = trainProc) -> recep
recep <- recipes::step_mutate(recep,Avg_BILL_Amt = ifelse(Avg_BILL_Amt <= 0, 0.0001, Avg_BILL_Amt))
recep %<>% recipes::step_YeoJohnson(Avg_BILL_Amt)
recep %<>% recipes::step_scale(Avg_BILL_Amt)
recipes::prep(recep) -> finalrecep
recipes::bake(finalrecep,trainProc) -> trainProc2

recipes::bake(finalrecep,validProc) -> validProc2
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
caret::train(x = trainProc2[,-which(names(trainProc) %in% 'DEFAULT')], 
                  y = y_trainProc,
                  tuneLength = 10,
                  method = "rf",
                  metric = "ROC",
                  trControl = ctrl
      
                  )  ->  rf2
stopCluster(cl)

getPerformance(trainProc2[,-which(names(trainProc2) %in% 'DEFAULT')],validProc2[,-which(names(validProc2) %in% 'DEFAULT')],y_trainProc,validY,rf2$finalModel)

```

```{r,eval = F}
#plot(varImp(rf2))
```

## 5.4 xgboost

An extreme gradient boosting model was tuned with a constant learning rate of 10% and L2 regularization of 0. The model was tuned by varying depth of trees and limiting the predictors used with 10% to 90% of the total predictor space. The tuning was performed over 100% and 75% of the training samples. `r figr('xgbtuning', TRUE, type="Figure")` and  `r figr('xgbVarImp', TRUE, type="Figure")` shows the model tuning results and the variable importance of top 20 important variables. The training perforamnce peaks when the model uses 30% of the predictor space and uses tree depth of 2.

```{r,eval =F}

library(xgboost)
#---------------------------------
#       Convert data to matrix
#----------------------------------

trainProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> trainProcMat

validProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> validProcMat

testProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> testProcMat

#---------------------------------
#     tuning grid  
#---------------------------------
 
xgb_trcontrol <- trainControl(
  method = "cv",
  number = 5,
  allowParallel = TRUE,
  verboseIter = FALSE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
 
xgbGrid <- expand.grid(
 
  nrounds = c(100,200),
  max_depth = c(2,5,10,15),
  eta = 0.1,
  gamma = 0,
  #lambda = c(0,0.05,0.1,0.5),
  min_child_weight=1,
  subsample= c(0.75,1),
  colsample_bytree = seq(0.1,0.9,length.out = 5)
  
 
)
 
 
 
library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
 
xgb_model <- caret::train(
  trainProcMat[,-which(colnames(trainProcMat) %in% 'DEFAULT')],y_trainProc,
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  #objective = "binary:logistic",
  metric =  "ROC"
)
 
 
 
# xgb <- xgboost(data = xymat,
#                label = y,
#                eta = 0.1,
#                max_depth = 10,
#                subsample = 0.5,
#                colsample_bytree = 0.5,
#                seed = 123,
#                eval_metric = )
 
stopCluster(cl)
```

```{r xgbtuning, fig.cap = "xgBoost tuning", fig.height=4,fig.width=6}
plot(xgb_model,
     #ylab=list(cex=1), 
     #xlab = list(cex = 1),
     main = list(label =  "xgboost tuning",cex = 1),
     par.strip.text = list(cex = .7),
     scales = list(x = list(cex =0.5),
                   y = list(cex = 0.5)),
     xlab = list(cex = 0.75),
     ylab = list(cex = 0.5),
     auto.key = list(size = 1,
                     cex.title = 0.75,
                     title = " Max depth",
                     columns = 4))


```


```{r xgbVarImp, fig.cap = "xgBoost variable importance",fig.height=3,fig.width=4}

plot(varImp(xgb_model),top = 10, main = "Top 10 predictors - xgboost",tl.cex = 0.1) 

#gridExtra::grid.arrange(xgbtuning,varImpxgb,ncol = 2)


```

### 5.4.1 xgBoost Performance

The performance of the model is shown in `r figr('xgbPerf', TRUE, type="Figure")`. It is surprising that the training accuracy was not any closer to random forest with L2 regularization at 0. The type - I error (false pass) is very high making the F1 score to be very low at 0.27.

```{r xgbPerf, fig.cap = "model 8 - xgbBoost model performance"}
#xgb.Perf <- getPerformance(trainProcMat[,-which(colnames(trainProcMat) %in% 'DEFAULT')],validProcMat[,-which(colnames(validProcMat) %in% 'DEFAULT')],y_trainProc,validY,xgb_model$finalModel, title = "xgBoost performance",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]))

#xgb.Perf.test <- getPerformance(trainProcMat[,-which(colnames(trainProcMat) %in% 'DEFAULT')],testProcMat[,-which(colnames(testProcMat) %in% 'DEFAULT')],y_trainProc,testY,xgb_model$finalModel, title = "xgBoost performance",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1])

# xgb.Perf$Plots
knitr::include_graphics('modelResults/model8.png',auto_pdf = T)
```




```{r,eval = F}

thresholder(glmnetLogit,threshold = seq(0.1,0.9,by = 0.1)) ->thdcal

thdcal %>% 
  ggplot(aes(x=prob_threshold,y = F1
             )) + geom_point()
```

# 6. Selected Features

The union of top 10 predictors from each of the models fit in section 6 are used for further modeling the classification model. The chosen predictors are as shown in table 21.

```{r}
# LassoPreds <- Importance %>% top_n(8,abs(Coef))
# xgbPreds <- data.frame(Preds = rownames(varImp(xgb_model)$importance),Imp = varImp(xgb_model)$importance) %>% top_n(10)
# rf2Preds <- data.frame(Preds = rownames(varImp(rf2)$importance),Imp = varImp(rf2)$importance) %>% top_n(10)
# varImp(glmLogit$finalModel)->ImpglmLogit
# glmPreds <- data.frame(Preds = as.character(rownames(ImpglmLogit)), Importance = ImpglmLogit$Overall)
# glmPreds$Preds <- as.character(glmPreds$Preds)
# glmPreds %<>% mutate(Preds = ifelse(str_detect(Preds,'EDUCATION'),'EDUCATION',
#                                     ifelse(str_detect(Preds,'SEX'),'SEX',
#                                            ifelse(str_detect(Preds,'MARRIAGE'),'MARRIAGE',Preds))))
# #glmPreds$Preds <- as.character(glmPreds$Preds)
# # clean glmPreds
# which(str_sub(glmPreds$Preds,nchar(as.character(glmPreds$Preds)))=="1") -> cleanReq
# glmPreds$Preds[cleanReq] <- substr(as.character(glmPreds$Preds)[cleanReq],start = 1,stop = nchar(as.character(glmPreds$Preds)[cleanReq])-1)
# glmPreds %<>% arrange(desc(Importance)) 
# glmPredictors <- glmPreds %>% top_n(10) %>% .$Preds
# rfPreds <- data.frame(Preds = rownames(varImp(rf)$importance),Imp = varImp(rf)$importance) %>% top_n(10)
# 
# # manual extraction of the preds rather than intersection for now
# Preds <-union(union(union( union(LassoPreds$Pred[-1],xgbPreds$Preds),rf2Preds$Preds),rfPreds$Preds),glmPredictors)

#trainProc %>% select(!!Preds) %>% mutate_if(is.factor,as.numeric) %>% cor() -> corsPred

#corrplot::corrplot(corsPred,tl.cex = .7,method = "number")

matrix(c(Preds,""),ncol = 3) %>% 
  kable("latex",caption = "Predictors chosen for modeling",booktabs = T) %>% 
  kable_styling(latex_options = "hold_position")
```


```{r,eval=F}
caret::train(x = trainProc[,which(names(trainProc) %in% c("DLQ_Total_x_1", "DLQ_Count_3_X"))], 
                  y = y_trainProc,
                  method = "glm",
                 family = "binomial",
                  #tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
      
                  ) -> simpleLogit

# getPerformance(trainProc[,which(names(trainProc) %in% c("DLQ_Total_x_1", "DLQ_Count_3_X"))],validProc[,which(names(validProc) %in% c("DLQ_Total_x_1", "DLQ_Count_3_X"))],y_trainProc,validY,simpleLogit$finalModel)
```

# 7. Predictive Modeling with Selected features

In this section the predictors identified from the previous section are used to build classification model to predict defaulting customers. The models are fit and their appropriate statistics are shown in this section.

## 7.1 Logistic regression 

The summary of the logistic regression shown in table 22 makes intuitive sense for the most part. For example one standard deviation increase in average bill amount leads to 11% increased odds of default. Increased balance over 6 months on credit card increases odds of  default by 7%. As the measure of balance as a distance to credit limit in 3 standard deviations decreases (the distances were replaced by weights of evidence - ppkwoe), the odds of default increases by 17%. As the average monthly payments decreases the odds of default increases from 24% to 114% (see odds ratio of AvgPMT_0_910 to AvgPMT_1750_3333). As the number of delinquencies increase beyond 3, the odds of defaulting increases almost 3 times. The lower levels of standard deviations of bill amounts have surprisingly higher odds of default (27%), this could be potentially interpreted as customers only paying the interest and leaving the principal amount on their credit card. 

```{r}
# registerDoSEQ()
# 
# caret::train(x = trainProc[,which(names(trainProc) %in% Preds)], 
#                   y = y_trainProc,
#                   method = "glm",
#                  family = "binomial",
#                   #tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   ) -> logitSelectedPred

#summary(logitSelectedPred$finalModel) %>% pander()


broom::tidy(logitSelectedPred$finalModel) %>% 
  mutate(oddsRatio = exp(estimate)) %>% 
  kable("latex",digits = 4,caption = "logistic regression coefficients and odds- ratio",booktabs = T) %>% 
  kable_styling(latex_options = "hold_position")
```

### 7.1.1 Logistic regression model performance

The sensitivity of the model is poor at 15% and 14% for validation and test data sets respectively. In the previous sections, it was seen xgBoost and lasso regression had close to 30% sensitivity. The F1 score of the model is 0.27. The performance statistics is shown in `r figr('logitSelected', TRUE, type="Figure")`

```{r logitSelected, fig.cap = "model 9 - Logistic regression selected predictors model performance"}
#getPerformance(trainProc[,which(names(trainProc) %in% Preds)],validProc[,which(names(validProc) %in% Preds)],y_trainProc,validY,logitSelectedPred$finalModel,title = "Logit model with selected features",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> logitSelectedPredPerf

#getPerformance(trainProc[,which(names(trainProc) %in% Preds)],testProc[,which(names(testProc) %in% Preds)],y_trainProc,testY,logitSelectedPred$finalModel,title = "Logit model with selected features",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> logitSelectedPredPerf.test

# logitSelectedPredPerf$Plots
knitr::include_graphics('modelResults/model9.png',auto_pdf = T)
```

### 7.1.2 Improved logistic regression model

The statistically insignificant features are now removed and the logistic regression model is re-fit. The sensitivity improves by a 1% point. As seen in `r figr('logitSelectedRefit', TRUE, type="Figure")`.

```{r logitSelectedRefit, fig.cap = "model 10 - Logistic regression selected significant predictors model performance"}
# ctrl <- trainControl(method = "cv",
#                      summaryFunction = twoClassSummary,
#                      classProbs = TRUE,
#                      savePredictions = "final")
# PredsLogit <- setdiff(Preds,c("PerUTILinc","perBALinc","AVG_UTIL","Max_Pmt_Amt","payFlag","UTIL_Growth_6mo","Avg_BILL_Amt","Avg_UTIL"))
# df <- trainProc
# df_valid <- validProc
# df_test <- testProc
# df$ppkwoe <- as.factor(df$ppkwoe)
# df_valid$ppkwoe <- as.factor(df_valid$ppkwoe)
# df_test$ppkwoe <- as.factor(df_test$ppkwoe)
# caret::train(x = df[,PredsLogit], 
#                   y = y_trainProc,
#                   method = "glm",
#                  family = "binomial",
#                   #tuneLength = 10,
#                   metric = "ROC",
#                   trControl = ctrl
#       
#                   ) -> logitSelectedPred2

#getPerformance(df[,PredsLogit],df_valid[,PredsLogit],y_trainProc,validY,logitSelectedPred2$finalModel, title = "Logit refit",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> logitSelectedPredPerf2

#getPerformance(df[,PredsLogit],df_test[,PredsLogit],y_trainProc,testY,logitSelectedPred2$finalModel, title = "Logit refit",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> logitSelectedPredPerf2.test

# logitSelectedPredPerf2$Plots

knitr::include_graphics('modelResults/model10.png',auto_pdf = T)

```
## 7.2 Naive Bayes 

Though our predictor space contains features that are not independent of each other, a Naive Bayes classifier is fit to the data to evaluate its performance. The AUC is comparable to the rest of the models, but Naive Bayes' Sensitivity is as high as the xgBoost's model with a high specificity. The F1 score is 0.5, much higher than all other models. As seen in `r figr('NaiveBayes', TRUE, type="Figure")`.

```{r NaiveBayes, fig.cap = "model 11 - Naive Bayes model performance"}
# search_grid <- expand.grid(
#   usekernel = c(TRUE, FALSE),
#   fL = 0:5,
#   adjust = seq(0, 5, by = 1)
# )
# 
# caret::train(x = df[,Preds], 
#                   y = y_trainProc,
#                   method = "nb",
#                   metric = "ROC",
#                   trControl = ctrl,
#                   tuneGrid = search_grid
#       
#                   ) -> nbSelectedPred
# 
# #plot(nbSelectedPred)

#getPerformance(df[,-which(names(df) %in% 'DEFAULT')],df_valid[,-which(names(df_valid) %in% 'DEFAULT')],y_trainProc,validY,nbSelectedPred, title = "Naive Bayes model Performance",validBILL = creditdata$BILL_AMT1[creditdata$data.group==3],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> nbPerf

#getPerformance(df[,-which(names(df) %in% 'DEFAULT')],df_test[,-which(names(df_test) %in% 'DEFAULT')],y_trainProc,testY,nbSelectedPred, title = "Naive Bayes model Performance",validBILL = creditdata$BILL_AMT1[creditdata$data.group==2],trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1]) -> nbPerf.test
# nbPerf$Plots
#trainBILL = creditdata$BILL_AMT1[creditdata$data.group==1],validBILL = creditdata$BILL_AMT1[creditdata$data.group==2]

knitr::include_graphics('modelResults/model11.png',auto_pdf = T)
```


## 7.3 Neural network

A feed forward neural network with 1 input layer and 3 hidden layers with drop out layers (rate of 10% drop out) after each hidden layer was chosen. With input layer having as many neurons as the number of chosen predictors and the output layer with a single neuron. Here a model that uses a larger predictor set than the "chosen" predictors from section 7 is presented. While a model that used only the predictors chosen from section 6 was fit, the results were not very encouraging and not presented here. All the hidden layers have ReLU activation and output layer has a sigmoid activation. The objective function was to minimize the binary crossentropy with Adam's optimizer. The model architecture is shown in `r figr('NNArch', TRUE, type="Figure")`. Architecture was chosen based on how model weights updated over epochs. Tensorboard was used to visualize updates in weights. Architecture in which there were diminishing gradient problems, were eliminated. Keras' package's callback function was used to monitor AUC values. `r figr('NNtraining', TRUE, type="Figure")` shows the training evolution. The gradient change in loss over epoch was minimal. However, as the training loss improved, the validation loss increased. Hence there was diminishing returns for dditional iterations of training. Though drop outs were used at every subsequent layer, it did not improve the training process. The model performance is shown in `r figr('NNPerf', TRUE, type="Figure")`. The Naive bayes model performance is better than the neural network model.



```{r,eval = F}
library(keras)

y_train_nn <-  ifelse(y_trainProc == "yes",1,0)
y_valid_nn <-  ifelse(validY=="yes",1,0)

# model <- keras_model_sequential() %>% 
#     # first hidden layer
#   layer_dense(units = 8,activation = "relu",input_shape = length(Preds)) %>% 
#     # add drop out layer
#   layer_dropout(rate = 0.1) %>% 
#   # second hidden layer
#   layer_dense(units = 4, activation = "relu") %>% 
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # third layer
#    layer_dense(units = 2, activation = "relu") %>% 
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # output layer
#   layer_dense(units = 1,activation = "sigmoid")
# 
# # creating custome metric
# metric_auc <- custom_metric("auc",function(y_true,y_pred){
#   K <- backend()
#   y_true <- K$eval(y_true)
#   y_pred <- K$eval(y_pred)
#   roccurve <- roc(response = y_true, predictor = y_pred)
#   aucval <- pROC::auc(roccurve)
#   K$constant(aucval,'float64')
# })
# 
# # compile the network
# model %>% 
# compile(
#   optimizer = 'adam',
#   loss = "binary_crossentropy",
#   metrics = c('accuracy')
# )
# 
# model


```


```{r}
set.seed(10)
# history <- model %>% 
#   fit(trainProcMat[,Preds],
#       y_train_nn,
#       epochs = 200,
#       batch_size = 30,
#       validation_data = list(validProcMat[,Preds],y_valid_nn))
```

```{r}
# y_train_pred_prob <- predict_proba(model,x = trainProcMat[,Preds]) %>% as.vector()
# y_train_pred_class <- predict_classes(model,x = trainProcMat[,Preds]) %>% as.vector()
# estimates_keras_tbl <- tibble(
#   truth      = y_trainProc,
#   estimate   = as.factor(y_train_pred_class) %>% fct_recode(yes = "1",no = "0"),
#   class_prob = y_train_pred_prob
# )
# 
# y_valid_pred_prob <- predict_proba(model,x = validProcMat[,Preds]) %>% as.vector()
# y_valid_pred_class <- predict_classes(model,x = validProcMat[,Preds]) %>% as.vector()
# estimates_valid <- tibble(
#   truth      = validY,
#   estimate   = as.factor(y_valid_pred_class) %>% fct_recode(yes = "1",no = "0"),
#   class_prob = y_valid_pred_prob
# )
# 
# estimates_keras_tbl %>% yardstick::roc_auc(truth, class_prob)
# estimates_valid %>% yardstick::roc_auc(truth, class_prob)
# 
#  estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
#  estimates_valid %>% f_meas(truth, estimate, beta = 1)
#  
#  estimates_keras_tbl %>% metrics(truth, estimate)
#  estimates_valid %>% metrics(truth, estimate)
#  
#  tibble(
#   precision = estimates_keras_tbl %>% precision(truth, estimate),
#   recall    = estimates_keras_tbl %>% recall(truth, estimate)
# )
#  tibble(
#   precision = estimates_valid %>% precision(truth, estimate),
#   recall    = estimates_valid %>% recall(truth, estimate)
# )
 
```

```{r,eval =F}
auc_roc <- R6::R6Class("ROC", 
                       inherit = KerasCallback, 
                       public = list(
                             
                             losses = NULL,
                             x = NA,
                             y = NA,
                             x_val = NA,
                             y_val = NA,
                             
                             initialize = function(training = list(), validation= list()){
                                   self$x <- training[[1]]                                   
                                   self$y <- training[[2]]
                                   self$x_val <- validation[[1]]
                                   self$y_val <- validation[[2]]
                             },
                             
                             
                             on_epoch_end = function(epoch, logs = list()){
                                   
                                   self$losses <- c(self$losses, logs[["loss"]])
                                   y_pred <- self$model$predict(self$x)
                                   y_pred_val <- self$model$predict(self$x_val)
                                   score = Metrics::auc(actual = self$y, predicted =  y_pred)
                                   score_val = Metrics::auc(actual = self$y_val, predicted =  y_pred_val)
                                   print(paste("epoch: ", epoch+1, " roc:", score, ' roc_val:', score_val))
                             }    
                       ))

# auc_roc_metric <- auc_roc$new(training = list(trainProcMat[,Preds], y_train_nn), validation = list(validProcMat[,Preds],y_valid_nn))

# tensorboard("logs/run_f")

# history2 <- model %>% 
#   fit(trainProcMat[,Preds],
#       y_train_nn,
#       epochs = 100,
#       batch_size = 30,
#       validation_data = list(validProcMat[,Preds],y_valid_nn),
#       callbacks = list(auc_roc_metric,callback_tensorboard("logs/run_b",histogram_freq = 1,write_graph = T,write_grads = T)))
 
```

```{r}
# estimates_keras_tbl %>% yardstick::roc_auc(truth, class_prob)
# estimates_valid %>% yardstick::roc_auc(truth, class_prob)
# 
#  estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
#  estimates_valid %>% f_meas(truth, estimate, beta = 1)
#  
#  estimates_keras_tbl %>% metrics(truth, estimate)
#  estimates_valid %>% metrics(truth, estimate)
#  
#  tibble(
#   precision = estimates_keras_tbl %>% precision(truth, estimate),
#   recall    = estimates_keras_tbl %>% recall(truth, estimate)
# )
#  tibble(
#   precision = estimates_valid %>% precision(truth, estimate),
#   recall    = estimates_valid %>% recall(truth, estimate)
# )
```

```{r,eval =F}
# take the train data and set it for neural network
recipes::recipe(DEFAULT ~ ., data = trainProc) -> recep
#recep <- recipes::step_mutate(recep,Avg_BILL_Amt = ifelse(Avg_BILL_Amt <= 0, 0.0001, Avg_BILL_Amt))
#recep %<>% recipes::step_YeoJohnson(Avg_BILL_Amt)
#recep %<>% recipes::step_scale(Avg_BILL_Amt)
recep %<>% 
  recipes::step_dummy(EDUCATION)

recep %<>% 
  recipes::step_dummy(MARRIAGE)

recipes::prep(recep) -> finalrecep
recipes::bake(finalrecep,trainProc) -> trainProc2

recipes::bake(finalrecep,validProc)-> validProc2

recipes::bake(finalrecep,testProc)-> testProc2

trainProc2 %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> trainProcMat2

validProc2 %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> validProcMat2


testProc2 %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> testProcMat2


```


```{r NNArch,fig.cap = "NN architecture",out.height="200px",out.height="200px"}
# model2 <- keras_model_sequential() %>%
#     # first hidden layer
#   layer_dense(units = 8,activation = "relu",input_shape = ncol(trainProcMat2)-1) %>%
#     # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # second hidden layer
#   layer_dense(units = 4, activation = "relu") %>%
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # third layer
#    layer_dense(units = 2, activation = "relu") %>%
#       # add drop out layer
#   layer_dropout(rate = 0.1) %>%
#   # output layer
#   layer_dense(units = 1,activation = "sigmoid")
# 
# # creating custome metric
# metric_auc <- custom_metric("auc",function(y_true,y_pred){
#   K <- backend()
#   y_true <- K$eval(y_true)
#   y_pred <- K$eval(y_pred)
#   roccurve <- roc(response = y_true, predictor = y_pred)
#   aucval <- pROC::auc(roccurve)
#   K$constant(aucval,'float64')
# })
# 
# # compile the network
# model2 %>%
# compile(
#   optimizer = 'adam',
#   loss = "binary_crossentropy",
#   metrics = c('accuracy')
# )
knitr::include_graphics('modelResults/NNarch.png',auto_pdf = T)
```

```{r,eval = F}
auc_roc_metric <- auc_roc$new(training = list(trainProcMat2[,-27], y_train_nn), validation = list(validProcMat2[,-27], y_valid_nn))

tensorboard("logs/run_z2")

checkpoint_dir <- "checkpoints"
dir.create(checkpoint_dir, showWarnings = FALSE)
filepath <- file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hdf5")

history3 <- model2 %>%
  fit(trainProcMat2[,-27],
      y_train_nn,
      epochs = 100,
      batch_size = 30,
      validation_data = list(validProcMat2[,-27],y_valid_nn),
      callbacks = list(auc_roc_metric,callback_tensorboard("logs/run_z2",histogram_freq = 1,write_graph = T,write_grads = T),
                       callback_model_checkpoint(filepath = filepath,
                                                 save_weights_only = T,
                                                 verbose = 1)))

# history3 <- model2 %>% 
#   fit(trainProcMat2[,-27],
#       y_train_nn,
#       epochs = 50,
#       batch_size = 30,
#       validation_data = list(validProcMat2[,-27],y_valid_nn))


y_train_pred_prob2 <- predict_proba(model2,x = trainProcMat2[,-27]) %>% as.vector()
y_train_pred_class2 <- predict_classes(model2,x = trainProcMat2[,-27]) %>% as.vector()


y_valid_pred_prob2 <- predict_proba(model2,x = validProcMat2[,-27]) %>% as.vector()
y_valid_pred_class2 <- predict_classes(model2,x = validProcMat2[,-27]) %>% as.vector()


y_test_pred_prob2 <- predict_proba(model2,x = testProcMat2[,-27]) %>% as.vector()
y_test_pred_class2 <- predict_classes(model2,x = testProcMat2[,-27]) %>% as.vector()


estimates_keras_tbl2 <- tibble(
  truth      = y_trainProc,
  estimate   = as.factor(y_train_pred_class2) %>% fct_recode(yes = "1",no = "0"),
  class_prob = y_train_pred_prob2
)

confusionMatrix(estimates_keras_tbl2$estimate,estimates_keras_tbl2$truth, positive = "yes") -> trainNNConf


estimates_valid2 <- tibble(
  truth      = validY,
  estimate   = as.factor(y_valid_pred_class2) %>% fct_recode(yes = "1",no = "0"),
  class_prob = y_valid_pred_prob2
)

confusionMatrix(estimates_valid2$estimate,estimates_valid2$truth, positive = "yes") -> validNNConf

estimates_test2 <- tibble(
  truth      = testY,
  estimate   = as.factor(y_test_pred_class2) %>% fct_recode(yes = "1",no = "0"),
  class_prob = y_test_pred_prob2
)

confusionMatrix(estimates_valid2$estimate,estimates_valid2$truth, positive = "yes") -> validNNConf

confusionMatrix(estimates_test2$estimate,estimates_test2$truth, positive = "yes") -> testNNConf

# estimates_keras_tbl2 %>% yardstick::roc_auc(truth, class_prob)
# estimates_valid2 %>% yardstick::roc_auc(truth, class_prob)
# pROC::roc(testY,predictor = y_test_pred_prob2)
creditdata$BILL_AMT1[creditdata$data.group==1] -> trainBILL
creditdata$BILL_AMT1[creditdata$data.group==2] -> testBILL
creditdata$BILL_AMT1[creditdata$data.group==3] -> validBILL
ActualVAR_train <- sum(y_train_nn*ifelse(trainBILL <= 0, 0, trainBILL))
ActualVAR_valid <- sum(ifelse(validY=='yes',1,0)*ifelse(validBILL <= 0, 0 , validBILL))
ActualVAR_test <- sum(ifelse(testY=='yes',1,0)*ifelse(validBILL <= 0, 0 , testBILL))

trainVar <- sum(y_train_pred_prob2* ifelse(trainBILL<=0,0,trainBILL))
validVar <- sum(y_valid_pred_prob2* ifelse(validBILL<=0,0,validBILL))
testVar <- sum(y_test_pred_prob2* ifelse(testBILL<=0,0,testBILL))

recoveredVAR_train <- trainVar/ActualVAR_train
recoveredVAR_valid <- validVar/ActualVAR_valid
recoveredVAR_test <- testVar/ActualVAR_test
```



```{r NNtraining, fig.cap = "Neural Network training", fig.height=3.5}
plot(history3)+ggtitle("Neural Network training and validation statistics") + theme_bw()

```

```{r NNPerf,fig.cap = "model 12 - Neural network model performance"}
knitr::include_graphics('modelResults/model12.png',auto_pdf = T)
```

