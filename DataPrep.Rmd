---
title: "Credit Default Prediction"
author: "Sri Seshadri"
date: "9/28/2019"
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
output:
    pdf_document:
      toc: true
      toc_depth: 3
 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
```

# 1. Introduction (DRAFT)

This paper discusses the various modeling approaches to predict customers defaulting on their credit card payments by using six months' credit card bill amounts, payment history. A working model that takes in customer data and reports a list of customers at risk of default and the amount of money that is at risk. The following are the key insights that were discovered from the data:

  1. X % increase in monthly bill for a customer paying bills at a rate of y% of the monthly bill has Z% increased risk compared to average customer.
  2. 

# 2. Executive Summary (DRAFT)

Insights as of 10/13:

1. Total delinquency (total number of months late over 6 months) when greater than 2 has increases the chance of default (15% to 52%)
2. Historical delinquencies, Variance of invoice and average amount paid towards bill are key contributors towards prediction of default.


# 3. Data

Data are available for 30,000 customers spanning between April 2005 and September 2005. The description of the data are shown in Table 1 below. Table 2 and Table 3 show the general summary of the data. Upon inspection of the data summaries, there were data quality issues identified, that are discussed in the following section.

```{r, warning = F, message = F}
library(tidyverse)
library(caret)
library(magrittr)
library(pander)
library(knitr)
library(kableExtra)
library(matrixStats)
source('getPerformance.R')


#------------------------------------------------------
#               READ DATA IN
#---------------------------------------------------------

creditdata <- readRDS("credit_card_default.RData")

creditdata <- as.data.frame(creditdata) %>% as_tibble()
creditdata %<>% 
  mutate_at(c("DEFAULT","EDUCATION","MARRIAGE","SEX","data.group"),.funs = as.factor)

#-----------------------------------------------------
#              DATA DICTIONARY
#-----------------------------------------------------

Dict <- tibble(FEATURE = colnames(creditdata)[1:25],
               DESCRIPTION =
                 c("CUSTOMER ID",
                   "CREDIT LIMIT",
                   "GENDER",
                   "EDUCATION",
                   "MARITAL STATUS",
                   "AGE",
                   "REPAYMENT STATUS SEP 2005",
                   "REPAYMENT STATUS AUG 2005",
                   "REPAYMENT STATUS JUL 2005",
                   "REPAYMENT STATUS JUN 2005",
                   "REPAYMENT STATUS MAY 2005",
                   "REPAYMENT STATUS APR 2005",
                   "BILL SEP 2005",
                   "BILL AUG 2005",
                   "BILL JUL 2005",
                   "BILL JUN 2005",
                   "BILL MAY 2005",
                   "BILL APR 2005",
                   "PAYMENT SEP 2005",
                   "PAYMENT AUG 2005",
                   "PAYMENT JUL 2005",
                   "PAYMENT JUN 2005",
                   "PAYMENT MAY 2005",
                   "PAYMENT APR 2005",
                   "DEFAULTING CUSTOMER"
                  ),
               COMMENTS = 
                 c("",
                   "INCLUDES SUPPL CARDS",
                   "1 = MALE, 2 = FEMALE",
                   "1 = GRAD,2 = UNIV,3 = HIGH SCH,4 = OTHERS",
                   "1=MARRIED,2=SINGLE,3=OTHERS",
                   "YEARS",
                   "assumed SEP 2005 ; -1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "-1 = DULY PAID,1= 1 MONTH DELAY, 2 = 2 MONTH DELAY... 9 = >=9 MONTH DELAY",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "STATEMENT BALANCE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "PAYMENT MADE",
                   "1 = DEFAULT, 0 = NON-DEFAULT"
                 )
           
       ) 

#*******************************************
#         REPORT DICTIONARY
#*******************************************

kable(Dict,format = "latex",align = "c",caption = "Data Dictionary") %>% 
  kable_styling(full_width = F,latex_options = "hold_position",font_size = 7) %>% 
  column_spec(1,border_left = T) %>% 
  column_spec(3,width = "20em",border_right = T) %>% 
  row_spec(0,bold = T) %>% 
  collapse_rows(valign = "middle")


# Get stats on the data
skimr::skim_with(numeric = list(hist = NULL),integer = list(hist = NULL))
skimr::skim_to_wide(creditdata) %>% 
  filter(type == "factor") %>% 
  filter(!variable %in% "data.group") %>% 
  select(variable,missing,complete,n,n_unique,top_counts) %>% 
  #mutate(top_counts = cell_spec(top_counts,"latex",align = "r")) %>% 
  kable(format = "latex",digits = 1,booktabs = T,caption = "categorical variables summary") %>% 
  kable_styling(latex_options = "hold_position")

skimr::skim_to_wide(creditdata) %>% 
  filter(type == "integer") %>% 
  select(variable,missing,complete,n,mean,sd,p0,p25,median,p75,p100) %>% 
  #mutate(top_counts = cell_spec(top_counts,"latex",align = "r")) %>% 
  kable(format = "latex",digits = 1,booktabs = T,caption = "numeric variables summary") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)

```

## 3.1 Data Issues

1. Table 2 shows the variable $EDUCATION$ to have 7 possible values, while the dictionary in table 1 shows us expect 4 possible values. The values 0,5 and 6 would be assigned to a category "unknown", coded as 9.

2. Similarly for 54 customers, the value of $MARRIAGE$ is entered as 0. The 0 values are assumed to be "unknown" marital status.

3. The variable PAY_0 is assumed to be the repayment status as of September 2005.Hence renamed as PAY_1

4. Repayment status has values that include -2 and 0. Approximately 26,000 customers have repayment status that is not described in the dictionary. It is very likely that the data dictionary had an omission in describing -2 and 0. Therefore they are assumed to be non-delayed payments.

5. There are 543 customers in the data who are marked delinquent when the average bill over 6 months is 0. It is not clear if the customers defaulted in the months prior to the 6 months of data that is provided.


```{r,eval=T}

# remap unknown educaton to code 9
creditdata %<>% 
  mutate(EDUCATION = as.numeric(EDUCATION)-1) %>% 
  mutate(EDUCATION = ifelse(EDUCATION %in% c(0,5,6),9,EDUCATION)) %>% 
  mutate(EDUCATION = as.factor(EDUCATION)) 

# rename PAY_0 to PAY_1
creditdata %<>% 
  rename(PAY_1 = PAY_0)



```

## 3.2 Data Splitting

The data was separated into three groups for model training, validation and testing purposes. The number of customers in each of these groups are shown below in table 4.

```{r,eval=T}
creditdata %>% 
  mutate(data.group = ifelse(data.group == 1, "Train", ifelse(data.group == 2, "Test", "Validate"))) %>% 
  group_by(data.group) %>% 
  summarise(datapoints = n()) %>% 
  knitr::kable(caption = "Train,Validation and Test splits",format = "latex",booktabs = T) %>% 
  kable_styling(latex_options = c("striped","hold_position"))


```


# 4. Feature Engineering

Table 5 shows the features that are derived from the data along with their name and calculation method. The AGE and ppk (the distance between credit limit and average bill in units of 3 time standard deviations of bill amounts) variables are continuous variables that are binned using Weights Of Evidence (WOE) method.

It is interesting to note that the WOE indicates that the customers in higher age group (over 39 years) have relatively higher risk of default compared to customers in the late twenties. Possibly due to potential loss in employment and difficulty to secure an income source. This hypothesis will be explored in the exploratory data analysis section below.

ppk values were binned and replaced with their respective WOE. Figure 1 shows the binning of these variables.


\newpage
\blandscape

```{r,eval=T}
# EngFeats <- data.frame(
#   Feature = c(
#     "Payment to Bill ratio",
#     "Average Payment to Bill ratio",
#     "Proportion of credit used in the month",
#     " Month's in/decrease in credit use",
#     "Average proportion of credit used",
#     "Average Montly bill",
#     "Month's in/decrease in bill",
#     "Average amount paid",
#     "Month's in/decrease in payment",
#     "Balance growth over 6 months",
#     "Credit utilization growth 6 months",
#     "Month's in/decrease in Payment ratio",
#     "Customer with increased utilization over time",
#     "Customer with increased bill over time",
#     "Customer with reduced payments time",
#     "Customer with reduced payment/bill over time",
#     "Max bill",
#     "Max payment",
#     "Max delinquency",
#     "Age of customers between mentioned range",
#      "Ppk -Dist to credit limit in units of 3 std.dev of Bill",
#     "Ppk discretized with WOE values"
#     
#     
#   ),
#   `Column_Names` = c(
#     "PMT_RATIO2 - PMT_RATIO6",
#     # "$PMT\\_RATIO3$",
#     # "$PMT\\_RATIO4$",
#     # "$PMT\\_RATIO5$",
#     # "$PMT\\_RATIO6$",
#     
#     "$Avg\\_PMT\\_RATIO$",
#     
#     "$UTIL\\_1$ to $UTIL\\_6$ ",
#     # "$UTIL\\_2$",
#     # "$UTIL\\_3$",
#     # "$UTIL\\_4$",
#     # "$UTIL\\_5$",
#     # "$UTIL\\_6$",
#     
#     "$UTILMR1$ to $UTILMR5$",
#     # "$UTILMR4$",
#     # "$UTILMR3$",
#     # "$UTILMR2$",
#     # "$UTILMR1$",
#   
#    
#     "$Avg\\_UTIL$",
#     
#     "$Avg\\_BILL\\_Amt$",
#     
#      "$BILLAMTMR1$ to $BILLAMTMR5$ ", 
#      # "$BILLAMTMR4$",
#      # "$BILLAMTMR3$",
#      # "$BILLAMTMR2$",
#      # "$BILLAMTMR1$",
#     
#      "$Avg\\_PMT\\_Amt$",
#     
#       "$PAYMR1$ to $PAYMR5$",
#       # "$PAYMR4$",
#       # "$PAYMR3$",
#       # "$PAYMR2$",
#       # "$PAYMR1$",
#     
#     "$Bal\\_Growth\\_6mo$", 
#     
#     "$UTIL\\_Growth\\_6mo$", 
#     
#          "$PMT\\_RATIOMR2$ to $PMT\\_RATIOMR5$", 
#          # "$PMT\\_RATIOMR4$",
#          # "$PMT\\_RATIOMR3$", 
#          # "$PMT\\_RATIOMR2$", 
#     
#     "$UtilFlag$",
#     
#     '$balFlag$',
#     
#     "$payFlag$",
#     
#     
#     "$payRatioFlag$",
#     
#     "$Max\\_Bill\\_Amt$",
#     
#     "$Max\\_Pmt\\_Amt$",
#     
#     "$Max\\_Dlq$",
#     
#     "$Age\\_21\\_25$",
#     
#     "$ppk$",
#     
#     "$ppkwoe$"
#     
#     
#     
#   ),
#   Calculation = c(' $\\frac{PAY\\_AMT_{mnth-1}}{BILL\\_AMT{mnth}}$ $mnth \\in {2,3,4,5,6}$',
#                   # ' $\\frac{PAY\\_AMT2}{BILL\\_AMT3}$',
#                   # ' $\\frac{PAY\\_AMT3}{BILL\\_AMT4}$',
#                   # ' $\\frac{PAY\\_AMT4}{BILL\\_AMT5}$',
#                   # ' $\\frac{PAY\\_AMT5}{BILL\\_AMT6}$',
#   
#                   ' $\\frac{\\sum_{Month=1}^{5}PAY\\_AMT_{mnth}}{\\sum_{mnth=2}^{6}BILL\\_AMT_{mnth}}$',
#                   
#                   ' $\\frac{BILL\\_AMT_{mnth}}{LIMIT\\_BAL}$ $mnth \\in {1,2,3,4,5,6}$',
#                   # ' $\\frac{BILL\\_AMT2}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT3}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT4}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT5}{LIMIT\\_BAL}$',
#                   # ' $\\frac{BILL\\_AMT6}{LIMIT\\_BAL}$',
#                   
#                   '$UTIL_{mnth-1}- UTIL_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$UTIL\\_4 - UTIL\\_5$',
#                   # '$UTIL\\_3 - UTIL\\_4$',
#                   # '$UTIL\\_2 - UTIL\\_3$',
#                   # '$UTIL\\_1 - UTIL\\_2$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}UTIL_{mnth}}{5}$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}BILL\\_AMT_{mnth}}{5}$',
#                   
#                   '$BILL\\_AMT_{mnth-1} - BILL\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$BILL\\_AMT4 - BILL\\_AMT5$',
#                   # '$BILL\\_AMT3 - BILL\\_AMT4$',
#                   # '$BILL\\_AMT2 - BILL\\_AMT3$',
#                   # '$BILL\\_AMT1 - BILL\\_AMT2$',
#                   
#                   '$\\frac{\\sum_{mnth=1}^{6}PAY\\_AMT_{mnth}}{5}$',
#                   
#                   
#                   
#                   '$PAY\\_AMT_{mnth-1} - PAY\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$',
#                   # '$PAY\\_AMT4 - PAY\\_AMT5$',
#                   # '$PAY\\_AMT3 - PAY\\_AMT4$',
#                   # '$PAY\\_AMT2 - PAY\\_AMT3$',
#                   # '$PAY\\_AMT1 - PAY\\_AMT2$',
#                   
#                   "$BILL\\_AMT_{mnth-1} - BILL\\_AMT_{mnth}$ $mnth \\in {2,3,4,5,6}$",
#                   
#                   "$UTIL\\_1 - UTIL\\_6$",
#                   
#                   "$PMT\\_RATIO5 - PMT\\_RATIO6$",
#                   # "$PMT\\_RATIO4 - PMT\\_RATIO5$",
#                   # "$PMT\\_RATIO3 - PMT\\_RATIO4$",
#                   # "$PMT\\_RATIO2 - PMT\\_RATIO3$",
#                   
#                   "$\\forall {UTILMR5,UTILMR4,UTILMR3,UTILMR2,UTILMR1} > 0$",
#                   
#                   "$\\forall {BILLAMTMR5,BILLAMTMR4,BILLAMTMR3,BILLAMTMR2,BILLAMTMR1} > 0$",
#                   
#                   "$\\forall{PAYMR5,PAYMR4,PAYMR3,PAYMR2,PAYMR1} > 0$",
#                   
#                   "$\\forall{PMT\\_RATIOMR5,PMT\\_RATIOMR4,PMT\\_RATIOMR3,PMT\\_RATIOMR2} > 0$",
#                   
#                   '$max(BILL\\_AMT1,BILL\\_AMT2,BILL\\_AMT3,BILL\\_AMT4,BILL\\_AMT5,BILL\\_AMT6)$',
#                   
#                   '$max(PAY\\_AMT1,PAY\\_AMT2,PAY\\_AMT3,PAY\\_AMT4,PAY\\_AMT5,PAY\\_AMT6)$',
#                   
#                   '$max(PAY\\_1,PAY\\_2,PAY\\_3,PAY\\_4,PAY\\_5,PAY\\_6)$',
#                   
#                   '$if AGE>=21 AND AGE<=25 then 1 else 0$',
#                   
#               '$\\frac{(LIMIT\\_BAL - \\overline{BILL\\_AMT_{mth}})}{\\sqrt{\\sum_{mth=1}^6\\frac{(BILL\\_AMT_{mth} - \\overline{BILL\\_AMT_{mth}})^2}{5}}}$',
#                   
#                   'ppk binned and replaced by WOE'
#                   
#                   
#                   
#                   
#                   
#                   
#   )
#   
# 
# 
# )

knitr::kable(data.frame(l = ""),"latex", escape = F,booktabs = T,caption = "Engineered features") 

```

\elandscape
\newpage


```{r,eval=T}

# ------------------------------------------------------------
# PAY RATIO defined as payment over bill 
# Make the PAY_RATIO as 1 when the buill and pay amounts are 0
#-------------------------------------------------------------
creditdata %<>% 
  mutate(PMT_RATIO2 = ifelse(PAY_AMT1==0 & BILL_AMT2==0|BILL_AMT2 <= 0,1,PAY_AMT1/BILL_AMT2),
         PMT_RATIO3 = ifelse(PAY_AMT2==0 & BILL_AMT3== 0|BILL_AMT3 <= 0,1,PAY_AMT2/BILL_AMT3),
         PMT_RATIO4 = ifelse(PAY_AMT3==0 & BILL_AMT4==0|BILL_AMT4 <= 0,1,PAY_AMT3/BILL_AMT4),
         PMT_RATIO5 = ifelse(PAY_AMT4==0 & BILL_AMT5==0|BILL_AMT5 <= 0,1,PAY_AMT4/BILL_AMT5),
         PMT_RATIO6 = ifelse(PAY_AMT5==0 & BILL_AMT6==0|BILL_AMT6 <= 0,1,PAY_AMT5/BILL_AMT6)) %>% 
  mutate(
         Avg_PMT_RATIO = ifelse(rowSums(select(.,matches("BILL_AMT[2-6]")))==0,1,rowSums(select(.,matches("PAY_AMT[1-5]")))/rowSums(select(.,matches("BILL_AMT[2-6]")))))


```



```{r,eval=T}
library(matrixStats)

# Utilization defined as a % usage of credit limit. When there is a negative bill the util is made 0.

creditdata %<>% 
  mutate(UTIL_1 = ifelse(BILL_AMT1/LIMIT_BAL < 0 , 0 , BILL_AMT1/LIMIT_BAL),
         UTIL_2 = ifelse(BILL_AMT2/LIMIT_BAL < 0 , 0 , BILL_AMT2/LIMIT_BAL),
         UTIL_3 = ifelse(BILL_AMT3/LIMIT_BAL < 0 , 0 , BILL_AMT3/LIMIT_BAL),
         UTIL_4 = ifelse(BILL_AMT4/LIMIT_BAL < 0 , 0 , BILL_AMT4/LIMIT_BAL),
         UTIL_5 = ifelse(BILL_AMT5/LIMIT_BAL < 0 , 0 , BILL_AMT5/LIMIT_BAL),
         UTIL_6 = ifelse(BILL_AMT6/LIMIT_BAL < 0 , 0 , BILL_AMT6/LIMIT_BAL))
```



```{r,eval=T}
#--------------------------------------------------------------------------------------------------------------
# While we that the median balance increases over time, how much of the population is causing this to increase
# Calculate the difference between subsequent months in UTIL and BIll_AMTs
#--------------------------------------------------------------------------------------------------------------

creditdata %<>% 
  mutate(Avg_UTIL = rowMeans(select(.,starts_with("UTIL"))),
         UTILMR5 = UTIL_5 - UTIL_6,
         UTILMR4 = UTIL_4 - UTIL_5,
         UTILMR3 = UTIL_3 - UTIL_4,
         UTILMR2 = UTIL_2 - UTIL_3,
         UTILMR1 = UTIL_1 - UTIL_2,
         Avg_BILL_Amt = rowMeans(select(.,starts_with("BILL_AMT")),na.rm = T),
         stdBILL = ifelse(rowSds(select(.,starts_with("BILL_AMT")) %>% as.matrix()) == 0, 0.01,rowSds(select(.,starts_with("BILL_AMT")) %>% as.matrix()))) %>% 
  mutate(ppk = (LIMIT_BAL - Avg_BILL_Amt)/(3*stdBILL),
         BILLAMTMR5 = BILL_AMT5 - BILL_AMT6,
         BILLAMTMR4 = BILL_AMT4 - BILL_AMT5,
         BILLAMTMR3 = BILL_AMT3 - BILL_AMT4,
         BILLAMTMR2 = BILL_AMT2 - BILL_AMT3,
         BILLAMTMR1 = BILL_AMT1 - BILL_AMT2,
         Avg_PMT_Amt = rowMeans(select(.,starts_with("PAY_AMT")),na.rm = T))


creditdata %<>%
  mutate(PAYMR5 = PAY_AMT5 - PAY_AMT6,
         PAYMR4 = PAY_AMT4 - PAY_AMT5,
         PAYMR3 = PAY_AMT3 - PAY_AMT4,
         PAYMR2 = PAY_AMT2 - PAY_AMT3,
         PAYMR1 = PAY_AMT1 - PAY_AMT2)

creditdata %<>% 
  mutate(Bal_Growth_6mo = BILL_AMT1 - BILL_AMT6,
         UTIL_Growth_6mo = UTIL_1 - UTIL_6,
         PMT_RATIOMR5 = PMT_RATIO5 - PMT_RATIO6,
         PMT_RATIOMR4 = PMT_RATIO4 - PMT_RATIO5,
         PMT_RATIOMR3 = PMT_RATIO3 - PMT_RATIO4,
         PMT_RATIOMR2 = PMT_RATIO2 - PMT_RATIO3
         )


  

# Customers with increasing utilization over time
creditdata %>% filter_at(vars(starts_with("UTILM")),all_vars(. > 0)) %>% select(ID) ->IDs

# customers with increasing Balance over time
creditdata %>% filter_at(vars(starts_with("BILLAMTMR")),all_vars(. > 0)) %>% select(ID) ->BalIDs

# customers with decreasing payments over time

creditdata %>% filter_at(vars(starts_with("PAYMR")),all_vars(. < 0)) %>% select(ID) ->payIDs

# customers with decreasing payment ratio over time

creditdata %>% filter_at(vars(starts_with("PMT_RATIOMR")),all_vars(. < 0)) %>% select(ID) ->payRatioIDs

creditdata %<>% 
  mutate(utilFlag = ifelse(ID %in% IDs$ID,1,0))

creditdata %<>% 
  mutate(balFlag = ifelse(ID %in% BalIDs$ID,1,0))

creditdata %<>% 
  mutate(payFlag = ifelse(ID %in% payIDs$ID,1,0))

creditdata %<>% 
  mutate(payRatioFlag = ifelse(ID %in% payRatioIDs$ID,1,0))





## Get percentage increase between month 6 and month 1

creditdata %<>%
  mutate(PerBALinc = ifelse(!is.infinite((BILL_AMT1 - BILL_AMT6)/BILL_AMT6)&!is.na((BILL_AMT1 - BILL_AMT6)/BILL_AMT6),(BILL_AMT1 - BILL_AMT6)/BILL_AMT6,(BILL_AMT1 - BILL_AMT6)/0.01),
         PerUTILinc = ifelse(!is.infinite((UTIL_1 - UTIL_6)/UTIL_6)&!is.na((UTIL_6 - UTIL_1)/UTIL_6),(UTIL_1- UTIL_6)/UTIL_6,(UTIL_1 - UTIL_6)/0.0001)
         )

```



```{r,eval=T}
creditdata %<>% 
  mutate(Max_Bill_Amt = rowMaxs(select(.,matches("BILL_AMT[0-9]")) %>% as.matrix()),
         Max_Pmt_Amt = rowMaxs(select(.,matches("PAY_AMT[0-9]")) %>% as.matrix()))
```


```{r,eval=T}

# Customers delinquent often
# creditdata %<>%
#   mutate(Max_Dlq = ifelse(rowMaxs(select(.,matches("PAY_[0-9]")) %>% as.matrix()) < 0,0,rowMaxs(select(.,matches("PAY_[0-9]")) %>% as.matrix()) ))

creditdata %>% 
  select(ID,matches("PAY_[0-9]")) %>% 
  gather(key = "Period", value = "DLQ",-ID) %>% 
  mutate(DLQ = ifelse(DLQ < 0, 0,DLQ)) %>% 
  group_by(ID) %>% 
  summarise(DLQ_Count = count(DLQ > 0),
            DLQ_Total = sum(DLQ),
            DLQ_Max = max(DLQ)) -> Delequencies

creditdata %<>% 
  left_join(Delequencies)


  
```


```{r AGEppkBinning,eval=T,fig.cap="WOE binning",fig.height=4}
# creditdata %<>% 
#   mutate(AgeBins = cut(AGE,breaks = seq(from = 25, to = 80,by = 5)))
#   
# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 7) -> tst7
# 
# Information::plot_infotables(tst7,"AGE")

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 6) -> tst6

Information::plot_infotables(tst6,"AGE") + coord_flip()->p1


# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) -> tst5
# 
# Information::plot_infotables(tst5,"AGE")
# 
# Information::create_infotables(data = creditdata %>% select(AGE, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) -> tst4
# 
# Information::plot_infotables(tst4,"AGE")

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(ppk, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) -> tst5ppk

Information::plot_infotables(tst5ppk,"ppk") + coord_flip()->p2

gridExtra::grid.arrange(p1,p2,ncol=2,top = grid::textGrob("WOE binning"))

```


```{r,eval=F}
library(OneR)

OneR::optbin(as.factor(DEFAULT) ~ AGE, data = creditdata, method = "infogain")-> optbin

plot(optbin)

```


```{r,eval=T}


creditdata %<>% 
  mutate(AGE_21_25 = ifelse(AGE>=21 & AGE<=25,1,0),
         AGE_26_29 = ifelse(AGE>=26 & AGE<=29,1,0),
         AGE_30_33 = ifelse(AGE>=30 & AGE<=33,1,0),
         AGE_34_38 = ifelse(AGE>=34 & AGE<=38,1,0),
         AGE_39_44 = ifelse(AGE>=39 & AGE<=44,1,0))

creditdata %<>% 
  mutate(ppkwoe = ifelse(ppk<0.49,0.30368328,
                         ifelse(ppk < 1.82,-0.06824883,
                                ifelse(ppk < 6.98,-0.08848357,
                                       ifelse(ppk < 27.3,-0.29885247,0.09571116)))))

```


```{r,eval=F}
table(creditdata$DEFAULT,creditdata$utilFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$utilFlag))


table(creditdata$DEFAULT,creditdata$balFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$balFlag))

table(creditdata$DEFAULT,creditdata$payFlag)

chisq.test(table(creditdata$DEFAULT,creditdata$payFlag))
```

## 4.1 Data Quality of Engineered Features

The use of very small values like a penny when denominator dollar amounts are 0 prevented the feature from going to an infinite value.Table 6 Shows the summary of the engineered features.


```{r}

EngFeatNames <- creditdata %>% select(matches("RATIO|Avg|UTIL|MR|6mo|Flag|Max|Age_|ppk|inc|DLQ|stdB")) %>% colnames()
SelectedFeat <- EngFeatNames[!str_detect(EngFeatNames,"[A-Z][0-9]|UTIL_[0-9]")]

creditdata %>% 
  select(!!EngFeatNames) %>% 
  skimr::skim_to_wide() %>% 
  select(-type,-sd,-mean) %>% 
  kable("latex",digits = 2,booktabs = T,caption = "Engineering features summary",longtable = T) %>% 
  kable_styling(latex_options = "repeat_header",font_size = 7)
```

## 5. Exploratory Data Analysis (EDA).

In this section the training data are analyzed to explore potential relationships between the predictor and propensity of default. Table 7 shows the distribution of the defaulting customers among the 15180 customers.

```{r train}
train <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat,DEFAULT)
data.frame(prop.table(table(train$DEFAULT))) %>% 
  rename("DEFAULT" = Var1) %>% 
  kable("latex", booktabs = T, digits = 2,caption = "Distribution of Defaulting customers in training data") %>% 
  kable_styling(latex_options = "hold_position")
```

### 5.1 Magnitude of revolving credit

It is hypothesized that the magnitude of revolving credit is a reflection of income of customers and income has an effect on credit card default. There is no evidence of magnitude of revolving credit or income having an influence on default. In instances where either the payment or the bill amount were $<=0$ the value was assumed to be one-tenth of a New Taiwan(NT) penny. Figure 2 shows the log of the amounts for easier visualization. 

```{r,fig.cap="Effect of revolving credit on default",fig.height=4}
train %>% 
  ggplot(aes(x=log(ifelse(Avg_BILL_Amt<=0,0.001,Avg_BILL_Amt)),y=log(ifelse(Avg_PMT_Amt<=0,0.001,Avg_PMT_Amt)),col = DEFAULT)) + 
  xlab("log(Avg Bill Amount)") +
  ylab("log(Avg Payment Amount)")+
  geom_point(alpha = 0.4) + 
  theme_bw() -> scp

ggExtra::ggMarginal(scp,type = "histogram",groupColour = T) -> mp

mp

```

### 5.2 Past Delinquencies in payment

Here the frequency of delinquency (how often customers are delinquent) in payments' effect on default is explored. It is surprising that there are customers who were not late on payment were marked default. Its likely those customers though were not late, might have paid less than the minimum due. The second panel in the plot below explores the continued delinquency's effect on default. The higher total months late associates to increased odds of defaulting (thicker tails in the density plot). The weights of evidence plot on total & maximum delinquencies shows increased chances of defaulting when total delinquency is past 1 month. This potentially indicates that the bank pardon's a single late payment.

```{r,fig.cap="Effect of Delinqucies",fig.height=4}
train %>% 
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,1,0))) %>% 
  ggplot(aes(x=as.numeric(DLQ_Count),group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Frequency of Delinquency (DLQ_Count)") ->p1


train %>%
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,"Bill = 0","Bill > 0"))) %>%
  ggplot(aes(x=DLQ_Total,group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Total months late (DLQ_Total)") ->p2

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Total, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Total") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Total WOE") -> p3

train %>%
  mutate(flag = as.factor(ifelse(Avg_BILL_Amt==0|Avg_PMT_Amt==0,"Bill = 0","Bill > 0"))) %>%
  ggplot(aes(x=DLQ_Max,group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("Max months late (DLQ_Max)") ->p4

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Max, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Max") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Max WOE") -> p5

gridExtra::grid.arrange(p1,p2,p3,p5, ncol = 2)

```

## 5.3 Utilization of credit limit

A process capability metric used in the quality engineering domain is used here to determine how close does the bill come to the credit limit in the units of 3 times standard deviation of the bills. Figure 1 in section 4, shows that ppk less that 0.5 have a higher odds (odds of 1.34) for defaulting. It is surprising that there are is a increase in the odds for customers who have higher ppk. It was hypothesized that these very low utilization of credit may be from higher age group of customers, but there is no solid evidence of that as seen in the second panel of the plot below. It is to be noted that for easier visualization the natural log of ppk was used with negative ppk (bills higher than credit limit) set to very low (1e-07) value. The bottom panel of the plot shows the effect due to the average utilization, 

```{r,fig.cap="Utilization of credit limit's effect on Default",fig.height=4}
train %>% ggplot(aes(x=log(ifelse(ppk <=0, 0.0000001,ppk)),group = DEFAULT, col = DEFAULT, fill = DEFAULT)) + geom_density(alpha = 0.4) + theme_bw() + xlab("log(ppk)")-> u1
creditdata %>% filter(train==1) %>%  ggplot(aes(x=AGE, y = log(ifelse(ppk <=0, 0.0000001,ppk)), col = DEFAULT)) + geom_point(alpha = 0.4) + theme_bw() + ylab("log(ppk)")-> u2
train %>% ggplot(aes(y=Avg_UTIL,x = DEFAULT,col = DEFAULT)) + geom_boxplot() + theme_bw()-> u3
#gridExtra::tableGrob(broom::glance(chisq.test(utilflagtbl))) -> u4
gridExtra::grid.arrange(u1,u2,u3,ncol = 2)
```

The customers who have  continuous growth in utilization over 6 months is flagged and analyzed. The chi-squared test of independence shows there is a statistical difference. 


```{r}
table(train$DEFAULT,train$utilFlag) -> utilflagtbl
prop.table(utilflagtbl) %>% 
  data.frame() %>% rename("utilFlag" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Continous growth in utilization's effect on DEFAULT - ChiSq test") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(utilflagtbl) %>% pander()

```

## 5.4 Effect of balances

The effect of increased balances is quantified in the table below. The customers whose balances increases over time (month after month) are flagged and analyzed for the odds in default. The chi squared test of independence show there is a statistical significance due to increasing credit card balance over time. 

```{r}
table(train$DEFAULT,train$balFlag) -> balflagtbl
prop.table(balflagtbl) %>% 
  data.frame() %>% rename("balFlag" = Var1, "DEFAULT" = Var2) %>% 
  kable("latex", booktabs = T,caption = "Continous growth in balance's effect on DEFAULT - ChiSq test") %>% 
  kable_styling(latex_options = "hold_position",font_size = 7)
chisq.test(balflagtbl) %>% pander()
```

## 5.5 Effect of decreased payments

Payments as a ratio of bill amounts, average payment amounts, continuous decrease in payments/pay ratios over time did not yield statistically significant results when explored graphically using density plots. However, in the section 5.7.1, decision trees help bring out signal from payment variables. The influence of payments are discussed in section 5.7.1 below.

## 5.6 Effect of Age

Figure 1 in section 4 shows relatively higher chances of defaulting in the early ages (21-24) and in the age group past mid-life 45 years and above. The increased chances in default past mid-life may be potentially attributed to loss in employment and or health related events in life.

## 5.7 Model based EDA

In this section, two methods are employed to gain more insights on the effects of the predictor variables on chances of default. 

- Tree based methods.
- Simple logistic regression.


### 5.7.1 Tree based methods.

Multivariate decision tree method, where a tree is grown using all predictor variables in the model. Here a generalizable tree is obtained by using two different methods of pruning a) Using a cost complexity parameter on the leaves b) Depth of the tree. Decision trees are fit on 10 fold cross validated samples to arrive at a generalizable decision tree. 

#### 5.7.1.1 Decision tree with complexity parameter tuning

The best tuning for cost parameter based on 10 fold cross validation was 0.015. Figure 5 shows the tuning and the importance of the various predictors. It is to be noted that count of delinquencies (DLQ_Count) and maximum delinquencies (DLQ_Max) were removed from the predictors list for the model shown below. When included in the model, all the trees zero in on DLQ_Total, DLQ_Count and DLQ_Max leaving very little signal from other predictors to come out. 

It can be seen that apart from total delinquencies (DLQ_Total) the standard deviation of bill amount (stdBILL), Average payment amount (Avg_PMT_Amt), Average payment ratio (Avg_PMT_RATIO) and maximum payment amount (Max_Pmt_Amt)  are key contributors to the model. Upon inspecting the decision tree in Figure 6, discretizing Avg_PMT_RATIO at cut points 0.03, 0.12 could be useful for modeling purposes.  Avg_PMT_Amt cut points are not very clear from the decision tree. The standard deviation of bill cut points for discretization appears to be 1060 and 4900 NT dollars

The WOE methods would be used to check or corroborate the decision trees and explore opportunities for further engineering these variables. 

```{r, eval = T,fig.cap = "left:Tuning results of Cost Complexity; right: variable importance plot",fig.height=4}

library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

x = train %>% select(-DEFAULT,-DLQ_Count,-DLQ_Max)
y = as.factor(train$DEFAULT)
levels(y) <- c("no","yes")
rpartFit <- caret::train(x = x, 
                  y = y,
                  method = "rpart",
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
                  )
stopCluster(cl)


gridExtra::grid.arrange(plot(rpartFit),plot(varImp(rpartFit),tl.cex = 5),ncol = 2)

```


\newpage
\blandscape

```{r, fig.cap="Decision tree based on cost parameter tuning",fig.height=9, fig.width=16}
partykit::as.party(rpartFit$finalModel) -> prty
library(partykit)
plot(prty,gp = gpar(fontsize = 5),
     inner_panel = node_inner,
     ip_args = list(
       abbreviate = F,
       id = F
     ),
     tp_args = list(
       id = F
     ))
```

\elandscape
\newpage

Figure 7 confirms the interpretation of the splitting point of 0.03 and 0.12 for average payment ratio (or a singleton cut point of 0.12). The split points align with WOE for the average payment ratio (left panel of Fig 7). The average payment amount can be binned into 5 groups as shown in the right panel of figure 7. The splitting of total delinquencies as seen in the analysis in section 5.2 is reproduced is reproduced in Figure 7. 

```{r,fig.cap="WOE binning of (top left) Avg_PMT_RATIO, (top right) Avg_PMT_Amt,(bottom left) Bill Standard Deviation, (bottom right) DLQ_Total",fig.pos='asis'}
Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(Avg_PMT_RATIO, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) ->payratioamt

Information::plot_infotables(payratioamt,"Avg_PMT_RATIO") + coord_flip() + theme_bw() + ggtitle("") + ylab("Avg_PMT_RATIO WOE") + geom_vline(xintercept = c(1.5,2.5),lty = 2, col = 'red') -> pymtratio

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(Avg_PMT_Amt, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->avgpayamt

Information::plot_infotables(avgpayamt,"Avg_PMT_Amt") + coord_flip() + theme_bw() + ggtitle("") + ylab("Avg_PMT_Amt WOE") -> avgpymt

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(stdBILL, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 4) -> stdbillwoe
Information::plot_infotables(stdbillwoe,"stdBILL") + coord_flip() + theme_bw() + ggtitle("") + ylab("stdBill WOE") + geom_vline(xintercept = c(2.5),lty = 2, col = 'red') -> stdbillplt

Information::create_infotables(data = creditdata %>% filter(train==1) %>% select(DLQ_Total, DEFAULT) %>% mutate(DEFAULT = as.numeric(DEFAULT)-1), y = "DEFAULT", parallel = T, bins = 5) ->dlqbin

Information::plot_infotables(dlqbin,"DLQ_Total") + coord_flip() + theme_bw() + ggtitle("") + ylab("DLQ_Total WOE") -> p3

gridExtra::grid.arrange(pymtratio,avgpymt,stdbillplt,p3,ncol = 2)
```

#### 5.7.1.2 Decision tree with depth of tree tuning

When constraining the tree building process on the depth of the tree, the contribution of total delinquencies is very high relative to the other predictor variables. Figures 8 and 9 show the performance & variable importance and decision tree. The interpretation is consistent with the tree built in above section. 

```{r,eval = T,fig.cap="Decision tree based on depth of tree",fig.pos = "asis",message=F, warning=F}

cl <- makePSOCKcluster(5)
registerDoParallel(cl)

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

x = train %>% select(-DEFAULT,-DLQ_Count,-DLQ_Max)
y = as.factor(train$DEFAULT)
levels(y) <- c("no","yes")
rpartFit2 <- caret::train(x = x, 
                  y = y,
                  method = "rpart2",
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
                  )
stopCluster(cl)


gridExtra::grid.arrange(plot(rpartFit2),plot(varImp(rpartFit2),tl.cex = 5),ncol = 2)
```



```{r,eval = T, fig.cap = "Decision tree pruned by depth of tree", fig.pos="asis", message=F, warning = F}
partykit::as.party(rpartFit2$finalModel) -> prty
library(partykit)
plot(prty,gp = gpar(fontsize = 7),
     inner_panel = node_inner,
     ip_args = list(
       abbreviate = F,
       id = F
     ),
     tp_args = list(
       id = F
     ))
```


#### 5.7.1.3 Decision tree with one parameter at a time.

In this section, trees are built using only one parameter per model. The goal is to get a baseline or null model whose performance would be lower threshold for any complex model built in the following sections. 

The best single predictor model is using DLQ_Count with an accuracy of 80.23%. But it can be seen the "No information Rate" is 77.4% (i.e a null model with no predictors). The DLQ_Count variable provides a gain of ~ 2.8 percentage points in accuracy. 

```{r, message = T,warning = F}
library(OneR)
#OneR(DEFAULT ~ .,data = train %>% select(-DLQ_Count)%>% as.data.frame(), verbose = T) -> baselinemod
OneR(DEFAULT ~ .,data = train %>% as.data.frame(), verbose = T) -> baselinemod
confusionMatrix(predict(baselinemod,newdata = train %>% as.data.frame()), train$DEFAULT,positive = "1")


```

#### 5.7.1.4 Re-Engineer features

Based on the discoveries above, the following vraiables are discretized

- Average Payment Ratio
- Avergae Payment Amount
- Standard deviation of bill.
- Total delinquencies


```{r}

#--------------------------------------------------------------------------
#                      RE-ENGINEER DATA 
#--------------------------------------------------------------------------

creditdata %<>% 
  mutate(AvgPMT_0_910 = ifelse(Avg_PMT_Amt >= 0 & Avg_PMT_Amt <= 910,1, 0),
         AvgPMT_911_1749 = ifelse(Avg_PMT_Amt > 910 & Avg_PMT_Amt <= 1749,1, 0),
         AvgPMT_1750_3333 = ifelse(Avg_PMT_Amt > 1749 & Avg_PMT_Amt <= 3333.17,1, 0),
         AvgPMT_3333_6835 = ifelse(Avg_PMT_Amt > 3333.17 & Avg_PMT_Amt <= 6835,1, 0)
        # AvgPMT_6336_LOT = ifelse(Avg_PMT_Amt >6835,1, 0)
         ) %>% 
  mutate(Avg_PMT_RATIO_X_005 = ifelse(Avg_PMT_RATIO < 0.05, 1, 0),
         Avg_PMT_RATIO_005_011 = ifelse(Avg_PMT_RATIO >= 0.05 & Avg_PMT_RATIO < 0.11,1,0)
         #Avg_PMT_RATIO_011_X = ifelse(Avg_PMT_RATIO > 0.011,1,0)
         ) %>% 
  mutate(stdBILL_x_4521 = ifelse(stdBILL <= 4521.27, 1,0),
         DLQ_Total_x_1 = ifelse(DLQ_Total < 2,1,0))

#--------------------------------------------------------------------------------
#               log pmt_amount & bill
# -------------------------------------------------------------------------------

creditdata %<>% 
  mutate(logBillAvg = log(ifelse(Avg_BILL_Amt<=0,0.001,Avg_BILL_Amt)),
         logPayAvg = log(ifelse(Avg_PMT_Amt<=0,0.001,Avg_PMT_Amt)),
         logppk = log(ifelse(ppk <=0, 0.0000001,ppk))
  )

#-------------------------------------------------------
#       re assign train data with new predictors 
#-------------------------------------------------------
PmtRatioBucketsCol <- creditdata %>% select(matches("Avg_PMT_RATIO_|AvgPMT_|stdBILL_|DLQ_Total_")) %>% colnames()
SelectedFeat2 <- c(SelectedFeat[!str_detect(SelectedFeat,"Avg_PMT|std|DLQ_Total")],PmtRatioBucketsCol)


train2 <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat2,DEFAULT)
levels(train2$DEFAULT) <- c("no","yes")

valid2 <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat2,DEFAULT)
levels(valid2$DEFAULT) <- c("no","yes")

#------------------------------------------------------------
#        re-run baseline model that includes new features
#------------------------------------------------------------

OneR(DEFAULT ~ .,data = train2 %>% as.data.frame(), verbose = T) -> baselinemod
confusionMatrix(predict(baselinemod,newdata = train2 %>% as.data.frame()), train2$DEFAULT,positive = "yes")

getPerformance(train2 %>% as.data.frame(),valid2 %>% as.data.frame(),trainY = train2$DEFAULT, validY = valid2$DEFAULT,model = baselinemod)

```



```{r}
#--------------------------------------------------------------
#    Feature engineering to split DLQ_Count into 2 buckets
#--------------------------------------------------------------
creditdata %<>% 
  mutate(DLQ_Count_3_X = as.factor(ifelse(DLQ_Count > 2, 1,0)))

SelectedFeat3 <- c(SelectedFeat2[which(SelectedFeat2 == "DLQ_Count")*-1],"DLQ_Count_3_X")

#---------------------------
#   re-assign training & validation data
#-----------------------

train2 <- creditdata %>% filter(train == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(train2$DEFAULT) <- c("no","yes")

valid2 <- creditdata %>% filter(validate == 1) %>% select(!!SelectedFeat3,DEFAULT)
levels(valid2$DEFAULT) <- c("no","yes")

train2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)

valid2 %<>% 
  mutate_at(PmtRatioBucketsCol,as.factor) %>% 
  mutate_at(vars(matches("AGE_|Flag|DLQ_Count_")),as.factor)


```


### 5.7.2 Simple Logistic Regression model



```{r,eval =F}
train %>% select(!!colnames(x[,c(1,3,4,6,18,19:22)])) %>% mutate_at(vars(contains('AGE')),as.factor) -> logitdata
preProcess(logitdata %>% select(.,matches("Avg")),method = c("center","scale")) -> rec1
predict(rec1,newdata = logitdata %>% select(.,matches("Avg")))->prpd
logitdata %>% select(colnames(logitdata)[!colnames(logitdata) %in% colnames(prpd)]) %>% cbind.data.frame(prpd) -> logitprpd



caret::train(x = logitprpd, 
                  y = y,
                  method = "glm",
             family = "binomial",
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
                  )  ->  logitmdl
#broom::glance(logitmdl$finalModel)

broom::tidy(logitmdl$finalModel) %>% 
  kable("latex",digits = 2,caption = "simple logit model") %>% 
  kable_styling(latex_options = "hold_position")
```

```{r getPerformance}
getPerformance <- function(trainX,validX,trainY,validY,model, positive = "yes", rowNames = c("train","valid"),cutoff = 0.5, title = "", modelname =""){
  
  # mertrics to be used
  # 1. Accuracy, 2. Sensitivity, 3. Specificty 4. NIV, AUC
  
  library(caret)
  library(magrittr)
  library(dplyr)
  library(pROC)
 
  
if(class(model) == "randomForest"|class(model) == "OneR"){
  
  if(class(model) == "randomForest"){
    trainPred <- predict(model,trainX,type = "response")
    validPred <- predict(model,validX,type = "response")
  } else{
    trainPred <- predict(model,trainX,type = "class")
    validPred <- predict(model,validX,type = "class")
  }
    
    train.roc <- roc(response = trainY, predictor = predict(model,trainX,type = "prob")[,"yes"])
    valid.roc <- roc(response = validY, predictor = predict(model,validX,type = "prob")[,"yes"])
    
} else{
    trainPred <- as.factor(ifelse(predict(model,trainX) > cutoff, "yes", "no"))
    validPred <- as.factor(ifelse(predict(model,validX) > cutoff, "yes","no"))
    
    train.roc <- roc(response = trainY, predictor = predict(model,trainX))
    valid.roc <- roc(response = validY, predictor = predict(model,validX))
  }
  
  

  trainConfusion <- confusionMatrix(trainPred,trainY, positive = positive)
  testConfusion <- confusionMatrix(validPred,validY,positive = positive)
  
  PerformanceMeasures <- rbind(trainConfusion$byClass, testConfusion$byClass)
  rownames(PerformanceMeasures) <- rowNames
  
  PerformanceMeasures <- data.frame(PerformanceMeasures) %>% 
    mutate(auc = c(auc(train.roc),auc(valid.roc)),
           Model = modelname)
  
  
  rownames(PerformanceMeasures) <- rowNames
  
  plot(train.roc,legacy.axes = T, asp = NA, col = "blue",main = title)
  plot(valid.roc,legacy.axes = T, asp = NA, col = "red",add = T)
  legend("bottomright",legend = c(paste0("Train; AUC:",round(PerformanceMeasures$auc[1],2)), paste0("validation; AUC:",round(PerformanceMeasures$auc[2],2))), col = c("blue", "red"), lty = 1,cex = 0.6)
  
  p <- recordPlot()
  plot.new()
  
  return(list(Perf = PerformanceMeasures,
              Plots = p))
  
  
  
  
}
```



```{r}

registerDoSEQ()


preProc <- preProcess(train2 %>% select_if(is.numeric),method = c("center","scale"))

predict(preProc,newdata = train2) -> trainProc
y_trainProc <- trainProc$DEFAULT
levels(y_trainProc) <- c("no","yes")

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = "final")

caret::train(x = trainProc[,c(-3,-30)], 
                  y = y_trainProc,
                  method = "glm",
                 family = "binomial",
                  #tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl
      
                  )  ->  logitmdl2

summary(logitmdl2$finalModel)

confusionMatrix(predict(logitmdl2),y_trainProc,positive = "yes")
logit2.roc <- pROC::roc(response = y_trainProc, predictor = predict(logitmdl2$finalModel))

# -------------------------------------------
#        Get the validation data ready
#--------------------------------------------


validProc <- predict(preProc,newdata = valid2)
validY <- validProc$DEFAULT
levels(validY) <- c("no","yes")

perf.logit <- getPerformance(trainProc[,c(-3,-30)],validProc[,c(-3,-30)],y_trainProc,validY,logitmdl2$finalModel)

perf.logit$Perf %>% kable("latex",digits = 2)
```




## 6. Model based Feature Selection

### 6.1 Random forest 

```{r}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
caret::train(x = trainProc[,c(-3,-30)], 
                  y = y_trainProc,
                  tuneLength = 10,
                  method = "rf",
                  metric = "ROC",
                  trControl = ctrl
      
                  )  ->  rf
stopCluster(cl)

plot(rf)
plot(varImp(rf))
perf.rf <- getPerformance(trainProc[,c(-3,-30)],validProc[,c(-3,-30)],y_trainProc,validY,rf$finalModel)

```

```{r}
perf.rf$Perf

```
### 6.2 Logistic model with stepwise feature selection

```{r}

registerDoSEQ()
caret::train(x = trainProc[,c(-3,-30)], 
                  y = y_trainProc,
                  #tuneLength = 10,
                  method = "glmStepAIC",
                  family = "binomial",
                  metric = "ROC",
                  trControl = ctrl,
             verbose = F
      
                  )  ->  glmLogit

glmLogit.perf <- getPerformance(trainProc[,c(-3,-30)],validProc[,c(-3,-30)],y_trainProc,validY,glmLogit$finalModel)

summary(glmLogit$finalModel)

glmLogit.perf$Perf
```
### 6.3 Random forest model with  transformed Avg_Bill_Amt

```{r}
recipes::recipe(DEFAULT ~ ., data = trainProc) -> recep
recep <- recipes::step_mutate(recep,Avg_BILL_Amt = ifelse(Avg_BILL_Amt <= 0, 0.0001, Avg_BILL_Amt))
recep %<>% recipes::step_YeoJohnson(Avg_BILL_Amt)
recep %<>% recipes::step_scale(Avg_BILL_Amt)
recipes::prep(recep) -> finalrecep
recipes::bake(finalrecep,trainProc) -> trainProc2

recipes::bake(finalrecep,validProc) -> validProc2
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
caret::train(x = trainProc2[,c(-3,-30)], 
                  y = y_trainProc,
                  tuneLength = 10,
                  method = "rf",
                  metric = "ROC",
                  trControl = ctrl
      
                  )  ->  rf2
stopCluster(cl)

getPerformance(trainProc2[,c(-3,-30)],validProc2[,c(-3,-30)],y_trainProc,validY,rf$finalModel)

```

```{r}
plot(varImp(rf2))
```

## xgboost

```{r}

library(xgboost)
#---------------------------------
#       Convert data to matrix
#----------------------------------

trainProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> trainProcMat

validProc %>% 
  #select(-ppk,-DEFAULT) %>% 
  mutate_if(is.factor, function(x) as.numeric(x)-1) %>% 
  as.matrix() -> validProcMat



#---------------------------------
#     tuning grid  
#---------------------------------
 
xgb_trcontrol <- trainControl(
  method = "cv",
  number = 5,
  allowParallel = TRUE,
  verboseIter = FALSE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
 
xgbGrid <- expand.grid(
 
  nrounds = c(100,200),
  max_depth = c(2,5,10,15),
  eta = 0.1,
  gamma = 0,
  #lambda = c(0,0.05,0.1,0.5),
  min_child_weight=1,
  subsample= c(0.75,1),
  colsample_bytree = seq(0.5,0.9,length.out = 5)
  
 
)
 
 
 
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
 
xgb_model <- caret::train(
  trainProcMat[,c(-3,-30)],y_trainProc,
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  #objective = "binary:logistic",
  metric =  "ROC"
)
 
 
 
# xgb <- xgboost(data = xymat,
#                label = y,
#                eta = 0.1,
#                max_depth = 10,
#                subsample = 0.5,
#                colsample_bytree = 0.5,
#                seed = 123,
#                eval_metric = )
 
stopCluster(cl)
 
plot(xgb_model)

plot(varImp(xgb_model))


```

```{r}
getPerformance(trainProcMat[,c(-3,-30)],validProcMat[,c(-3,-30)],y_trainProc,validY,xgb_model$finalModel)
```




# lasso regression

```{r}
caret::train(x = trainProc[,c(-3,-30)] %>% data.matrix(), 
                  y = y_trainProc,
                  method = "glmnet",
                  family = "binomial",
                  metric = "ROC",
                  trControl = ctrl,
             tuneGrid = expand.grid(alpha = 1,
                                   lambda = seq(0.001,0.1,by = 0.001))
            
      
                  )  ->  glmnetLogit

lassoPred <- coef(glmnetLogit$finalModel,glmnetLogit$bestTune$lambda) %>% as.matrix()
colnames(lassoPred) <- "Coef"

Importance <- data.frame(Pred = rownames(lassoPred),Coef = lassoPred[,1])

Importance %>% ggplot(aes(x=fct_reorder(Pred,abs(Coef)), y = abs(Coef))) + geom_col(width = 0.1) + geom_point() + coord_flip() + theme_bw()

glmLogit.perf <- getPerformance(trainProc[,c(-3,-30)] %>% data.matrix(),validProc[,c(-3,-30)] %>% data.matrix(),y_trainProc,validY,glmnetLogit)

glmLogit.perf$Perf
```

```{r}
thresholder(glmnetLogit,threshold = seq(0.1,0.9,by = 0.1)) ->thdcal

thdcal %>% 
  ggplot(aes(x=prob_threshold,y = F1
             )) + geom_point()
```

### Selected Features

