
@online{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero Quick Start Guide},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@book{davenport_competing_2007,
	location = {Boston, Mass},
	edition = {1 edition},
	title = {Competing on Analytics: The New Science of Winning},
	isbn = {978-1-4221-0332-6},
	shorttitle = {Competing on Analytics},
	abstract = {You have more information at hand about your business environment than ever before. But are you using it to “out-think” your rivals? If not, you may be missing out on a potent competitive tool.In Competing on Analytics: The New Science of Winning, Thomas H. Davenport and Jeanne G. Harris argue that the frontier for using data to make decisions has shifted dramatically. Certain high-performing enterprises are now building their competitive strategies around data-driven insights that in turn generate impressive business results. Their secret weapon? Analytics: sophisticated quantitative and statistical analysis and predictive modeling.Exemplars of analytics are using new tools to identify their most profitable customers and offer them the right price, to accelerate product innovation, to optimize supply chains, and to identify the true drivers of financial performance. A wealth of examples—from organizations as diverse as Amazon, Barclay’s, Capital One, Harrah’s, Procter \& Gamble, Wachovia, and the Boston Red Sox—illuminate how to leverage the power of analytics.},
	pagetotal = {240},
	publisher = {Harvard Business Review Press},
	author = {Davenport, Thomas H. and Harris, Jeanne G.},
	date = {2007-03-06}
}

@article{jun_predictive_2006,
	title = {Predictive algorithm to determine the suitable time to change automotive engine oil},
	volume = {51},
	issn = {0360-8352},
	url = {http://www.sciencedirect.com/science/article/pii/S0360835206001161},
	doi = {10.1016/j.cie.2006.06.017},
	abstract = {Recently, emerging technologies related to various sensors, product identification, and wireless communication give us new opportunities for improving the efficiency of automotive maintenance operations, in particular, implementing predictive maintenance. The key point of predictive maintenance is to develop an algorithm that can analyze degradation status of automotive and make predictive maintenance decisions. In this study, as a basis for implementing the predictive maintenance of automotive engine oil, we propose an algorithm to determine the suitable change time of automotive engine oil by analyzing its degradation status with mission profile data. For this, we use several statistical methods such as factor analysis, discriminant and classification analysis, and regression analysis. We identify main factors of mission profile and engine oil quality with factor analysis. Subsequently, with regression analysis, we specify relations between main factors considering the types of mission profile of automotive: urban-mode and highway-mode. Based on them, we determine the proper change time of engine oil through discriminant and classification analysis. To evaluate the proposed approach, we carry out a case study and have discussion about limitations of our approach.},
	pages = {671--683},
	number = {4},
	journaltitle = {Computers \& Industrial Engineering},
	shortjournal = {Computers \& Industrial Engineering},
	author = {Jun, Hong-Bae and Kiritsis, Dimitris and Gambera, Mario and Xirouchakis, Paul},
	urldate = {2017-04-22},
	date = {2006-12},
	keywords = {Predictive maintenance, Statistical methods, Degradation, Engine oil, Mission profile data},
	file = {ScienceDirect Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/9GVSC3B2/Jun et al. - 2006 - Predictive algorithm to determine the suitable tim.pdf:application/pdf;ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/FAGD6N9V/S0360835206001161.html:text/html}
}

@article{singh_dynamic_2009,
	title = {Dynamic Multiple Fault Diagnosis: Mathematical Formulations and Solution Techniques},
	volume = {39},
	issn = {1083-4427},
	doi = {10.1109/TSMCA.2008.2007986},
	shorttitle = {Dynamic Multiple Fault Diagnosis},
	abstract = {Imperfect test outcomes, due to factors such as unreliable sensors, electromagnetic interference, and environmental conditions, manifest themselves as missed detections and false alarms. This paper develops near-optimal algorithms for dynamic multiple fault diagnosis ({DMFD}) problems in the presence of imperfect test outcomes. The {DMFD} problem is to determine the most likely evolution of component states, the one that best explains the observed test outcomes. Here, we discuss four formulations of the {DMFD} problem. These include the deterministic situation corresponding to perfectly observed coupled Markov decision processes to several partially observed factorial hidden Markov models ranging from the case where the imperfect test outcomes are functions of tests only to the case where the test outcomes are functions of faults and tests, as well as the case where the false alarms are associated with the nominal (fault free) case only. All these formulations are intractable {NP}-hard combinatorial optimization problems. Our solution scheme can be viewed as a two-level coordinated solution framework for the {DMFD} problem. At the top (coordination) level, we update the Lagrange multipliers (coordination variables, dual variables) using the subgradient method. At the bottom level, we use a dynamic programming technique (specifically, the Viterbi decoding or Max-sum algorithm) to solve each of the subproblems, one for each component state sequence. The key advantage of our approach is that it provides an approximate duality gap, which is a measure of the suboptimality of the {DMFD} solution. Computational results on real-world problems are presented. A detailed performance analysis of the proposed algorithm is also discussed.},
	pages = {160--176},
	number = {1},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {Singh, S. and Kodali, A. and Choi, K. and Pattipati, K. R. and Namburu, S. M. and Sean, S. C. and Prokhorov, D. V. and Qiao, L.},
	date = {2009-01},
	keywords = {automotive engineering, condition monitoring, decision theory, duality (mathematics), dynamic programming, fault diagnosis, fault trees, gradient methods, hidden Markov models, maintenance engineering, statistical testing, Lagrange multiplier, {NP}-hard combinatorial optimization problem, duality gap, dynamic multiple fault diagnosis, electromagnetic interference, environmental condition, false alarm, fault free, imperfect test, near-optimal algorithm, partially observed factorial hidden Markov model, perfectly observed coupled Markov decision process, subgradient method, unreliable sensor, Decoding, Heuristic algorithms, Lagrangian functions, Performance analysis, Testing, Viterbi algorithm, Dynamic faults, imperfect tests, intermittent faults, multiple fault diagnosis},
	file = {IEEE Xplore Abstract Record:/Users/srivathsanseshadri/Zotero/storage/79G9NF9N/4717835.html:text/html;IEEE Xplore Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/INFHGEH7/Singh et al. - 2009 - Dynamic Multiple Fault Diagnosis Mathematical For.pdf:application/pdf}
}

@book{miller_modeling_2014,
	location = {Indianapolis, {IN}},
	edition = {1 edition},
	title = {Modeling Techniques in Predictive Analytics with Python and R: A Guide to Data Science},
	isbn = {978-0-13-389206-2},
	shorttitle = {Modeling Techniques in Predictive Analytics with Python and R},
	abstract = {Master predictive analytics, from start to finish      Start with strategy and management  Master methods and build models  Transform your models into highly-effective code—in both Python and R     This one-of-a-kind book will help you use predictive analytics, Python, and R to solve real business problems and drive real competitive advantage. You’ll master predictive analytics through realistic case studies, intuitive data visualizations, and up-to-date code for both Python and R—not complex math.     Step by step, you’ll walk through defining problems, identifying data, crafting and optimizing models, writing effective Python and R code, interpreting results, and more. Each chapter focuses on one of today’s key applications for predictive analytics, delivering skills and knowledge to put models to work—and maximize their value.     Thomas W. Miller, leader of Northwestern University’s pioneering program in predictive analytics, addresses everything you need to succeed: strategy and management, methods and models, and technology and code.     If you’re new to predictive analytics, you’ll gain a strong foundation for achieving accurate, actionable results. If you’re already working in the field, you’ll master powerful new skills. If you’re familiar with either Python or R, you’ll discover how these languages complement each other, enabling you to do even more.     All data sets, extensive Python and R code, and additional examples available for download at http://www.ftpress.com/miller/        Python and R offer immense power in predictive analytics, data science, and big data. This book will help you leverage that power to solve real business problems, and drive real competitive advantage.     Thomas W. Miller’s unique balanced approach combines business context and quantitative tools, illuminating each technique with carefully explained code for the latest versions of Python and R. If you’re new to predictive analytics, Miller gives you a strong foundation for achieving accurate, actionable results. If you’re already a modeler, programmer, or manager, you’ll learn crucial skills you don’t already have.     Using Python and R, Miller addresses multiple business challenges, including segmentation, brand positioning, product choice modeling, pricing research, finance, sports, text analytics, sentiment analysis, and social network analysis. He illuminates the use of cross-sectional data, time series, spatial, and spatio-temporal data.     You’ll learn why each problem matters, what data are relevant, and how to explore the data you’ve identified. Miller guides you through conceptually modeling each data set with words and figures; and then modeling it again with realistic code that delivers actionable insights.     You’ll walk through model construction, explanatory variable subset selection, and validation, mastering best practices for improving out-of-sample predictive performance. Miller employs data visualization and statistical graphics to help you explore data, present models, and evaluate performance. Appendices include five complete case studies, and a detailed primer on modern data science methods.        Use Python and R to gain powerful, actionable, profitable insights about:       Advertising and promotion     Consumer preference and choice     Market baskets and related purchases     Economic forecasting     Operations management     Unstructured text and language     Customer sentiment     Brand and price     Sports team performance     And much more},
	pagetotal = {448},
	publisher = {Pearson {FT} Press},
	author = {Miller, Thomas W.},
	date = {2014-10-11}
}

@article{ruiz_new_2017,
	title = {A new criterion to validate and improve the classification process of {LAMDA} algorithm applied to diesel engines},
	volume = {60},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197617300283},
	doi = {10.1016/j.engappai.2017.02.005},
	abstract = {This work proposes a new criterion to validate and improve the classification efficiency of the Learning Algorithm Multivariable and Data Analysis ({LAMDA}) fuzzy algorithm, which is an algorithm that combines the concepts of neural networks architecture and fuzzy clustering. {LAMDA} is based on finding the Global Adequacy Degree ({GAD}) of one data (individual) to a class (functional state), considering the contributions of each descriptor or variable. {LAMDA} is capable of generating new classes after the training stage and it uses probability density functions ({PDF}) for the estimation of similarity analysis between classes in order to determine the grouping criterion. The {LAMDA} algorithm was used here to identify new functional states that were not included during the training stage. However, this algorithm induced significant uncertainties when the recognized classes, corresponding to engine operating modes, exhibited similar membership degree values ({MDV}). To solve this, a new criterion to validate functional states after recognition ({LAMDA}-{FAR}), based on the minimum and maximum distances among {MDV} was developed. Both {LAMDA} and {LAMDA}-{FAR} algorithms were used in supervised learning mode to classify a historical database obtained from an experimental mapping methodology of an automotive diesel engine operating under several steady state conditions. For each engine operating mode the engine speed (rpm), exhaust gas temperature (°C) and accelerator pedal position (\%) were measured as the representative variables to carry out the classification. Both algorithms were trained with 70\% of the historical database. The remaining 30\% of the data, as well as new engine operating modes (not taken into account during the training stage), were used to validate classifier results. It was found that the {LAMDA} algorithm alone was unable to properly classify similar engine operating modes, while the {LAMDA}-{FAR} algorithm showed 100\% efficiency for both known and unknown operating modes. This high efficiency and low computational cost tool can be used to improve engine control strategies based on experimental mapping methods, as well as to monitoring and controlling on-line vehicle performance.},
	pages = {117--127},
	journaltitle = {Engineering Applications of Artificial Intelligence},
	shortjournal = {Engineering Applications of Artificial Intelligence},
	author = {Ruiz, Frank A. and Isaza, Claudia V. and Agudelo, Andrés F. and Agudelo, John R.},
	urldate = {2017-04-22},
	date = {2017-04},
	keywords = {Fuzzy classification, {LAMDA}, Membership degrees, Diesel engines},
	file = {ScienceDirect Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/VW9ATX53/Ruiz et al. - 2017 - A new criterion to validate and improve the classi.pdf:application/pdf;ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/PPVZ4WU3/S0952197617300283.html:text/html}
}

@book{tan_introduction_2005,
	location = {Boston},
	edition = {1 edition},
	title = {Introduction to Data Mining},
	isbn = {978-0-321-32136-7},
	abstract = {Introduction to Data Mining presents fundamental concepts and algorithms for those learning data mining for the first time. Each major topic is organized into two chapters, beginning with basic concepts that provide necessary background for understanding each data mining technique, followed by more advanced concepts and algorithms.},
	pagetotal = {769},
	publisher = {Pearson},
	author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
	date = {2005-05-12}
}

@book{wickham_r_2017,
	location = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {1 edition},
	title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
	isbn = {978-1-4919-1039-9},
	shorttitle = {R for Data Science},
	abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, {RStudio}, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible.Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You’ll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you’ve learned along the way.You’ll learn how to:Wrangle—transform your datasets into a form convenient for {analysisProgram}—learn powerful R tools for solving data problems with greater clarity and {easeExplore}—examine your data, generate hypotheses, and quickly test {themModel}—provide a low-dimensional summary that captures true "signals" in your {datasetCommunicate}—learn R Markdown for integrating prose, code, and results},
	pagetotal = {522},
	publisher = {O'Reilly Media},
	author = {Wickham, Hadley and Grolemund, Garrett},
	date = {2017-01-05}
}

@online{noauthor_image:_nodate,
	title = {Image:},
	url = {https://www.google.com/imgres?imgurl=http://t3.gstatic.com/images%3Fq%3Dtbn:ANd9GcTwSNNRnnOmiKf7QYzhAqW7BcSjqKBfG-z-nB2xteJVN4Vngy9k&imgrefurl=http://books.google.com/books/about/Survey_Methodology.html%3Fid%3Dctow8zWdyFgC%26source%3Dkp_cover&h=900&w=628&tbnid=j5Whh3SkQfQaQM:&tbnh=132&tbnw=91&usg=__MT4v6mHdmJZ4WOAsDemChnn6JrQ=&vet=10ahUKEwiYx8qshN_TAhWj14MKHVK7BF0Q_B0IeTAK..i&docid=Y02BKqQNnxzYdM&itg=1&client=safari&sa=X&ved=0ahUKEwiYx8qshN_TAhWj14MKHVK7BF0Q_B0IeTAK},
	shorttitle = {Image},
	abstract = {Found on Google from},
	urldate = {2017-05-08},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/ZZ98X4HA/imgres.html:text/html}
}

@book{groves_survey_2009,
	location = {Hoboken, N.J},
	edition = {2 edition},
	title = {Survey Methodology},
	isbn = {978-0-470-46546-2},
	abstract = {Praise for the First Edition:  "The book makes a valuable contribution by synthesizing current research and identifying areas for future investigation for each aspect of the survey process." —Journal of the American Statistical Association "Overall, the high quality of the text material is matched by the quality of writing . . ." —Public Opinion Quarterly ". . . it should find an audience everywhere surveys are being conducted." —Technometrics This new edition of Survey Methodology continues to provide a state-of-the-science presentation of essential survey methodology topics and techniques. The volume's six world-renowned authors have updated this Second Edition to present newly emerging approaches to survey research and provide more comprehensive coverage of the major considerations in designing and conducting a sample survey. Key topics in survey methodology are clearly explained in the book's chapters, with coverage including sampling frame evaluation, sample design, development of questionnaires, evaluation of questions, alternative modes of data collection, interviewing, nonresponse, post-collection processing of survey data, and practices for maintaining scientific integrity. Acknowledging the growing advances in research and technology, the Second Edition features:  Updated explanations of sampling frame issues for mobile telephone and web surveys  New scientific insight on the relationship between nonresponse rates and nonresponse errors   Restructured discussion of ethical issues in survey research, emphasizing the growing research results on privacy, informed consent, and confidentiality issues   The latest research findings on effective questionnaire development techniques   The addition of 50\% more exercises at the end of each chapter, illustrating basic principles of survey design   An expanded {FAQ} chapter that addresses the concerns that accompany newly established methods   Providing valuable and informative perspectives on the most modern methods in the field, Survey Methodology, Second Edition is an ideal book for survey research courses at the upper-undergraduate and graduate levels. It is also an indispensable reference for practicing survey methodologists and any professional who employs survey research methods.},
	pagetotal = {488},
	publisher = {Wiley},
	author = {Groves, Robert M. and Jr, Floyd J. Fowler and Couper, Mick P. and Lepkowski, James M. and Singer, Eleanor and Tourangeau, Roger},
	date = {2009-07-14}
}

@book{groves_survey_2009-1,
	location = {Hoboken, N.J},
	edition = {2 edition},
	title = {Survey Methodology},
	isbn = {978-0-470-46546-2},
	abstract = {Praise for the First Edition:  "The book makes a valuable contribution by synthesizing current research and identifying areas for future investigation for each aspect of the survey process." —Journal of the American Statistical Association "Overall, the high quality of the text material is matched by the quality of writing . . ." —Public Opinion Quarterly ". . . it should find an audience everywhere surveys are being conducted." —Technometrics This new edition of Survey Methodology continues to provide a state-of-the-science presentation of essential survey methodology topics and techniques. The volume's six world-renowned authors have updated this Second Edition to present newly emerging approaches to survey research and provide more comprehensive coverage of the major considerations in designing and conducting a sample survey. Key topics in survey methodology are clearly explained in the book's chapters, with coverage including sampling frame evaluation, sample design, development of questionnaires, evaluation of questions, alternative modes of data collection, interviewing, nonresponse, post-collection processing of survey data, and practices for maintaining scientific integrity. Acknowledging the growing advances in research and technology, the Second Edition features:  Updated explanations of sampling frame issues for mobile telephone and web surveys  New scientific insight on the relationship between nonresponse rates and nonresponse errors   Restructured discussion of ethical issues in survey research, emphasizing the growing research results on privacy, informed consent, and confidentiality issues   The latest research findings on effective questionnaire development techniques   The addition of 50\% more exercises at the end of each chapter, illustrating basic principles of survey design   An expanded {FAQ} chapter that addresses the concerns that accompany newly established methods   Providing valuable and informative perspectives on the most modern methods in the field, Survey Methodology, Second Edition is an ideal book for survey research courses at the upper-undergraduate and graduate levels. It is also an indispensable reference for practicing survey methodologists and any professional who employs survey research methods.},
	pagetotal = {488},
	publisher = {Wiley},
	author = {Groves, Robert M. and Jr, Floyd J. Fowler and Couper, Mick P. and Lepkowski, James M. and Singer, Eleanor and Tourangeau, Roger},
	date = {2009-07-14}
}

@online{street_4._2016,
	title = {4. Conclusion},
	url = {http://www.pewresearch.org/2016/01/07/likely-voters-conclusion/},
	titleaddon = {Pew Research Center},
	author = {Street, 1615 L. and {NW} and Washington, Suite 800 and Inquiries, DC 20036 202 419 4300 {\textbar} Main 202 419 4349 {\textbar} Fax 202 419 4372 {\textbar} Media},
	urldate = {2017-05-08},
	date = {2016-01-07},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/2S62RAUE/likely-voters-conclusion.html:text/html}
}

@article{patil_optimization_2011,
	title = {Optimization of direct conversion of wet algae to biodiesel under supercritical methanol conditions},
	volume = {102},
	issn = {0960-8524},
	url = {http://www.sciencedirect.com/science/article/pii/S0960852410010126},
	doi = {10.1016/j.biortech.2010.06.031},
	series = {Special Issue: Biofuels - {II}: Algal Biofuels and Microbial Fuel Cells},
	abstract = {This study demonstrated a one-step process for direct liquefaction and conversion of wet algal biomass containing about 90\% of water to biodiesel under supercritical methanol conditions. This one-step process enables simultaneous extraction and transesterification of wet algal biomass. The process conditions are milder than those required for pyrolysis and prevent the formation of by-products. In the proposed process, fatty acid methyl esters ({FAMEs}) can be produced from polar phospholipids, free fatty acids, and triglycerides. A response surface methodology ({RSM}) was used to analyze the influence of the three process variables, namely, the wet algae to methanol (wt./vol.) ratio, the reaction temperature, and the reaction time, on the {FAMEs} conversion. Algal biodiesel samples were analyzed by {ATR}-{FTIR} and {GC}–{MS}. Based on the experimental analysis and {RSM} study, optimal conditions for this process are reported as: wet algae to methanol (wt./vol.) ratio of around 1:9, reaction temperature and time of about 255 °C, and 25 min respectively. This single-step process can potentially be an energy efficient and economical route for algal biodiesel production.},
	pages = {118--122},
	number = {1},
	journaltitle = {Bioresource Technology},
	shortjournal = {Bioresource Technology},
	author = {Patil, Prafulla D. and Gude, Veera Gnaneswar and Mannarswamy, Aravind and Deng, Shuguang and Cooke, Peter and Munson-{McGee}, Stuart and Rhodes, Isaac and Lammers, Pete and Nirmalakhandan, Nagamany},
	urldate = {2017-05-13},
	date = {2011-01},
	keywords = {Biodiesel, Wet algae, Supercritical methanol, Response surface methodology},
	file = {ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/9SSGZDRQ/S0960852410010126.html:text/html}
}

@online{noauthor_counties_nodate,
	title = {The Counties That Flipped From Obama To Trump, In 3 Charts},
	url = {http://www.npr.org/2016/11/15/502032052/lots-of-people-voted-for-obama-and-trump-heres-where-in-3-charts},
	abstract = {Many counties in Rust Belt states like Wisconsin, Iowa and Pennsylvania that had backed President Obama just four years ago were crucial to Donald Trump's victory.},
	titleaddon = {{NPR}.org},
	urldate = {2017-05-17},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/5WIQ2GN4/lots-of-people-voted-for-obama-and-trump-heres-where-in-3-charts.html:text/html}
}

@online{noauthor_big_2015,
	title = {The Big Issues Of The 2016 Campaign},
	url = {https://fivethirtyeight.com/features/year-ahead-project/},
	abstract = {With less than a year to go before the election, {FiveThirtyEight}’s staff members took a detailed look at seven issues we cover regularly, examining the positions of the presidential candidates and …},
	titleaddon = {{FiveThirtyEight}},
	urldate = {2017-05-17},
	date = {2015-11-19},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/P696I54Z/year-ahead-project.html:text/html}
}

@online{noauthor_union_nodate,
	title = {Union membership rates by state in 2016 : The Economics Daily: U.S. Bureau of Labor Statistics},
	url = {https://www.bls.gov/opub/ted/2017/union-membership-rates-by-state-in-2016.htm},
	shorttitle = {Union membership rates by state in 2016},
	abstract = {The union membership rate in the United States—the percentage of wage and salary workers who were members of unions—was 10.7 percent in 2016. Twenty-seven states and the District of Columbia had union membership rates below the U.S. average, while 23 states had rates above it.},
	urldate = {2017-05-19},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/VWINSPIX/union-membership-rates-by-state-in-2016.html:text/html}
}

@online{noauthor_table_nodate,
	title = {Table 5. Union affiliation of employed wage and salary workers by state},
	url = {https://www.bls.gov/news.release/union2.t05.htm},
	urldate = {2017-05-19},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/SIJSJ4IC/union2.t05.html:text/html}
}

@online{noauthor_big_2015-1,
	title = {The Big Issues Of The 2016 Campaign},
	url = {https://fivethirtyeight.com/features/year-ahead-project/},
	abstract = {With less than a year to go before the election, {FiveThirtyEight}’s staff members took a detailed look at seven issues we cover regularly, examining the positions of the presidential candidates and …},
	titleaddon = {{FiveThirtyEight}},
	urldate = {2017-05-20},
	date = {2015-11-19},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/T6XFQ3XU/year-ahead-project.html:text/html}
}

@article{bonett_sample_2000,
	title = {Sample size requirements for estimating Pearson, Kendall and Spearman correlations},
	volume = {65},
	url = {http://link.springer.com/article/10.1007/BF02294183},
	pages = {23--28},
	number = {1},
	journaltitle = {Psychometrika},
	author = {Bonett, Douglas G. and Wright, Thomas A.},
	urldate = {2017-05-25},
	date = {2000},
	file = {art%3A10.1007%2FBF02294183.pdf:/Users/srivathsanseshadri/Zotero/storage/68ZW9MUB/art%3A10.1007%2FBF02294183.pdf:application/pdf}
}

@online{noauthor_:_nodate,
	title = {:= {TQMP}.{ORG} =:},
	url = {http://www.tqmp.org/},
	urldate = {2017-05-25},
	file = {\:= TQMP.ORG =\::/Users/srivathsanseshadri/Zotero/storage/77WP5DDU/www.tqmp.org.html:text/html}
}

@online{noauthor_p029.pdf_nodate,
	title = {p029.pdf},
	url = {http://www.tqmp.org/RegularArticles/vol10-1/p029/p029.pdf},
	urldate = {2017-05-25},
	file = {p029.pdf:/Users/srivathsanseshadri/Zotero/storage/KPN4NNHX/p029.pdf:application/pdf}
}

@article{weaver_spss_2014,
	title = {An {SPSS} Macro to Compute Confidence Intervals for Pearson’s Correlation},
	volume = {10},
	issn = {2292-1354},
	url = {http://www.tqmp.org/RegularArticles/vol10-1/p029},
	doi = {10.20982/tqmp.10.1.p029},
	pages = {29--39},
	number = {1},
	journaltitle = {The Quantitative Methods for Psychology},
	author = {Weaver, Bruce and Koopman, Ray},
	urldate = {2017-05-25},
	date = {2014-04-01}
}

@online{noauthor_union_nodate-1,
	title = {Union Members Summary},
	url = {https://www.bls.gov/news.release/union2.nr0.htm},
	urldate = {2017-06-01},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/DDIUVU3T/union2.nr0.html:text/html}
}

@online{noauthor_table_nodate-1,
	title = {Table 5. Union affiliation of employed wage and salary workers by state},
	url = {https://www.bls.gov/news.release/union2.t05.htm},
	urldate = {2017-06-01},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/3ZWZMS69/union2.t05.html:text/html}
}

@book{montgomery_introduction_2012,
	location = {Hoboken, {NJ}},
	edition = {5 edition},
	title = {Introduction to Linear Regression Analysis},
	isbn = {978-0-470-54281-1},
	abstract = {Praise for the Fourth Edition  "As with previous editions, the authors have produced a leading textbook on regression." —Journal of the American Statistical Association A comprehensive and up-to-date introduction to the fundamentals of regression analysis Introduction to Linear Regression Analysis, Fifth Edition continues to present both the conventional and less common uses of linear regression in today’s cutting-edge scientific research. The authors blend both theory and application to equip readers with an understanding of the basic principles needed to apply regression model-building techniques in various fields of study, including engineering, management, and the health sciences. Following a general introduction to regression modeling, including typical applications, a host of technical tools are outlined such as basic inference procedures, introductory aspects of model adequacy checking, and polynomial regression models and their variations. The book then discusses how transformations and weighted least squares can be used to resolve problems of model inadequacy and also how to deal with influential observations. The Fifth Edition features numerous newly added topics, including:   A chapter on regression analysis of time series data that presents the Durbin-Watson test and other techniques for detecting autocorrelation as well as parameter estimation in time series regression models Regression models with random effects in addition to a discussion on subsampling and the importance of the mixed model Tests on individual regression coefficients and subsets of coefficients Examples of current uses of simple linear regression models and the use of multiple regression models for understanding patient satisfaction data.  In addition to Minitab, {SAS}, and S-{PLUS}, the authors have incorporated {JMP} and the freely available R software to illustrate the discussed techniques and procedures in this new edition. Numerous exercises have been added throughout, allowing readers to test their understanding of the material. Introduction to Linear Regression Analysis, Fifth Edition is an excellent book for statistics and engineering courses on regression at the upper-undergraduate and graduate levels. The book also serves as a valuable, robust resource for professionals in the fields of engineering, life and biological sciences, and the social sciences.},
	pagetotal = {672},
	publisher = {Wiley},
	author = {Montgomery, Douglas C. and Peck, Elizabeth A. and Vining, G. Geoffrey},
	date = {2012-04-09}
}

@book{montgomery_introduction_2012-1,
	location = {Hoboken, {NJ}},
	edition = {5 edition},
	title = {Introduction to Linear Regression Analysis},
	isbn = {978-0-470-54281-1},
	abstract = {Praise for the Fourth Edition  "As with previous editions, the authors have produced a leading textbook on regression." —Journal of the American Statistical Association A comprehensive and up-to-date introduction to the fundamentals of regression analysis Introduction to Linear Regression Analysis, Fifth Edition continues to present both the conventional and less common uses of linear regression in today’s cutting-edge scientific research. The authors blend both theory and application to equip readers with an understanding of the basic principles needed to apply regression model-building techniques in various fields of study, including engineering, management, and the health sciences. Following a general introduction to regression modeling, including typical applications, a host of technical tools are outlined such as basic inference procedures, introductory aspects of model adequacy checking, and polynomial regression models and their variations. The book then discusses how transformations and weighted least squares can be used to resolve problems of model inadequacy and also how to deal with influential observations. The Fifth Edition features numerous newly added topics, including:   A chapter on regression analysis of time series data that presents the Durbin-Watson test and other techniques for detecting autocorrelation as well as parameter estimation in time series regression models Regression models with random effects in addition to a discussion on subsampling and the importance of the mixed model Tests on individual regression coefficients and subsets of coefficients Examples of current uses of simple linear regression models and the use of multiple regression models for understanding patient satisfaction data.  In addition to Minitab, {SAS}, and S-{PLUS}, the authors have incorporated {JMP} and the freely available R software to illustrate the discussed techniques and procedures in this new edition. Numerous exercises have been added throughout, allowing readers to test their understanding of the material. Introduction to Linear Regression Analysis, Fifth Edition is an excellent book for statistics and engineering courses on regression at the upper-undergraduate and graduate levels. The book also serves as a valuable, robust resource for professionals in the fields of engineering, life and biological sciences, and the social sciences.},
	pagetotal = {672},
	publisher = {Wiley},
	author = {Montgomery, Douglas C. and Peck, Elizabeth A. and Vining, G. Geoffrey},
	date = {2012-04-09}
}

@book{wheeler_normality_2000,
	location = {Knoxville, Tenn},
	title = {Normality And the Process Behavior Chart},
	isbn = {978-0-945320-56-2},
	abstract = {Normality is not a prerequisite for a process behavior chart or an unavoidable consequence of a predictable process. This first careful and complete examination of the relationship between the normal distribution and the process behavior chart clears up much of the confusion surrounding this subject, and it will help you overcome the superstitions that have hampered the effective use of this valuable tool. Topics include: the history of the normal distribution and early attempts to use it to analyze data; the shortcomings of procedures that check for goodness of fit; how to really compute parts-per-million defect rates; the fundamental difference between theory and practice; the relationship between R\&D and {SPC}; the linkage between the normal distribution and basic constants in chart formulas; how non-normal distributions affect these basic constants; how three sigma limits work with over 1100 different probability models; the shortcomings of average run lengths as a tool for sensitivity analysis.},
	pagetotal = {156},
	publisher = {{SPC} {PRESS}},
	author = {Wheeler, Donald J.},
	date = {2000-01-03}
}

@article{zheng_semiparametric_2018,
	title = {Semiparametric time series regression modeling with a diverging number of parameters},
	volume = {72},
	issn = {1467-9574},
	url = {https://onlinelibrary-wiley-com.turing.library.northwestern.edu/doi/abs/10.1111/stan.12121},
	doi = {10.1111/stan.12121},
	abstract = {Variable selection and error structure determination of a partially linear model with time series errors are important issues. In this paper, we investigate the regression coefficient and autoregressive order shrinkage and selection via the smoothly clipped absolute deviation penalty for a partially linear model with a divergent number of covariates and finite order autoregressive time series errors. Both consistency and asymptotic normality of the proposed penalized estimators are derived. The oracle property of the resultant estimators is proved. Simulation studies are carried out to assess the finite-sample performance of the proposed procedure. A real data analysis is made to illustrate the usefulness of the proposed procedure as well.},
	pages = {90--108},
	number = {2},
	journaltitle = {Statistica Neerlandica},
	author = {Zheng, Shengchao and Li, Degao},
	date = {2018-05-01},
	langid = {english},
	keywords = {high-dimensional data, autoregressive error, consistency, asymptotic normality},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/92XQVBSW/Zheng and Li - 2018 - Semiparametric time series regression modeling wit.pdf:application/pdf;Snapshot:/Users/srivathsanseshadri/Zotero/storage/DDHEBC2W/stan.html:text/html}
}

@article{engle_autoregressive_1982,
	title = {Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation},
	volume = {50},
	rights = {Copyright Econometric Society Jul 1982},
	issn = {00129682},
	url = {https://search-proquest-com.turing.library.northwestern.edu/docview/214655190/abstract/22664B7D93DC4719PQ/1},
	abstract = {Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption, a new class of stochastic processes called autore-gressive conditional heteroscedastic ({ARCH}) processes are introduced in this paper. These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance.A regression model is then introduced with disturbances following an {ARCH} process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an {ARCH} process, the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared {OLS} residuals.This model is used to estimate the means and variances of inflation in the U.K. The {ARCH} effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.},
	pages = {987},
	number = {4},
	journaltitle = {Econometrica (pre-1986); Evanston},
	author = {Engle, Robert F.},
	date = {1982-07},
	keywords = {Business And Economics},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/WVIZDPG7/Engle - 1982 - Autoregressive Conditional Heteroscedasticity with.pdf:application/pdf}
}

@article{hyndman_optimal_2011,
	title = {Optimal combination forecasts for hierarchical time series},
	volume = {55},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947311000971},
	doi = {10.1016/j.csda.2011.03.006},
	abstract = {In many applications, there are multiple time series that are hierarchically organized and can be aggregated at several different levels in groups based on products, geography or some other features. We call these “hierarchical time series”. They are commonly forecast using either a “bottom-up” or a “top-down” method. In this paper we propose a new approach to hierarchical forecasting which provides optimal forecasts that are better than forecasts produced by either a top-down or a bottom-up approach. Our method is based on independently forecasting all series at all levels of the hierarchy and then using a regression model to optimally combine and reconcile these forecasts. The resulting revised forecasts add up appropriately across the hierarchy, are unbiased and have minimum variance amongst all combination forecasts under some simple assumptions. We show in a simulation study that our method performs well compared to the top-down approach and the bottom-up method. We demonstrate our proposed method by forecasting Australian tourism demand where the data are disaggregated by purpose of travel and geographical region.},
	pages = {2579--2589},
	number = {9},
	journaltitle = {Computational Statistics \& Data Analysis},
	shortjournal = {Computational Statistics \& Data Analysis},
	author = {Hyndman, Rob J. and Ahmed, Roman A. and Athanasopoulos, George and Shang, Han Lin},
	date = {2011-09-01},
	keywords = {Bottom-up forecasting, Combining forecasts, {GLS} regression, Hierarchical forecasting, Reconciling forecasts, Top-down forecasting},
	file = {ScienceDirect Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/5CJHPQST/Hyndman et al. - 2011 - Optimal combination forecasts for hierarchical tim.pdf:application/pdf;ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/5UEVNAX3/S0167947311000971.html:text/html}
}

@article{hyndman_fast_2016,
	title = {Fast computation of reconciled forecasts for hierarchical and grouped time series},
	volume = {97},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S016794731500290X},
	doi = {10.1016/j.csda.2015.11.007},
	abstract = {It is shown that the least squares approach to reconciling hierarchical time series forecasts can be extended to much more general collections of time series with aggregation constraints. The constraints arise due to the need for forecasts of collections of time series to add up in the same way as the observed time series. It is also shown that the computations involved can be handled efficiently by exploiting the structure of the associated design matrix, or by using sparse matrix routines. The proposed algorithms make forecast reconciliation feasible in business applications involving very large numbers of time series.},
	pages = {16--32},
	journaltitle = {Computational Statistics \& Data Analysis},
	shortjournal = {Computational Statistics \& Data Analysis},
	author = {Hyndman, Rob J. and Lee, Alan J. and Wang, Earo},
	date = {2016-05-01},
	keywords = {Combining forecasts, Reconciling forecasts, Grouped time series, Hierarchical time series, Weighted least squares},
	file = {ScienceDirect Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/QBBFF6FH/Hyndman et al. - 2016 - Fast computation of reconciled forecasts for hiera.pdf:application/pdf;ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/3GF63CDG/S016794731500290X.html:text/html}
}

@article{lian_ensemble_2014,
	title = {Ensemble of extreme learning machine for landslide displacement prediction based on time series analysis},
	volume = {24},
	issn = {0941-0643, 1433-3058},
	url = {https://link-springer-com.turing.library.northwestern.edu/article/10.1007/s00521-013-1446-3},
	doi = {10.1007/s00521-013-1446-3},
	abstract = {Landslide hazard is a complex nonlinear dynamical system with uncertainty. The evolution of landslide is influenced by many factors such as tectonic, rainfall and reservoir level fluctuation. Using a time series model, total accumulative displacement of landslide can be divided into the trend component displacement and the periodic component displacement according to the response relation between dynamic changes in landslide displacement and inducing factors. In this paper, a novel neural network technique called ensemble of extreme learning machine (E-{ELM}) is proposed to investigate the interactions of different inducing factors affecting the evolution of landslide. Grey relational analysis is used to sieve out the more influential inducing factors as the inputs in E-{ELM}. Trend component displacement and periodic component displacement are forecasted, respectively; then, total predictive displacement is obtained by adding the calculated predictive displacement value of each sub. Performances of our model are evaluated by using real data from Baishuihe landslide in the Three Gorges Reservoir of China, and it provides a good representation of the measured slide displacement behavior.},
	pages = {99--107},
	number = {1},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Lian, Cheng and Zeng, Zhigang and Yao, Wei and Tang, Huiming},
	urldate = {2018-05-13},
	date = {2014-01-01},
	langid = {english},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/XUKGIAEF/Lian et al. - 2014 - Ensemble of extreme learning machine for landslide.pdf:application/pdf;Snapshot:/Users/srivathsanseshadri/Zotero/storage/9Z6ITP5R/s00521-013-1446-3.html:text/html}
}

@article{yoon_feature_2005,
	title = {Feature subset selection and feature ranking for multivariate time series},
	volume = {17},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2005.144},
	abstract = {Feature subset selection ({FSS}) is a known technique to preprocess the data before performing any data mining tasks, e.g., classification and clustering. {FSS} provides both cost-effective predictors and a better understanding of the underlying process that generated the data. We propose a family of novel unsupervised methods for feature subset selection from multivariate time series ({MTS}) based on common principal component analysis, termed {CLeVer}. Traditional {FSS} techniques, such as recursive feature elimination ({RFE}) and Fisher criterion ({FC}), have been applied to {MTS} data sets, e.g., brain computer interface ({BCI}) data sets. However, these techniques may lose the correlation information among features, while our proposed techniques utilize the properties of the principal component analysis to retain that information. In order to evaluate the effectiveness of our selected subset of features, we employ classification as the target data mining task. Our exhaustive experiments show that {CLeVer} outperforms {RFE}, {FC}, and random selection by up to a factor of two in terms of the classification accuracy, while taking up to 2 orders of magnitude less processing time than {RFE} and {FC}.},
	pages = {1186--1198},
	number = {9},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Yoon, H. and Yang, K. and Shahabi, C.},
	date = {2005-09},
	keywords = {data analysis, data mining, feature extraction, pattern classification, principal component analysis, time series, unsupervised learning, {BCI} data sets, {CLeVer}, {FC}, {FSS}, Fisher criterion, {MTS} data sets, {RFE}, brain computer interface, feature ranking, feature representation, feature subset selection, multivariate time series, recursive feature elimination, unsupervised methods, Application software, Brain computer interfaces, Electroencephalography, Frequency selective surfaces, Humans, Time measurement, Time series analysis, Index Terms- Data mining, feature evaluation and selection, feature extraction or construction, feature representation.},
	file = {IEEE Xplore Abstract Record:/Users/srivathsanseshadri/Zotero/storage/H2KINX4S/1490526.html:text/html}
}

@online{noauthor_r_nodate,
	title = {r package for time series feature selection - Google Search},
	url = {https://www.google.com/search?ei=oBAMW-PpGoGw0PEPt--peA&q=r+package+for+time+series+feature+selection&oq=r+package+for+time+series+feature+selection&gs_l=psy-ab.3..33i22i29i30k1.6221.20681.0.20840.69.50.4.1.1.0.289.5885.0j27j8.35.0....0...1c.1.64.psy-ab..37.32.4560...0j33i21k1j33i160k1j0i67k1j0i131k1j0i131i67k1j0i22i30k1.0.xI__7Ju4HI8},
	urldate = {2018-05-28}
}

@article{haselimashhadi_penalised_2014,
	title = {Penalised inference for autoregressive moving average models with time-dependent predictors},
	url = {http://arxiv.org/abs/1412.5870},
	abstract = {Linear models that contain a time-dependent response and explanatory variables have attracted much interest in recent years. The most general form of the existing approaches is of a linear regression model with autoregressive moving average residuals. The addition of the moving average component results in a complex model with a very challenging implementation. In this paper, we propose to account for the time dependency in the data by explicitly adding autoregressive terms of the response variable in the linear model. In addition, we consider an autoregressive process for the errors in order to capture complex dynamic relationships parsimoniously. To broaden the application of the model, we present an \$l\_1\$ penalized likelihood approach for the estimation of the parameters and show how the adaptive lasso penalties lead to an estimator which enjoys the oracle property. Furthermore, we prove the consistency of the estimators with respect to the mean squared prediction error in high-dimensional settings, an aspect that has not been considered by the existing time-dependent regression models. A simulation study and real data analysis show the successful applications of the model on financial data on stock indexes.},
	journaltitle = {{arXiv}:1412.5870 [stat]},
	author = {Haselimashhadi, Hamed and Vinciotti, Veronica},
	date = {2014-12-18},
	eprinttype = {arxiv},
	eprint = {1412.5870},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1412.5870 PDF:/Users/srivathsanseshadri/Zotero/storage/NPMJ6SDE/Haselimashhadi and Vinciotti - 2014 - Penalised inference for autoregressive moving aver.pdf:application/pdf;arXiv.org Snapshot:/Users/srivathsanseshadri/Zotero/storage/DP7J36DS/1412.html:text/html}
}

@article{johansson_local_2009,
	title = {Local and Global Effects of Climate on Dengue Transmission in Puerto Rico},
	volume = {3},
	issn = {1935-2735},
	url = {http://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0000382},
	doi = {10.1371/journal.pntd.0000382},
	abstract = {The four dengue viruses, the agents of dengue fever and dengue hemorrhagic fever in humans, are transmitted predominantly by the mosquito Aedes aegypti. The abundance and the transmission potential of Ae. aegypti are influenced by temperature and precipitation. While there is strong biological evidence for these effects, empirical studies of the relationship between climate and dengue incidence in human populations are potentially confounded by seasonal covariation and spatial heterogeneity. Using 20 years of data and a statistical approach to control for seasonality, we show a positive and statistically significant association between monthly changes in temperature and precipitation and monthly changes in dengue transmission in Puerto Rico. We also found that the strength of this association varies spatially, that this variation is associated with differences in local climate, and that this relationship is consistent with laboratory studies of the impacts of these factors on vector survival and viral replication. These results suggest the importance of temperature and precipitation in the transmission of dengue viruses and suggest a reason for their spatial heterogeneity. Thus, while dengue transmission may have a general system, its manifestation on a local scale may differ from global expectations.},
	pages = {e382},
	number = {2},
	journaltitle = {{PLOS} Neglected Tropical Diseases},
	shortjournal = {{PLOS} Neglected Tropical Diseases},
	author = {Johansson, Michael A. and Dominici, Francesca and Glass, Gregory E.},
	urldate = {2018-05-31},
	date = {2009-02-17},
	langid = {english},
	keywords = {Rain, Puerto Rico, Dengue virus, Seasons, Aedes aegypti, Dengue fever, Hemorrhagic fever viruses, Viral transmission and infection},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/UMCZZ6IT/Johansson et al. - 2009 - Local and Global Effects of Climate on Dengue Tran.pdf:application/pdf;Snapshot:/Users/srivathsanseshadri/Zotero/storage/8EAQU7F5/article.html:text/html}
}

@online{noauthor_crude_nodate,
	title = {Crude Oil Price Trends: Their Impact on Soybean Complex Prices and Biodiesel Economics {\textbar} Agricultural Marketing Resource Center},
	url = {https://www.agmrc.org/renewable-energy/renewable-energy-climate-change-report/renewable-energy-climate-change-report/august-2015-report/crude-oil-price-trends-their-impact-on-soybean-complex-prices-and-biodiesel-economics/},
	urldate = {2018-06-01},
	file = {Crude Oil Price Trends\: Their Impact on Soybean Complex Prices and Biodiesel Economics | Agricultural Marketing Resource Center:/Users/srivathsanseshadri/Zotero/storage/GFHFDM8U/crude-oil-price-trends-their-impact-on-soybean-complex-prices-and-biodiesel-economics.html:text/html}
}

@online{noauthor_back_2015,
	title = {Back to the Future: Using Historical Dengue Data to Predict the Next Epidemic},
	url = {https://obamawhitehouse.archives.gov/blog/2015/06/05/back-future-using-historical-dengue-data-predict-next-epidemic},
	shorttitle = {Back to the Future},
	abstract = {Several departments in the U.S. Federal Government (Department of Health and Human Services, Department of Defense, and Department of Commerce) have joined together, with the support of the National Science and Technology Council ({NSTC}), to design an infectious disease forecasting project with the aim of galvanizing efforts to predict local epidemics of dengue.},
	titleaddon = {whitehouse.gov},
	urldate = {2018-06-11},
	date = {2015-06-05},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/ZWV8NGNA/back-future-using-historical-dengue-data-predict-next-epidemic.html:text/html}
}

@book{kuhn_applied_2013,
	edition = {1st ed. 2013, Corr. 2nd printing 2018 edition},
	title = {Applied Predictive Modeling},
	abstract = {Winner of the 2014 Technometrics Ziegel Prize for Outstanding {BookApplied} Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning.  The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems.  Addressing practical concerns extends beyond model fitting to topics such as handling class imbalance, selecting predictors, and pinpointing causes of poor model performance―all of which are problems that occur frequently in practice. The text illustrates all parts of the modeling process through many hands-on, real-life examples.  And every chapter contains extensive R code for each step of the process.  The data sets and corresponding code are available in the book's companion {AppliedPredictiveModeling} R package, which is freely available on the {CRAN} archive. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses.  To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. Readers and students interested in implementing the methods should have some basic knowledge of R.  And a handful of the more advanced topics require some mathematical knowledge.},
	pagetotal = {600},
	publisher = {Springer},
	author = {Kuhn, Max and Johnson, Kjell},
	date = {2013-05-17}
}

@book{kuhn_applied_2013-1,
	location = {New York},
	title = {Applied Predictive Modeling},
	isbn = {978-1-4614-6848-6},
	url = {//www.springer.com/us/book/9781461468486},
	abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. The text illustrates all parts of the modeling process through many hands-on, real-life examples, and every chapter contains extensive R code for each step of the process. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner’s reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book’s R package.This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.},
	publisher = {Springer-Verlag},
	author = {Kuhn, Max and Johnson, Kjell},
	urldate = {2018-06-12},
	date = {2013},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/QLJBTF86/9781461468486.html:text/html}
}

@article{martinez_methodology_2017,
	title = {A methodology for applying {\textless}Emphasis Type="Italic"{\textgreater}k{\textless}/Emphasis{\textgreater}-nearest neighbor to time series forecasting},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/article/10.1007/s10462-017-9593-z},
	doi = {10.1007/s10462-017-9593-z},
	abstract = {In this paper a methodology for applying k-nearest neighbor regression on a time series forecasting context is developed. The goal is to devise an automatic tool, i.e., a tool that can work without human intervention; furthermore, the methodology should be effective and efficient, so that it can be applied to accurately forecast a great number of time series. In order to be incorporated into our methodology, several modeling and preprocessing techniques are analyzed and assessed using the N3 competition data set. One interesting feature of the proposed methodology is that it resolves the selection of important modeling parameters, such as k or the input variables, combining several models with different parameters. In spite of the simplicity of k-{NN} regression, our methodology seems to be quite effective.},
	pages = {1--19},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Martínez, Francisco and Frías, María Pilar and Pérez, María Dolores and Rivera, Antonio Jesús},
	urldate = {2018-06-12},
	date = {2017-11-21},
	langid = {english},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/HXBYDW58/Martínez et al. - 2017 - A methodology for applying Emphasis Type=Italic.pdf:application/pdf;Snapshot:/Users/srivathsanseshadri/Zotero/storage/PPXJBTZZ/s10462-017-9593-z.html:text/html}
}

@article{tyralis_variable_2017,
	title = {Variable Selection in Time Series Forecasting Using Random Forests},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1999-4893/10/4/114},
	doi = {10.3390/a10040114},
	abstract = {Time series forecasting using machine learning algorithms has gained popularity recently. Random forest is a machine learning algorithm implemented in time series forecasting; however, most of its forecasting properties have remained unexplored. Here we focus on assessing the performance of random forests in one-step forecasting using two large datasets of short time series with the aim to suggest an optimal set of predictor variables. Furthermore, we compare its performance to benchmarking methods. The first dataset is composed by 16,000 simulated time series from a variety of Autoregressive Fractionally Integrated Moving Average ({ARFIMA}) models. The second dataset consists of 135 mean annual temperature time series. The highest predictive performance of {RF} is observed when using a low number of recent lagged predictor variables. This outcome could be useful in relevant future applications, with the prospect to achieve higher predictive accuracy.},
	pages = {114},
	number = {4},
	journaltitle = {Algorithms},
	author = {Tyralis, Hristos and Papacharalampous, Georgia},
	urldate = {2018-06-12},
	date = {2017-10-04},
	langid = {english},
	keywords = {{ARFIMA}, {ARMA}, machine learning, one-step ahead forecasting, random forests, time series forecasting, variable selection},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/6VLLM48F/Tyralis and Papacharalampous - 2017 - Variable Selection in Time Series Forecasting Usin.pdf:application/pdf;Snapshot:/Users/srivathsanseshadri/Zotero/storage/WFRHZSQU/114.html:text/html}
}

@article{colon-gonzalez_effects_2013,
	title = {The Effects of Weather and Climate Change on Dengue},
	volume = {7},
	issn = {1935-2735},
	url = {http://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0002503},
	doi = {10.1371/journal.pntd.0002503},
	abstract = {Background There is much uncertainty about the future impact of climate change on vector-borne diseases. Such uncertainty reflects the difficulties in modelling the complex interactions between disease, climatic and socioeconomic determinants. We used a comprehensive panel dataset from Mexico covering 23 years of province-specific dengue reports across nine climatic regions to estimate the impact of weather on dengue, accounting for the effects of non-climatic factors. Methods and Findings Using a Generalized Additive Model, we estimated statistically significant effects of weather and access to piped water on dengue. The effects of weather were highly nonlinear. Minimum temperature (Tmin) had almost no effect on dengue incidence below 5°C, but Tmin values above 18°C showed a rapidly increasing effect. Maximum temperature above 20°C also showed an increasing effect on dengue incidence with a peak around 32°C, after which the effect declined. There is also an increasing effect of precipitation as it rose to about 550 mm, beyond which such effect declines. Rising access to piped water was related to increasing dengue incidence. We used our model estimations to project the potential impact of climate change on dengue incidence under three emission scenarios by 2030, 2050, and 2080. An increase of up to 40\% in dengue incidence by 2080 was estimated under climate change while holding the other driving factors constant. Conclusions Our results indicate that weather significantly influences dengue incidence in Mexico and that such relationships are highly nonlinear. These findings highlight the importance of using flexible model specifications when analysing weather–health interactions. Climate change may contribute to an increase in dengue incidence. Rising access to piped water may aggravate dengue incidence if it leads to increased domestic water storage. Climate change may therefore influence the success or failure of future efforts against dengue.},
	pages = {e2503},
	number = {11},
	journaltitle = {{PLOS} Neglected Tropical Diseases},
	shortjournal = {{PLOS} Neglected Tropical Diseases},
	author = {Colón-González, Felipe J. and Fezzi, Carlo and Lake, Iain R. and Hunter, Paul R.},
	urldate = {2018-06-12},
	date = {2013-11-14},
	langid = {english},
	keywords = {Rain, Seasons, Climate change, Mexican people, Mexico, Socioeconomic aspects of health, Water resources, Weather},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/H3YPPI4D/Colón-González et al. - 2013 - The Effects of Weather and Climate Change on Dengu.pdf:application/pdf;Snapshot:/Users/srivathsanseshadri/Zotero/storage/4BC72T8P/article.html:text/html}
}

@article{noauthor_notitle_nodate
}

@misc{noauthor_notitle_nodate-1
}

@online{davenport_competing_2006,
	title = {Competing on Analytics},
	url = {https://hbr.org/2006/01/competing-on-analytics},
	abstract = {Some companies have built their very businesses on their ability to collect, analyze, and act on data. Every company can learn from what these firms do.},
	titleaddon = {Harvard Business Review},
	author = {Davenport, Thomas H.},
	urldate = {2018-07-05},
	date = {2006-01-01},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/GXJQBKE9/competing-on-analytics.html:text/html}
}

@article{dumitrescu_traveling_2008,
	title = {The traveling salesman problem with pickup and delivery: polyhedral results and a branch-and-cut algorithm},
	volume = {121},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-008-0234-9},
	doi = {10.1007/s10107-008-0234-9},
	shorttitle = {The traveling salesman problem with pickup and delivery},
	abstract = {The Traveling Salesman Problem with Pickup and Delivery ({TSPPD}) is defined on a graph containing pickup and delivery vertices between which there exists a one-to-one relationship. The problem consists of determining a minimum cost tour such that each pickup vertex is visited before its corresponding delivery vertex. In this paper, the {TSPPD} is modeled as an integer linear program and its polyhedral structure is analyzed. In particular, the dimension of the {TSPPD} polytope is determined and several valid inequalities, some of which are facet defining, are introduced. Separation procedures and a branch-and-cut algorithm are developed. Computational results show that the algorithm is capable of solving to optimality instances involving up to 35 pickup and delivery requests, thus more than doubling the previous record of 15.},
	pages = {269},
	number = {2},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Dumitrescu, Irina and Ropke, Stefan and Cordeau, Jean-François and Laporte, Gilbert},
	urldate = {2018-08-27},
	date = {2008-07-22},
	langid = {english},
	keywords = {90C10, 90C27, Branch-and-cut algorithm, Pickup and delivery, Polyhedral results, Precedence relationships, Separation procedures, Traveling salesman problem, Valid inequalities},
	file = {Springer Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/9ZXPW224/Dumitrescu et al. - 2008 - The traveling salesman problem with pickup and del.pdf:application/pdf}
}

@article{oneil_exact_nodate,
	title = {Exact Methods for Solving Traveling Salesman Problems with Pickup and Delivery in Real Time},
	pages = {30},
	author = {O’Neil, Ryan J and Hoﬀman, Karla},
	langid = {english},
	file = {O’Neil and Hoﬀman - Exact Methods for Solving Traveling Salesman Probl.pdf:/Users/srivathsanseshadri/Zotero/storage/5HS7YMUC/O’Neil and Hoﬀman - Exact Methods for Solving Traveling Salesman Probl.pdf:application/pdf}
}

@online{noauthor_vehicle_nodate,
	title = {Vehicle Routing - Advanced Topics: Part I},
	url = {https://www.coursera.org/lecture/discrete-optimization/vehicle-routing-QB8JE},
	shorttitle = {Vehicle Routing - Advanced Topics},
	abstract = {Video created by The University of Melbourne for the course "Discrete Optimization". These lectures cover some more advanced concepts in optimization. They introduce constraint-programming techniques for scheduling and routing.  Learn online and ...},
	titleaddon = {Coursera},
	urldate = {2018-08-30},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/452JP45Q/vehicle-routing-QB8JE.html:text/html}
}

@inreference{noauthor_vehicle_2018,
	title = {Vehicle routing problem},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Vehicle_routing_problem&oldid=852580999},
	abstract = {The vehicle routing problem ({VRP}) is a combinatorial optimization and integer programming problem which asks "What is the optimal set of routes for a fleet of vehicles to traverse in order to deliver to a given set of customers?". It generalises the well-known travelling salesman problem ({TSP}). It first appeared in a paper by George Dantzig and John Ramser in 1959, in which first algorithmic approach was written and was applied to petrol deliveries. Often, the context is that of delivering goods located at a central depot to customers who have placed orders for such goods. The objective of the {VRP} is to minimize the total route cost. In 1964, Clarke and Wright improved on Dantzig and Ramser's approach using an effective greedy approach called the savings algorithm.
Determining the optimal solution to {VRP} is {NP}-hard, so the size of problems that can be solved, optimally, using mathematical programming or combinatorial optimization may be limited. Therefore, commercial solvers tend to use heuristics due to the size and frequency of real world {VRPs} they need to solve. (For a non-technical explanation of why the {VRP} is so challenging please see the External Links below.)
The {VRP} has many obvious applications in industry. In fact, the use of computer optimization programs can give savings of 5\% to a company as transportation is usually a significant component of the cost of a product (10\%) - indeed, the transportation sector makes up 10\% of the {EU}'s {GDP}. Consequently, any savings created by the {VRP}, even less than 5\%, are significant.},
	booktitle = {Wikipedia},
	urldate = {2018-08-31},
	date = {2018-07-30},
	langid = {english},
	note = {Page Version {ID}: 852580999},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/7S8SIRG9/index.html:text/html}
}

@online{noauthor_google_nodate,
	title = {Google Optimization Tools {\textbar} Optimization},
	url = {https://developers.google.com/optimization/},
	abstract = {The {OR}-Tools suite provides operations research software libraries and {APIs} for constraint optimization, linear optimization, and flow and graph algorithms.},
	titleaddon = {Google Developers},
	urldate = {2018-08-31},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/UI9ZXGPQ/optimization.html:text/html}
}

@online{noauthor_capacitated_nodate,
	title = {Capacitated Vehicle Routing Problem {\textbar} Optimization},
	url = {https://developers.google.com/optimization/routing/cvrp},
	titleaddon = {Google Developers},
	urldate = {2018-08-31},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/GNQVVQ8M/cvrp.html:text/html}
}

@article{gansterer_exact_2018,
	title = {Exact solutions for the collaborative pickup and delivery problem},
	volume = {26},
	issn = {1435-246X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5945767/},
	doi = {10.1007/s10100-017-0503-x},
	abstract = {In this study we investigate the decision problem of a central authority in pickup and delivery carrier collaborations. Customer requests are to be redistributed among participants, such that the total cost is minimized. We formulate the problem as multi-depot traveling salesman problem with pickups and deliveries. We apply three well-established exact solution approaches and compare their performance in terms of computational time. To avoid unrealistic solutions with unevenly distributed workload, we extend the problem by introducing minimum workload constraints. Our computational results show that, while for the original problem Benders decomposition is the method of choice, for the newly formulated problem this method is clearly dominated by the proposed column generation approach. The obtained results can be used as benchmarks for decentralized mechanisms in collaborative pickup and delivery problems.},
	pages = {357--371},
	number = {2},
	journaltitle = {Central European Journal of Operations Research},
	shortjournal = {Cent Eur J Oper Res},
	author = {Gansterer, Margaretha and Hartl, Richard F. and Salzmann, Philipp E. H.},
	urldate = {2018-08-31},
	date = {2018},
	pmid = {29773966},
	pmcid = {PMC5945767},
	file = {PubMed Central Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/SGACRMM4/Gansterer et al. - 2018 - Exact solutions for the collaborative pickup and d.pdf:application/pdf}
}

@article{oneil_exact_nodate-1,
	title = {Exact Methods for Solving Traveling Salesman Problems with Pickup and Delivery in Real Time},
	pages = {30},
	author = {O’Neil, Ryan J and Hoﬀman, Karla},
	langid = {english},
	file = {O’Neil and Hoﬀman - Exact Methods for Solving Traveling Salesman Probl.pdf:/Users/srivathsanseshadri/Zotero/storage/KGCH7CCA/O’Neil and Hoﬀman - Exact Methods for Solving Traveling Salesman Probl.pdf:application/pdf}
}

@book{laporte_traveling_1992,
	title = {The Traveling Salesman Problem: An overview of exact . . .},
	shorttitle = {The Traveling Salesman Problem},
	abstract = {In this paper, some of the main known algorithms for the traveling salesman problem are surveyed. The paper is organized as follows: 1) definition; 2) applications; 3) complexity analysis; 4) exact algorithms; 5) heuristic algorithms; 6) conclusion.},
	author = {Laporte, Gilbert},
	date = {1992},
	file = {Citeseer - Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/IRUT4P46/Laporte - 1992 - The Traveling Salesman Problem An overview of exa.pdf:application/pdf;Citeseer - Snapshot:/Users/srivathsanseshadri/Zotero/storage/YNUKQVVY/summary.html:text/html}
}

@online{noauthor_analytics_nodate,
	title = {Analytics in Marketing - Measure, Analyze, and Manage},
	url = {https://www.wordstream.com/marketing-analytics},
	urldate = {2018-10-05},
	file = {Analytics in Marketing - Measure, Analyze, and Manage:/Users/srivathsanseshadri/Zotero/storage/ZVGJS5BZ/marketing-analytics.html:text/html}
}

@online{noauthor_what_nodate,
	title = {What is marketing analytics?},
	url = {https://www.sas.com/en_us/insights/marketing/marketing-analytics.html},
	abstract = {Understand what marketing analytics is and how it can help you gauge the success of your marketing programs, and get insights on how to use it successfully.},
	urldate = {2018-10-05},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/X8YH4W8A/marketing-analytics.html:text/html}
}

@online{emerson_seven_2014,
	title = {Seven Essential Components to a Marketing Plan},
	url = {https://www.inc.com/theupsstore/seven-essential-components-to-a-marketing-plan.html},
	abstract = {When you start out in business, two things are scarce: time and resources. To create an effective new business strategy and ensure you're not wasting time or money, you need a marketing plan.},
	titleaddon = {Inc.com},
	author = {Emerson, Melinda},
	urldate = {2018-10-05},
	date = {2014-04-29},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/6NMHQVUB/seven-essential-components-to-a-marketing-plan.html:text/html}
}

@online{noauthor_real_nodate,
	title = {The Real Story of New Coke},
	url = {https://www.coca-colacompany.com/stories/coke-lore-new-coke},
	abstract = {On April 23, 1985, Coca-Cola took arguably the biggest risk in consumer goods history by announcing that it was changing the formula for Coke -- and spawning consumer angst the likes of which no business has ever seen.},
	titleaddon = {The Coca-Cola Company},
	urldate = {2018-10-05},
	langid = {american},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/N5R9U8WF/coke-lore-new-coke.html:text/html}
}

@article{muller_stability_2014,
	title = {Stability of market segmentation with cluster analysis – A methodological approach},
	volume = {34},
	issn = {0950-3293},
	url = {http://www.sciencedirect.com/science/article/pii/S0950329313002309},
	doi = {10.1016/j.foodqual.2013.12.004},
	abstract = {Market segmentation is a very popular marketing tool. In the food sector, the characteristics of different consumer attitudes and consumption habits are often used as the basis for segmentation. However, the success of a target-oriented marketing approach to selected groups of consumers depends on the results of the methodology applied. So far, relatively little attention has been paid to the reliability of the analysis used for attitude-based market segmentation, to the validity or internal stability of results or to the dynamic stability over time with regard to number, size and properties of the segments. In our study, we used data from a panel of more than 10,000 German households. The participants were segmented using a statement battery and the application of cluster analysis. In order to ensure an internally stable cluster solution, our focus was on the analytical and technical process of decision making when clustering a large dataset. A combination of various statistical measures was applied in order to enable objective decision making in the determination of the optimal number of clusters. The dynamic stability of the resulting segments was determined by confirmatory cluster analyses using data from the same individuals in three subsequent years. The results of the analyses show that neither the internal nor the dynamic stability of market segments should be taken for granted. Therefore, marketers face the challenge of designing segment-specific marketing strategies in a way that allows changes in consumer preferences to be integrated.},
	pages = {70--78},
	journaltitle = {Food Quality and Preference},
	shortjournal = {Food Quality and Preference},
	author = {Müller, Henriette and Hamm, Ulrich},
	urldate = {2018-10-13},
	date = {2014-06-01},
	keywords = {Cluster analysis, Market segmentation, Panel survey, Reliability, Stability over time, Validity},
	file = {ScienceDirect Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/5XT87ASU/Müller and Hamm - 2014 - Stability of market segmentation with cluster anal.pdf:application/pdf;ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/WUN7NCUC/S0950329313002309.html:text/html}
}

@online{tutorialspoint.com_partial_nodate,
	title = {Partial Dependency in {DBMS}},
	url = {https://www.tutorialspoint.com/Partial-Dependency-in-DBMS},
	author = {tutorialspoint.com},
	urldate = {2019-02-03},
	langid = {american},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/X6X87ZPM/Partial-Dependency-in-DBMS.html:text/html}
}

@article{ribeiro_model-agnostic_2016,
	title = {Model-Agnostic Interpretability of Machine Learning},
	url = {http://arxiv.org/abs/1606.05386},
	abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach ({LIME}) that addresses these challenges.},
	journaltitle = {{arXiv}:1606.05386 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2019-04-29},
	date = {2016-06-16},
	eprinttype = {arxiv},
	eprint = {1606.05386},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.05386 PDF:/Users/srivathsanseshadri/Zotero/storage/KKJGEXYU/Ribeiro et al. - 2016 - Model-Agnostic Interpretability of Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/srivathsanseshadri/Zotero/storage/8NWEZA4V/1606.html:text/html}
}

@article{ribeiro_model-agnostic_2016-1,
	title = {Model-Agnostic Interpretability of Machine Learning},
	url = {http://arxiv.org/abs/1606.05386},
	abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach ({LIME}) that addresses these challenges.},
	journaltitle = {{arXiv}:1606.05386 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2019-04-29},
	date = {2016-06-16},
	eprinttype = {arxiv},
	eprint = {1606.05386},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.05386 PDF:/Users/srivathsanseshadri/Zotero/storage/2MH8L75X/Ribeiro et al. - 2016 - Model-Agnostic Interpretability of Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/srivathsanseshadri/Zotero/storage/FW3TPLQY/1606.html:text/html}
}

@online{noauthor_deep_nodate,
	title = {Deep Learning with Python by Francois Chollet: Manning Publications 9781617294433 Soft cover - {MeridianBooks}},
	url = {https://www.abebooks.com/9781617294433/Deep-Learning-Python-Francois-Chollet-1617294438/plp},
	shorttitle = {Deep Learning with Python by Francois Chollet},
	abstract = {{AbeBooks}.com: Deep Learning with Python: New, fast shipping!},
	urldate = {2019-05-12},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/92DFQESM/bd.html:text/html}
}

@book{chollet_deep_2017,
	location = {Shelter Island, New York},
	edition = {1 edition},
	title = {Deep Learning with Python},
	isbn = {978-1-61729-443-3},
	abstract = {Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google {AI} researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free {eBook} in {PDF}, Kindle, and {ePub} formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning—a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google {AI} researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects.  What's Inside Deep learning from first {principlesSetting} up your own deep-learning environment Image-classification {modelsDeep} learning for text and {sequencesNeural} style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, {TensorFlow}, or machine learning is required. About the Author François Chollet works on deep learning at Google in Mountain View, {CA}. He is the creator of the Keras deep-learning library, as well as a contributor to the {TensorFlow} machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition ({CVPR}), the Conference and Workshop on Neural Information Processing Systems ({NIPS}), the International Conference on Learning Representations ({ICLR}), and others. Table of Contents {PART} 1 - {FUNDAMENTALS} {OF} {DEEP} {LEARNING} What is deep learning?Before we begin: the mathematical building blocks of neural networks Getting started with neural {networksFundamentals} of machine {learningPART} 2 - {DEEP} {LEARNING} {IN} {PRACTICEDeep} learning for computer {visionDeep} learning for text and {sequencesAdvanced} deep-learning best {practicesGenerative} deep {learningConclusionsappendix} A - Installing Keras and its dependencies on Ubuntuappendix B - Running Jupyter notebooks on an {EC}2 {GPU} instance},
	pagetotal = {384},
	publisher = {Manning Publications},
	author = {Chollet, Francois},
	date = {2017-12-22}
}

@inreference{noauthor_mnist_2019,
	title = {{MNIST} database},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=MNIST_database&oldid=896255090},
	abstract = {The {MNIST} database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by "re-mixing" the samples from {NIST}'s original datasets. The creators felt that since {NIST}'s training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from {NIST} were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.

The {MNIST} database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from {NIST}'s training dataset, while the other half of the training set and the other half of the test set were taken from {NIST}'s testing dataset.
There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the {MNIST} database of 0.23\%. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support vector machine to get an error rate of 0.8\%. An extended dataset similar to {MNIST} called {EMNIST} has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits and characters.},
	booktitle = {Wikipedia},
	urldate = {2019-05-12},
	date = {2019-05-09},
	langid = {english},
	note = {Page Version {ID}: 896255090},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/JZAB29RZ/index.html:text/html}
}

@online{noauthor_cs231n_nodate,
	title = {{CS}231n Convolutional Neural Networks for Visual Recognition},
	url = {http://cs231n.github.io/convolutional-networks/},
	urldate = {2019-05-13},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:/Users/srivathsanseshadri/Zotero/storage/6EGDNT9G/convolutional-networks.html:text/html}
}

@inproceedings{jain_recurrent_2016,
	title = {Recurrent Neural Networks for driver activity anticipation via sensory-fusion architecture},
	doi = {10.1109/ICRA.2016.7487478},
	abstract = {Anticipating the future actions of a human is a widely studied problem in robotics that requires spatio-temporal reasoning. In this work we propose a deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks ({RNNs}) that use Long Short-Term Memory ({LSTM}) units to capture long temporal dependencies. We train our architecture in a sequence-to-sequence prediction manner, and it explicitly learns to predict the future given only a partial temporal context. We further introduce a novel loss layer for anticipation which prevents over-fitting and encourages early anticipation. We use our architecture to anticipate driving maneuvers several seconds before they happen on a natural driving data set of 1180 miles. The context for maneuver anticipation comes from multiple sensors installed on the vehicle. Our approach shows significant improvement over the state-of-the-art in maneuver anticipation by increasing the precision from 77.4\% to 90.5\% and recall from 71.2\% to 87.4\%.},
	eventtitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {3118--3125},
	booktitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Jain, A. and Singh, A. and Koppula, H. S. and Soh, S. and Saxena, A.},
	date = {2016-05},
	keywords = {Computer architecture, Context, deep learning approach, driver activity anticipation, driving maneuver anticipation, learning systems, Logic gates, long short-term memory units, long temporal dependencies, loss layer, {LSTM} units, multiple sensors, neurocontrollers, recurrent neural nets, recurrent neural networks, Recurrent neural networks, road traffic control, Robot sensing systems, sensor fusion, sensory-fusion architecture, sensory-rich robotics, sequence-to-sequence prediction, spatiotemporal reasoning, Vehicles},
	file = {IEEE Xplore Abstract Record:/Users/srivathsanseshadri/Zotero/storage/P7QU8G25/7487478.html:text/html;IEEE Xplore Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/IPTNNGD7/Jain et al. - 2016 - Recurrent Neural Networks for driver activity anti.pdf:application/pdf}
}

@article{xu_unsupervised_2018,
	title = {Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal {KPIs} in Web Applications},
	url = {http://arxiv.org/abs/1802.03903},
	doi = {10.1145/3178876.3185996},
	abstract = {To ensure undisrupted business, large Internet companies need to closely monitor various {KPIs} (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal {KPIs} with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on {VAE}. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline {VAE} approach, and its best F-scores range from 0.75 to 0.9 for the studied {KPIs} from a top global Internet company. We come up with a novel {KDE} interpretation of reconstruction for Donut, making it the first {VAE}-based anomaly detection algorithm with solid theoretical explanation.},
	pages = {187--196},
	journaltitle = {Proceedings of the 2018 World Wide Web Conference on World Wide Web  - {WWW} '18},
	author = {Xu, Haowen and Chen, Wenxiao and Zhao, Nengwen and Li, Zeyan and Bu, Jiahao and Li, Zhihan and Liu, Ying and Zhao, Youjian and Pei, Dan and Feng, Yang and Chen, Jie and Wang, Zhaogang and Qiao, Honglin},
	urldate = {2019-05-25},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1802.03903},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1802.03903 PDF:/Users/srivathsanseshadri/Zotero/storage/N925NRI8/Xu et al. - 2018 - Unsupervised Anomaly Detection via Variational Aut.pdf:application/pdf;arXiv.org Snapshot:/Users/srivathsanseshadri/Zotero/storage/KPWMYP42/1802.html:text/html}
}

@online{mwiti_introduction_2018,
	title = {Introduction to Restricted Boltzmann Machines Using {PyTorch}},
	url = {https://heartbeat.fritz.ai/guide-to-restricted-boltzmann-machines-using-pytorch-ee50d1ed21a8},
	abstract = {Learn how to use {PyTorch} to build a simple machine learning model using restricted Boltzmann machines},
	titleaddon = {Heartbeat},
	author = {Mwiti, Derrick},
	urldate = {2019-05-27},
	date = {2018-08-03},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/L2ZRCVS9/guide-to-restricted-boltzmann-machines-using-pytorch-ee50d1ed21a8.html:text/html}
}

@online{noauthor_credit_nodate,
	title = {Credit Card Fraud Detection},
	url = {https://kaggle.com/mlg-ulb/creditcardfraud},
	abstract = {Anonymized credit card transactions labeled as fraudulent or genuine},
	urldate = {2019-05-27},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/G3DA67N2/creditcardfraud.html:text/html}
}

@article{li_lecture_nodate,
	title = {Lecture 13: Generative Models},
	pages = {133},
	author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
	langid = {english},
	file = {Li et al. - Lecture 13 Generative Models.pdf:/Users/srivathsanseshadri/Zotero/storage/YJ9JI5DM/Li et al. - Lecture 13 Generative Models.pdf:application/pdf}
}

@article{smith_bayesian_2018,
	title = {A {BAYESIAN} {PERSPECTIVE} {ON} {GENERALIZATION} {AND} {STOCHASTIC} {GRADIENT} {DESCENT}},
	pages = {13},
	author = {Smith, Samuel L and Le, Quoc V},
	date = {2018},
	langid = {english},
	file = {Smith and Le - 2018 - A BAYESIAN PERSPECTIVE ON GENERALIZATION AND STOCH.pdf:/Users/srivathsanseshadri/Zotero/storage/6V7LMINM/Smith and Le - 2018 - A BAYESIAN PERSPECTIVE ON GENERALIZATION AND STOCH.pdf:application/pdf}
}

@online{wang_credit_2017,
	title = {Credit card fraud detection 1 – using auto-encoder in {TensorFlow}},
	url = {https://weiminwang.blog/2017/06/23/credit-card-fraud-detection-using-auto-encoder-in-tensorflow-2/},
	abstract = {Github scripts The ipython notebook has been uploaded into github – free feel to jump there directly if you want to skip the explanations. Introduction In this post, we will be exploring data…},
	titleaddon = {Weimin Wang},
	author = {Wang, Weimin},
	urldate = {2019-06-07},
	date = {2017-06-23},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/2EES5PD7/credit-card-fraud-detection-using-auto-encoder-in-tensorflow-2.html:text/html}
}

@online{noauthor_deep_nodate-1,
	title = {Deep Autoencoder using Keras – Data Driven Investor – Medium},
	url = {https://medium.com/datadriveninvestor/deep-autoencoder-using-keras-b77cd3e8be95},
	urldate = {2019-06-08},
	file = {Deep Autoencoder using Keras – Data Driven Investor – Medium:/Users/srivathsanseshadri/Zotero/storage/98I3UFJ3/deep-autoencoder-using-keras-b77cd3e8be95.html:text/html}
}

@online{khandelwal_neural_2019,
	title = {Neural Network and Dropouts},
	url = {https://medium.com/datadriveninvestor/neural-network-and-dropouts-b6690c869a18},
	abstract = {In this post we will understand what is Dropout in neural networks, when should we use drop out and how it is implemented in neural…},
	titleaddon = {Data Driven Investor},
	author = {Khandelwal, Renu},
	urldate = {2019-06-08},
	date = {2019-01-10},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/Z6T8PSHS/neural-network-and-dropouts-b6690c869a18.html:text/html}
}

@online{lockridge_is_nodate,
	title = {Is it Time to Rethink Shipper-Carrier Contracts?},
	url = {http://www.truckinginfo.com/334884/is-it-time-to-rethink-shipper-carrier-contracts},
	abstract = {Some people believe that the move toward more dynamic, technology-driven interactions between shippers, carriers, and brokers could mean changes in the traditional way carriers work with shippers – including less reliance on the traditional year-long contract.},
	author = {Lockridge, Deborah},
	urldate = {2019-06-30},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/MTI37557/is-it-time-to-rethink-shipper-carrier-contracts.html:text/html}
}

@online{overdrive_slowdown_nodate,
	title = {Slowdown watch: How slow, how far down?},
	url = {https://www.overdriveonline.com/slowdown-watch-how-slow-how-far-down/},
	shorttitle = {Slowdown watch},
	abstract = {, At best, business as usual for an undetermined time. At worst, a more pronounced slowdown this year or the next.},
	author = {Overdrive},
	urldate = {2019-06-30},
	langid = {english},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/XKK2ZP2X/slowdown-watch-how-slow-how-far-down.html:text/html}
}

@online{noauthor_about_nodate,
	title = {About this Book · Natural Language Processing in Action: Understanding, analyzing, and generating text with Python},
	url = {https://livebook.manning.com/book/natural-language-processing-in-action/about-this-book/},
	shorttitle = {About this Book · Natural Language Processing in Action},
	abstract = {{liveBooks} are enhanced books. They add narration, interactive exercises, code execution, and other features to {eBooks}.},
	urldate = {2019-07-27},
	langid = {american},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/DASDIDHE/about-this-book.html:text/html}
}

@book{robinson_text_nodate,
	title = {Text Mining with R},
	url = {https://www.tidytextmining.com/},
	abstract = {A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools},
	author = {Robinson, Julia Silge \{and\} David},
	urldate = {2019-07-27},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/CKKW6AU5/www.tidytextmining.com.html:text/html}
}

@book{lane_natural_2019,
	location = {Shelter Island, {NY}},
	edition = {1 edition},
	title = {Natural Language Processing in Action: Understanding, analyzing, and generating text with Python},
	isbn = {978-1-61729-463-1},
	shorttitle = {Natural Language Processing in Action},
	abstract = {Summary Natural Language Processing in Action is your guide to creating machines that understand human language using the power of Python with its ecosystem of packages dedicated to {NLP} and {AI}.  Purchase of the print book includes a free {eBook} in {PDF}, Kindle, and {ePub} formats from Manning Publications. All examples are included in the open source `nlpia` package on python.org and github.com, complete with a conda environment and Dockerfile to help you get going quickly on any platform.  About the Technology Recent advances in deep learning empower applications to understand text and speech with extreme accuracy. The result? Chatbots that can imitate real people, meaningful resume-to-job matches, superb predictive search, and automatically generated document summaries--all at a low cost. New techniques, along with accessible tools like Keras and {TensorFlow}, make professional-quality {NLP} easier than ever before.  About the Book Natural Language Processing in Action is your guide to building machines that can read and interpret human language. In it, you'll use readily available Python packages to capture the meaning in text and react accordingly. The book expands traditional {NLP} approaches to include neural networks, modern deep learning algorithms, and generative techniques as you tackle real-world problems like extracting dates and names, composing text, and answering free-form questions.  What's inside  Some sentences in this book were written by {NLP}! Can you guess which ones?Working with Keras, {TensorFlow}, gensim, and scikit-learn Rule-based and data-based {NLP} Scalable pipelines   About the Reader This book requires a basic understanding of deep learning and intermediate Python skills.  About the Authors Hobson Lane, Cole Howard, and Hannes Max Hapke are experienced {NLP} engineers who use these techniques in production for profit and fun: contributing to social-benefit projects like smart guides for people with blindness and cognitive assistance for those with developmental challenges or suffering from information overload (don't we all?).  Table of Contents  {PART} 1 - {WORDY} {MACHINES} Packets of thought ({NLP} overview) Build your vocabulary (word tokenization) Math with words ({TF}-{IDF} vectors) Finding meaning in word counts (semantic analysis)  {PART} 2 - {DEEPER} {LEARNING} ({NEURAL} {NETWORKS}) Baby steps with neural networks (perceptrons and backpropagation) Reasoning with word vectors (Word2vec) Getting words in order with convolutional neural networks ({CNNs}) Loopy (recurrent) neural networks ({RNNs}) Improving retention with long short-term memory networks Sequence-to-sequence models and attention  {PART} 3 - {GETTING} {REAL} ({REAL}-{WORLD} {NLP} {CHALLENGES}) Information extraction (named entity extraction and question answering) Getting chatty (dialog engines) Scaling up (optimization, parallelization, and batch processing)},
	pagetotal = {544},
	publisher = {Manning Publications},
	author = {Lane, Hobson and Hapke, Hannes and Howard, Cole},
	date = {2019-04-14}
}

@article{falbel_tensorflow_2017,
	title = {{TensorFlow} for R: Word Embeddings with Keras},
	url = {https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras/},
	shorttitle = {{TensorFlow} for R},
	abstract = {Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.},
	author = {Falbel, Daniel},
	urldate = {2019-07-27},
	date = {2017-12-22},
	file = {Snapshot:/Users/srivathsanseshadri/Zotero/storage/S55XYG8H/2017-12-22-word-embeddings-with-keras.html:text/html}
}

@article{maren_extracting_nodate,
	title = {Extracting Meaning from Text by Matching Entities and Terms to Ontologies and Concepts},
	pages = {24},
	author = {Maren, Alianna J},
	langid = {english},
	file = {Maren - Extracting Meaning from Text by Matching Entities .pdf:/Users/srivathsanseshadri/Zotero/storage/X7ECEZRQ/Maren - Extracting Meaning from Text by Matching Entities .pdf:application/pdf}
}

@article{shen_learning_nodate,
	title = {Learning Context-Aware Convolutional Filters for Text Processing},
	abstract = {Convolutional neural networks ({CNNs}) have recently emerged as a popular building block for natural language processing ({NLP}). Despite their success, most existing {CNN} models employed in {NLP} share the same learned (and static) set of ﬁlters for all input sentences. In this paper, we consider an approach of using a small meta network to learn contextaware convolutional ﬁlters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware ﬁlters. We further generalize this framework to model sentence pairs, where a bidirectional ﬁlter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classiﬁcation, sentiment analysis, answer sentence selection, and paraphrase identiﬁcation, our proposed model, a modiﬁed {CNN} with context-aware ﬁlters, consistently outperforms the standard {CNN} and attentionbased {CNN} baselines. By visualizing the learned context-aware ﬁlters, we further validate and rationalize the effectiveness of proposed framework.},
	pages = {10},
	author = {Shen, Dinghan and Min, Martin Renqiang and Li, Yitong and Carin, Lawrence},
	langid = {english},
	file = {Shen et al. - Learning Context-Aware Convolutional Filters for T.pdf:/Users/srivathsanseshadri/Zotero/storage/B3YJPIH5/Shen et al. - Learning Context-Aware Convolutional Filters for T.pdf:application/pdf}
}

@inproceedings{wang_dimensional_2016,
	location = {Berlin, Germany},
	title = {Dimensional Sentiment Analysis Using a Regional {CNN}-{LSTM} Model},
	url = {https://www.aclweb.org/anthology/P16-2037},
	doi = {10.18653/v1/P16-2037},
	eventtitle = {{ACL} 2016},
	pages = {225--230},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Jin and Yu, Liang-Chih and Lai, K. Robert and Zhang, Xuejie},
	urldate = {2019-08-26},
	date = {2016-08},
	file = {Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/CF6VX5D3/Wang et al. - 2016 - Dimensional Sentiment Analysis Using a Regional CN.pdf:application/pdf}
}

@article{yeh_comparisons_2009,
	title = {The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients},
	volume = {36},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417407006719},
	doi = {10.1016/j.eswa.2007.12.020},
	abstract = {This research aimed at the case of customers’ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel “Sorting Smoothing Method” to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y=A+{BX}) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.},
	pages = {2473--2480},
	number = {2},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Yeh, I-Cheng and Lien, Che-hui},
	urldate = {2019-11-15},
	date = {2009-03-01},
	langid = {english},
	keywords = {Banking, Data mining, Neural network, Probability},
	file = {ScienceDirect Full Text PDF:/Users/srivathsanseshadri/Zotero/storage/2X3SG9ME/Yeh and Lien - 2009 - The comparisons of data mining techniques for the .pdf:application/pdf;ScienceDirect Snapshot:/Users/srivathsanseshadri/Zotero/storage/7PWMFGS5/S0957417407006719.html:text/html}
}

@online{noauthor_uci_nodate,
	title = {{UCI} Machine Learning Repository: default of credit card clients Data Set},
	url = {https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients},
	urldate = {2019-11-15},
	file = {UCI Machine Learning Repository\: default of credit card clients Data Set:/Users/srivathsanseshadri/Zotero/storage/ZCT999DZ/default+of+credit+card+clients.html:text/html}
}